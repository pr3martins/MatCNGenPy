{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModelBuilder:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_type='word2vec',\n",
    "                 filename = \"word_embeddings/word2vec/GoogleNews-vectors-negative300.bin\",\n",
    "                 limit=500000\n",
    "                ):        \n",
    "        \n",
    "        import gensim.models.keyedvectors as word2vec\n",
    "        from gensim.models import KeyedVectors\n",
    "        \n",
    "        if embedding_type=='word2vec':\n",
    "            self.Model = KeyedVectors.load_word2vec_format(filename,\n",
    "                                                           binary=True, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = EmbeddingModelBuilder().Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexBuilder:\n",
    "    \n",
    "    def __init__(dbms='psql'):                \n",
    "        if dbms=='psql':\n",
    "            self._GET_TABLENAMES_SQL = \"SELECT DISTINCT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\"        \n",
    "            \n",
    "            \n",
    "        self.wordHash = {}\n",
    "        self.attributeHash = {}\n",
    "        \n",
    "        \n",
    "    \n",
    "    def createInvertedIndex(self,embeddingModel, cur, showLog=True):\n",
    "        #Output: wordHash (Term Index) with this structure below\n",
    "        #map['word'] = [ 'table': ( {column} , ['ctid'] ) ]\n",
    "\n",
    "        '''\n",
    "        The Term Index is built in a preprocessing step that scans only\n",
    "        once all the relations over which the queries will be issued.\n",
    "        '''\n",
    "\n",
    "        self.wordHash = {}\n",
    "        self.attributeHash = {}\n",
    "\n",
    "\n",
    "        # Get list of tablenames\n",
    "        cur.execute(self._GET_TABLENAMES_SQL)\n",
    "        for table in cur.fetchall():\n",
    "            table_name = table[0]\n",
    "\n",
    "            if table_name not in embeddingModel:\n",
    "                print('TABLE ',table_name, 'SKIPPED')\n",
    "                continue\n",
    "\n",
    "            print('INDEXING TABLE ',table_name)\n",
    "\n",
    "            self.attributeHash[table_name] = {}\n",
    "\n",
    "            #Get all tuples for this tablename\n",
    "            cur.execute(\n",
    "                sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "                #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "            )\n",
    "            printSkippedColumns = True\n",
    "            for row in cur.fetchall(): \n",
    "                for column in range(1,len(row)):\n",
    "                    column_name = cur.description[column][0] \n",
    "\n",
    "                    if column_name not in embeddingModel or column_name=='id':\n",
    "                        if printSkippedColumns:\n",
    "                            print('\\tCOLUMN ',column_name,' SKIPPED')\n",
    "                        continue\n",
    "\n",
    "                    ctid = row[0]\n",
    "\n",
    "                    for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "\n",
    "                        #Ignoring STOPWORDS\n",
    "                        if word in stw_set:\n",
    "                            continue\n",
    "\n",
    "                        #If word entry doesn't exists, it will be inicialized (setdefault method),\n",
    "                        #Append the location for this word\n",
    "                        self.wordHash.setdefault(word, {})                    \n",
    "                        self.wordHash[word].setdefault( table_name , {} )\n",
    "                        self.wordHash[word][table_name].setdefault( column_name , [] ).append(ctid)\n",
    "\n",
    "                        self.attributeHash[table_name].setdefault(column_name,(0,set()))\n",
    "                        self.attributeHash[table_name][column_name][1].add(word)\n",
    "                printSkippedColumns=False\n",
    "\n",
    "            #Count words\n",
    "\n",
    "            for (column_name,(norm,wordSet)) in self.attributeHash[table_name].items():\n",
    "                num_distinct_words = len(wordSet)\n",
    "                wordSet.clear()\n",
    "                self.attributeHash[table_name][column_name] = (norm,num_distinct_words)\n",
    "\n",
    "        print ('INVERTED INDEX CREATED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    EmbeddingModel = None   \n",
    "    WordHash = {}\n",
    "    AttributeHash = {}\n",
    "    \n",
    "    \n",
    "    def __init__(dbms='psql'):        \n",
    "        \n",
    "        if dbms=='psql':\n",
    "            self._GET_TABLENAMES_SQL = \"SELECT DISTINCT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\"        \n",
    "            \n",
    "    \n",
    "    def load(self):\n",
    "        EmbeddingModel = EmbeddingModelBuilder().Model\n",
    "              \n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': ({'birth age'}, {'23'}), 'name': ({'name'}, {'paulo', 'rodrigo'})}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tupleset:\n",
    "    \n",
    "    table = None\n",
    "    predicates = {}\n",
    "    \n",
    "    def __init__(self, table):\n",
    "        self.table = table\n",
    "        \n",
    "    def addMapping(self,attribute,schemaWords,valueWords):\n",
    "        self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "        self.predicates[attribute][0].update(schemaWords)\n",
    "        self.predicates[attribute][1].update(valueWords)\n",
    "        \n",
    "    def isFreeTupleset(self):\n",
    "        return len(self.predicates)==0\n",
    "\n",
    "    def __repr__(self):\n",
    "        aux = (self.table,self.predicates)\n",
    "        return repr(  tuple(aux) )\n",
    "    \n",
    "        \n",
    "x = Tupleset('person')\n",
    "x.addMapping('name',{'name'},{'paulo','rodrigo'})\n",
    "x.addMapping('age',{'birth age'},{'23'})\n",
    "\n",
    "x.isFreeTupleset()\n",
    "x.predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSFind(Q):\n",
    "    #Input:  A keyword query Q=[k1, k2, . . . , km]\n",
    "    #Output: Set of non-free and non-empty tuple-sets Rq\n",
    "\n",
    "    '''\n",
    "    The tuple-set Rki contains the tuples of Ri that contain all\n",
    "    terms of K and no other keywords from Q\n",
    "    '''\n",
    "    \n",
    "    #Part 1: Find sets of tuples containing each keyword\n",
    "    global P\n",
    "    P = {}\n",
    "    for keyword in Q:\n",
    "        tupleset = set()\n",
    "        \n",
    "        if keyword not in wordHash:\n",
    "            continue\n",
    "        \n",
    "        for (table,attributes) in wordHash.get(keyword)[1].items():\n",
    "            for (attribute,ctids) in attributes.items():\n",
    "                for ctid in ctids:\n",
    "                    tupleset.add( (table,attribute,ctid) )\n",
    "        P[frozenset([keyword])] = tupleset\n",
    "    \n",
    "    #Part 2: Find sets of tuples containing larger termsets\n",
    "    P = TSInterMartins(P)\n",
    "    \n",
    "    #Part 3:Build tuple-sets\n",
    "    Rq = set()\n",
    "    \n",
    "    for valueWords , tuples in P.items():\n",
    "        for (table,attribute,ctid) in tuples:\n",
    "            \n",
    "            ts = Tupleset(table)\n",
    "            ts.addMapping(attribute,frozenset(),valueWords)\n",
    "            Rq.add( ts )\n",
    "    #print ('TUPLE SETS CREATED')\n",
    "    return Rq\n",
    "\n",
    "\n",
    "def TSInter(P):\n",
    "    #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "    #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    '''\n",
    "    Termset is any non-empty subset K of the terms of a query Q        \n",
    "    '''\n",
    "    \n",
    "    Pprev = {}\n",
    "    Pprev=copy.deepcopy(P)\n",
    "    Pcurr = {}\n",
    "\n",
    "    combinations = [x for x in itertools.combinations(Pprev.keys(),2)]\n",
    "    for ( Ki , Kj ) in combinations:\n",
    "        Tki = Pprev[Ki]\n",
    "        Tkj = Pprev[Kj]\n",
    "        \n",
    "        X = Ki | Kj\n",
    "        Tx = Tki & Tkj        \n",
    "        \n",
    "        if len(Tx) > 0:            \n",
    "            Pcurr[X]  = Tx            \n",
    "            Pprev[Ki] = Tki - Tx         \n",
    "            Pprev[Kj] = Tkj - Tx\n",
    "            \n",
    "    if Pcurr != {}:\n",
    "        Pcurr = copy.deepcopy(TSInter(Pcurr))\n",
    "        \n",
    "    #Pprev = Pprev U Pcurr\n",
    "    Pprev.update(Pcurr)     \n",
    "    return Pprev   \n",
    "\n",
    "\n",
    "def TSInterMartins(P):\n",
    "    #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "    #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    '''\n",
    "    Termset is any non-empty subset K of the terms of a query Q        \n",
    "    '''\n",
    "    somethingChanged = False\n",
    "    \n",
    "    combinations = [x for x in itertools.combinations(P.keys(),2)]\n",
    "    for ( Ki , Kj ) in combinations:\n",
    "        Tki = P[Ki]\n",
    "        Tkj = P[Kj]\n",
    "        \n",
    "        X = Ki | Kj\n",
    "        Tx = Tki & Tkj        \n",
    "        \n",
    "        if len(Tx) > 0:            \n",
    "            P[X]  = Tx            \n",
    "            P[Ki] = Tki - Tx         \n",
    "            P[Kj] = Tkj - Tx\n",
    "            somethingChanged = True\n",
    "            \n",
    "    if somethingChanged:\n",
    "        TSInterMartins(P)   \n",
    "    return P"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatCNGenpy",
   "language": "python",
   "name": "matcngenpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
