{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:26:59.463796Z",
     "start_time": "2019-03-07T15:26:59.457499Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "import gc #garbage collector usado no createinvertedindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:27:08.861149Z",
     "start_time": "2019-03-07T15:26:59.466645Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def loadWordEmbeddingsModel(filename = \"word_embeddings/word2vec/GoogleNews-vectors-negative300.bin\"):\n",
    "    model = KeyedVectors.load_word2vec_format(filename,\n",
    "                                                       binary=True, limit=500000)\n",
    "    return model\n",
    "\n",
    "model = loadWordEmbeddingsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:27:08.881864Z",
     "start_time": "2019-03-07T15:27:08.863712Z"
    }
   },
   "outputs": [],
   "source": [
    "class BabelItemsIter:\n",
    "    def __init__(self,babelhash):\n",
    "        __slots__ = ('__babelhash')\n",
    "        self.__babelhash = babelhash    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__babelhash)\n",
    "    \n",
    "    def __contains__(self,item):\n",
    "        (key,value) = item\n",
    "        return key in self.__babelhash and self.__babelhash[key]==value\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for key in self.__babelhash.keys():\n",
    "            yield key, self.__babelhash[key]\n",
    "            \n",
    "    #Apesar de que segundo o PEP 3106 (https://www.python.org/dev/peps/pep-3106/) recomenda que façamos\n",
    "    # outros métodos, como and,eq,ne para permitir que a saída seja um set,\n",
    "    # não estamos preocupados com isso aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:27:09.039620Z",
     "start_time": "2019-03-07T15:27:08.883307Z"
    }
   },
   "outputs": [],
   "source": [
    "class BabelHash(dict):\n",
    "    \n",
    "    def __init__(self,babel={}):\n",
    "        __slots__ = ('__babel')\n",
    "        dict.__init__(self)\n",
    "        self.__babel = babel\n",
    "        \n",
    "    def __getidfromkey__(self,key):\n",
    "        return self.__babel[key]\n",
    "    \n",
    "    def __getkeyfromid__(self,key_id):\n",
    "        key = self.__babel[key_id]\n",
    "        return key\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        return dict.__getitem__(self,key_id)\n",
    "    \n",
    "    def __setitem__(self,key,value):    \n",
    "        try:\n",
    "            key_id = self.__babel[key]\n",
    "        except KeyError:\n",
    "            key_id = len(self.__babel)+1\n",
    "                     \n",
    "            self.__babel[key] = key_id\n",
    "            self.__babel[key_id] = key\n",
    "        \n",
    "        dict.__setitem__(self, key_id,value)\n",
    "    \n",
    "    def __delitem__(self, key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        dict.__delitem__(self, key_id)\n",
    "        \n",
    "    def __missing__(self,key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        return key_id\n",
    "        \n",
    "    def __delitem__(self, key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        dict.__delitem__(self,key_id)\n",
    "    \n",
    "    def __contains__(self, key):\n",
    "        try:\n",
    "            key_id = self.__getidfromkey__(key)\n",
    "        except KeyError:\n",
    "            return False\n",
    "        \n",
    "        return dict.__contains__(self,key_id)    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for key_id in dict.keys(self):\n",
    "            yield self.__getkeyfromid__(key_id)\n",
    "    \n",
    "    def keys(self):\n",
    "        for key_id in dict.keys(self):\n",
    "            yield self.__getkeyfromid__(key_id)\n",
    "    \n",
    "    def items(self):\n",
    "        return BabelItemsIter(self)\n",
    "    \n",
    "    def get(self,key):\n",
    "        value = None\n",
    "        if key in self:\n",
    "            value = self.__getitem__(key)\n",
    "        return value\n",
    "    \n",
    "    def setdefault(self,key,default=None):\n",
    "        if key not in self:\n",
    "            self[key]=default\n",
    "        return self[key]\n",
    "    \n",
    "    def printBabel(self):\n",
    "        print(self.__babel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:27:09.085782Z",
     "start_time": "2019-03-07T15:27:09.042059Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordHash(dict):      \n",
    "        \n",
    "    def __init__(self): \n",
    "        dict.__init__(self)\n",
    "    \n",
    "    def addMapping(self,word,table,attribute,ctid):\n",
    "        self.setdefault( word, (0, BabelHash() ) )                    \n",
    "        self[word].setdefault(table , BabelHash() )       \n",
    "        self[word][table].setdefault( attribute , [] ).append(ctid)        \n",
    "        \n",
    "    def getMappings(self,word,table,attribute):\n",
    "        return self[word][table][attribute]\n",
    "    \n",
    "    def getIAF(self,key):\n",
    "        return dict.__getitem__(self,key)[0]\n",
    "    \n",
    "    def setIAF(self,key,IAF):\n",
    "\n",
    "        oldIAF,oldValue = dict.__getitem__(self,key)\n",
    "        \n",
    "        dict.__setitem__(self, key,  (IAF,oldValue)  )\n",
    "    \n",
    "    def __getitem__(self,word):\n",
    "        return dict.__getitem__(self,word)[1]\n",
    "    \n",
    "    def __setitem__(self,word,value): \n",
    "        oldIAF,oldValue = dict.__getitem__(self,word)\n",
    "        dict.__setitem__(self, word,  (oldIAF,value)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:27:10.610291Z",
     "start_time": "2019-03-07T15:27:09.087593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "class DatabaseIter:\n",
    "    def __init__(self,embeddingModel,dbname='dblp',user='imdb',password='imdb'):\n",
    "        self.dbname=dbname\n",
    "        self.user=user\n",
    "        self.password =password\n",
    "        self.embeddingModel=embeddingModel\n",
    "\n",
    "    def __iter__(self):\n",
    "        with psycopg2.connect(dbname=self.dbname,user=self.user,password=self.password) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "\n",
    "                # Get list of tablenames\n",
    "\n",
    "                GET_TABLE_NAMES_SQL='''\n",
    "                    SELECT DISTINCT table_name\n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_schema='public';\n",
    "                ''' \n",
    "                cur.execute(GET_TABLE_NAMES_SQL)\n",
    "\n",
    "                tables = cur.fetchall()\n",
    "                print(tables)\n",
    "                for table in tables:\n",
    "                    table_name = table[0]\n",
    "\n",
    "                    if table_name not in self.embeddingModel:\n",
    "                        print('TABLE ',table_name, 'SKIPPED')\n",
    "                        continue\n",
    "\n",
    "                    print('INDEXING TABLE ',table_name)\n",
    "\n",
    "                    #Get all tuples for this tablename\n",
    "                    cur.execute(\n",
    "                        sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "                        #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "                    )\n",
    "\n",
    "                    printSkippedColumns = True\n",
    "\n",
    "                    for row in cur.fetchall(): \n",
    "                        for column in range(1,len(row)):\n",
    "                            column_name = cur.description[column][0] \n",
    "\n",
    "                            if column_name not in self.embeddingModel or column_name=='id':\n",
    "                                if printSkippedColumns:\n",
    "                                    print('\\tCOLUMN ',column_name,' SKIPPED')\n",
    "                                continue\n",
    "\n",
    "                            ctid = row[0]\n",
    "\n",
    "                            for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "\n",
    "                                #Ignoring STOPWORDS\n",
    "                                if word in stw_set:\n",
    "                                    continue\n",
    "\n",
    "                                yield table_name,ctid,column_name, word\n",
    "\n",
    "                        printSkippedColumns=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:27:10.682726Z",
     "start_time": "2019-03-07T15:27:10.612084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "def createInvertedIndex(embeddingModel,dbname='dblp',user='imdb',password='imdb',showLog=True):\n",
    "    #Output: wordHash (Term Index) with this structure below\n",
    "    #map['word'] = [ 'table': ( {column} , ['ctid'] ) ]\n",
    "\n",
    "    '''\n",
    "    The Term Index is built in a preprocessing step that scans only\n",
    "    once all the relations over which the queries will be issued.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    wh = WordHash()\n",
    "    ah = {}\n",
    "    \n",
    "    previousTable = None\n",
    "    \n",
    "    for table,ctid,column,word in DatabaseIter(model):        \n",
    "        wh.addMapping(word,table,column,ctid)\n",
    "        \n",
    "        ah.setdefault(table,{}).setdefault(column,{}).setdefault(word,1)\n",
    "        ah[table][column][word]+=1\n",
    "        \n",
    "    for table in ah:\n",
    "        for column in ah[table]:\n",
    "            \n",
    "            maxFrequency = numDistinctWords = numWords = 0            \n",
    "            for word, frequency in ah[table][column].items():\n",
    "                \n",
    "                numDistinctWords += 1\n",
    "                \n",
    "                numWords += frequency\n",
    "                \n",
    "                if frequency > maxFrequency:\n",
    "                    maxFrequency = frequency\n",
    "            \n",
    "            norm = 0\n",
    "            ah[table][column] = (norm,numDistinctWords,numWords,maxFrequency)\n",
    "\n",
    "    print ('INVERTED INDEX CREATED')\n",
    "    gc.collect()\n",
    "    return wh,ah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:33:34.697905Z",
     "start_time": "2019-03-07T15:27:10.685949Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('paper',), ('citation',), ('author',)]\n",
      "INDEXING TABLE  paper\n",
      "\tCOLUMN  paper_key  SKIPPED\n",
      "\tCOLUMN  conf_key  SKIPPED\n",
      "INDEXING TABLE  citation\n",
      "\tCOLUMN  paper_cite_key  SKIPPED\n",
      "\tCOLUMN  paper_cited_key  SKIPPED\n",
      "INDEXING TABLE  author\n",
      "\tCOLUMN  paper_key  SKIPPED\n",
      "INVERTED INDEX CREATED\n"
     ]
    }
   ],
   "source": [
    "wh,ah = createInvertedIndex(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:33:34.706697Z",
     "start_time": "2019-03-07T15:33:34.700175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': {'name': (0, 535435, 16327107, 131605)},\n",
      " 'paper': {'conference': (0, 8660, 3148477, 69756),\n",
      "           'title': (0, 487592, 17033298, 218701),\n",
      "           'year': (0, 60, 2299223, 147970)}}\n"
     ]
    }
   ],
   "source": [
    "pp(ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:33:34.723872Z",
     "start_time": "2019-03-07T15:33:34.709813Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log1p \n",
    "\n",
    "def processIAF(wordHash,attributeHash):\n",
    "    \n",
    "    total_attributes = sum([len(attribute) for attribute in attributeHash.values()])\n",
    "    \n",
    "    for (term, values) in wordHash.items():\n",
    "        attributes_with_this_term = sum([len(attribute) for attribute in wordHash[term].values()])\n",
    "        IAF = log1p(total_attributes/attributes_with_this_term)\n",
    "        wordHash.setIAF(term,IAF)        \n",
    "        \n",
    "    print('IAF PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:33:37.783535Z",
     "start_time": "2019-03-07T15:33:34.725649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAF PROCESSED\n"
     ]
    }
   ],
   "source": [
    "processIAF(wh,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:33:38.201440Z",
     "start_time": "2019-03-07T15:33:37.785716Z"
    }
   },
   "outputs": [],
   "source": [
    "def processNormsOfAttributes(wordHash,attributeHash):    \n",
    "    for word in wh:\n",
    "        for table in wh[word]:\n",
    "            for column, ctids in wh[word][table].items():\n",
    "                   \n",
    "                (prevNorm,numDistinctWords,numWords,maxFrequency) = attributeHash[table][column]\n",
    "\n",
    "                IAF = wordHash.getIAF(word)\n",
    "\n",
    "                frequency = len(ctids)\n",
    "                \n",
    "                TF = frequency/maxFrequency\n",
    "                \n",
    "                Norm = prevNorm + (TF*IAF)\n",
    "\n",
    "                attributeHash[table][column]=(Norm,numDistinctWords,numWords,maxFrequency)\n",
    "                \n",
    "    print ('NORMS OF ATTRIBUTES PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:33:44.893148Z",
     "start_time": "2019-03-07T15:33:38.204607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMS OF ATTRIBUTES PROCESSED\n"
     ]
    }
   ],
   "source": [
    "processNormsOfAttributes(wh,ah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Tupleset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:45:52.141795Z",
     "start_time": "2019-03-07T17:45:51.399093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAPER(title{}{'discover'}),\n",
      " PAPER(title{'2002'}{}),\n",
      " PAPER(title{'2002'}{'discover'})]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "class Tupleset:\n",
    "   \n",
    "    def __init__(self, table, predicates = None, tuples = None):            \n",
    "        \n",
    "        self.table = table\n",
    "        self.predicates= predicates if predicates is not None else {}\n",
    "        self.tuples= tuples if tuples is not None else set()\n",
    "        \n",
    "    def addTuple(self, tuple_id):\n",
    "        self.tuples.add(tuple_id)\n",
    "        \n",
    "    def addTuples(self, tuple_ids):\n",
    "        self.tuples.update(tuple_ids)\n",
    "        \n",
    "    def addAttribute(self,attribute):\n",
    "        self.attributes[attribute].setdefault( (set(),set()) )\n",
    "    \n",
    "    def union(self, otherTupleset, changeSources = False, projectionOnly = False):\n",
    "              \n",
    "        if self.table != otherTupleset.table:\n",
    "            return None\n",
    "        \n",
    "        if self.table == None:\n",
    "            return None\n",
    "        \n",
    "        if len(self.getKeywords() & otherTupleset.getKeywords())>0:\n",
    "            #tuple sets com palavras repetidas\n",
    "            return None\n",
    "\n",
    "        if projectionOnly:\n",
    "            if self.isValueFreeTupleset()==False or otherTupleset.isValueFreeTupleset() == False:\n",
    "                return None\n",
    "                \n",
    "        \n",
    "        jointTuples = self.tuples & otherTupleset.tuples\n",
    "        \n",
    "        jointPredicates = {}\n",
    "        \n",
    "        jointPredicates.update(copy.deepcopy(self.predicates))\n",
    "        \n",
    "        for attribute, (schemaWords, valueWords) in otherTupleset.predicates.items():  \n",
    "            jointPredicates.setdefault(attribute,   (set(),set())    ) \n",
    "            jointPredicates[attribute][0].update(schemaWords)\n",
    "            jointPredicates[attribute][1].update(valueWords)\n",
    "            \n",
    "        jointTupleset = Tupleset(self.table, jointPredicates , jointTuples)\n",
    "        \n",
    "        if changeSources:\n",
    "            self.tuples.difference_update(jointTuples)\n",
    "            otherTupleset.tuples.difference_update(jointTuples)\n",
    "        \n",
    "        return jointTupleset    \n",
    "        \n",
    "    def addValueMapping(self,valueWord,attribute='*'):\n",
    "        self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "        self.predicates[attribute][1].add(valueWord)\n",
    "        \n",
    "    \n",
    "    def addSchemaMapping(self,schemaWord,attribute='*'):\n",
    "        self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "        self.predicates[attribute][0].add(schemaWord)\n",
    "\n",
    "    \n",
    "    def getMappings(self):\n",
    "        return [(self.table,attribute,schemaWords,valueWords) \n",
    "                for attribute, (schemaWords,valueWords) in self.predicates.items()]\n",
    "    \n",
    "    \n",
    "    def getValueMappings(self):\n",
    "        return [(self.table,attribute,valueWords) \n",
    "                for attribute, (schemaWords, valueWords ) in self.predicates.items() \n",
    "                if schemaWords != set()]\n",
    "                \n",
    "    def getSchemaMappings(self): \n",
    "        return [(self.table,attribute,schemaWords) \n",
    "                for attribute, (schemaWords, valueWords ) in self.predicates.items() \n",
    "                if schemaWords != set()]\n",
    "            \n",
    "    def getAttributes(self):\n",
    "        return [attr for attr in self.predicates.keys()]\n",
    "                \n",
    "    def getKeywords(self):\n",
    "        keywords = set()\n",
    "        for attribute in self.predicates.keys():\n",
    "            \n",
    "            schemaWords,valueWords = self.predicates[attribute]\n",
    "            \n",
    "            keywords.update(schemaWords)                      \n",
    "            keywords.update(valueWords)\n",
    "        return frozenset(keywords)\n",
    "        \n",
    "    def isFreeTupleset(self):\n",
    "        return len(self.predicates)==0\n",
    "    \n",
    "    def isValueFreeTupleset(self):\n",
    "        for schemaWords,valueWords in self.predicates.values():\n",
    "            if len(valueWords)>0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def isSchemaFreeTupleset(self):\n",
    "        for schemaWords,valueWords in self.predicates.values():\n",
    "            if len(schemaWords)>0:\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def hasTuples(self):\n",
    "        return len(self.tuples)>0\n",
    "    \n",
    "    def clearTuples(self):\n",
    "        self.tuples.clear()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = self.table.upper()\n",
    "        str_predicates = []\n",
    "        \n",
    "        for attribute in self.predicates.keys():\n",
    "            schemaWords , valueWords = self.predicates[attribute]\n",
    "            \n",
    "            if schemaWords == set():\n",
    "                schemaWords = {}\n",
    "                \n",
    "            if valueWords == set():\n",
    "                valueWords = {}\n",
    "            \n",
    "            \n",
    "            str_predicates.append (attribute + str(schemaWords) + str(valueWords))\n",
    "            \n",
    "        result += \"(\" + ','.join(str_predicates) + \")\"\n",
    "        return result        \n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Tupleset) and self.table == other.table and self.predicates == other.predicates and self.tuples == other.tuples  \n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "    \n",
    "x = Tupleset('paper')\n",
    "x.addValueMapping('discover','title')\n",
    "x.addTuple(1)\n",
    "x.addTuple(2)\n",
    "\n",
    "y = Tupleset('paper')\n",
    "y.addSchemaMapping('2002','title')\n",
    "y.addTuple(1)\n",
    "y.addTuple(3)\n",
    "\n",
    "w = x.union(y,changeSources = True)\n",
    "pp([x,y,w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:45:52.147941Z",
     "start_time": "2019-03-07T17:45:52.143906Z"
    }
   },
   "outputs": [],
   "source": [
    "x = 'PERSON'\n",
    "y = {'attrA':(set(),set()),'attrB':({1,2},set())}\n",
    "p = [(x,attribute,schemaWords) for attribute, (schemaWords, _ ) in y.items() if schemaWords != set()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:45:52.331073Z",
     "start_time": "2019-03-07T17:45:52.150084Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def TSFindClass(Q,wordHash):\n",
    "    #Input:  A keyword query Q=[k1, k2, . . . , km]\n",
    "    #Output: Set of non-free and non-empty tuple-sets Rq\n",
    "\n",
    "    '''\n",
    "    The tuple-set Rki contains the tuples of Ri that contain all\n",
    "    terms of K and no other keywords from Q\n",
    "    '''\n",
    "    \n",
    "    #Part 1: Find sets of tuples containing each keyword\n",
    "    P = set()\n",
    "    for keyword in Q:\n",
    "        \n",
    "        if keyword not in wordHash:\n",
    "            continue\n",
    "        \n",
    "        for table in wordHash[keyword]:\n",
    "            for (attribute,ctids) in wordHash[keyword][table].items():\n",
    "                \n",
    "                ts = Tupleset(table)\n",
    "                ts.addValueMapping(keyword,attribute)\n",
    "                ts.addTuples(ctids)                \n",
    "                P.add(ts)\n",
    "    \n",
    "    #Part 2: Find sets of tuples containing larger termsets\n",
    "    TSInterMartins(P)\n",
    "    \n",
    "    \n",
    "    #Part 3: Clean tuples\n",
    "    for ts in P:\n",
    "        ts.clearTuples()\n",
    "    \n",
    "    \n",
    "    return P\n",
    "\n",
    "# def TSInter(P):\n",
    "#     #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "#     #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     Termset is any non-empty subset K of the terms of a query Q        \n",
    "#     '''\n",
    "    \n",
    "#     Pprev = {}\n",
    "#     Pprev=copy.deepcopy(P)\n",
    "#     Pcurr = {}\n",
    "\n",
    "#     combinations = [x for x in itertools.combinations(Pprev.keys(),2)]\n",
    "#     for ( Ki , Kj ) in combinations:\n",
    "#         Tki = Pprev[Ki]\n",
    "#         Tkj = Pprev[Kj]\n",
    "        \n",
    "#         X = Ki | Kj\n",
    "#         Tx = Tki & Tkj        \n",
    "        \n",
    "#         if len(Tx) > 0:            \n",
    "#             Pcurr[X]  = Tx            \n",
    "#             Pprev[Ki] = Tki - Tx         \n",
    "#             Pprev[Kj] = Tkj - Tx\n",
    "            \n",
    "#     if Pcurr != {}:\n",
    "#         Pcurr = copy.deepcopy(TSInter(Pcurr))\n",
    "        \n",
    "#     #Pprev = Pprev U Pcurr\n",
    "#     Pprev.update(Pcurr)     \n",
    "#     return Pprev   \n",
    "\n",
    "\n",
    "def TSInterMartins(P):\n",
    "    #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "    #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    '''\n",
    "    Termset is any non-empty subset K of the terms of a query Q        \n",
    "    '''\n",
    "    \n",
    "#     print('TSInter\\n')\n",
    "#     pp(P)\n",
    "#     print('\\n====================================\\n')\n",
    "\n",
    "    \n",
    "    for ( Ti , Tj ) in itertools.combinations(P,2):\n",
    "        \n",
    "#         print('\\nTESTANDO UNION {} \\n {} \\n'.format(Ti,Tj))\n",
    "        \n",
    "        \n",
    "#         print('´´´´´´´TSInter\\n')\n",
    "#         pp(P)\n",
    "        \n",
    "        Tx = Ti.union(Tj, changeSources = True)        \n",
    "        \n",
    "#         print('\\nUNION COMPILADO de {} \\n {} \\n {}\\n\\n\\n'.format(Ti,Tj,Tx))\n",
    "        \n",
    "#         if Tx is not None:\n",
    "#             print(len(Tx.tuples), 'tuples on union')\n",
    "            \n",
    "#         print('´´´´´´´TSInter\\n')\n",
    "#         pp(P)    \n",
    "        \n",
    "        \n",
    "        if Tx is not None and Tx.hasTuples():            \n",
    "            P.add(Tx)\n",
    "            \n",
    "            if Ti.hasTuples() == False:\n",
    "#                 print('Ti {} has not tuples',Ti)\n",
    "                P.remove(Ti)\n",
    "#             else:\n",
    "#                 print('{} has {} tuples'.format(Ti,len(Ti.tuples)))\n",
    "                \n",
    "            if Tj.hasTuples() == False:\n",
    "#                 print('Tj {} has not tuples',Tj)\n",
    "                P.remove(Tj)\n",
    "#             else:\n",
    "#                 print('{} has {} tuples'.format(Tj,len(Tj.tuples)))\n",
    "            \n",
    "            TSInterMartins(P)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:45:52.342763Z",
     "start_time": "2019-03-07T17:45:52.332989Z"
    }
   },
   "outputs": [],
   "source": [
    "def getQuerySets(filename='querysets/queryset_dblp_martins.txt'):\n",
    "    QuerySet = []\n",
    "    with open(filename,encoding='utf-8-sig') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            #The line bellow Remove words not in OLIVEIRA experiments\n",
    "            #Q = [word.strip(string.punctuation) for word in line.split() if word not in ['title','dr.',\"here's\",'char','name'] and word not in stw_set]  \n",
    "            \n",
    "            Q = tuple([word.strip(string.punctuation) for word in line.lower().split() if word not in stw_set])\n",
    "            \n",
    "            QuerySet.append(Q)\n",
    "    return QuerySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:45:52.347589Z",
     "start_time": "2019-03-07T17:45:52.344579Z"
    }
   },
   "outputs": [],
   "source": [
    "Q= ['author','datacenter','2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:45:52.587762Z",
     "start_time": "2019-03-07T17:45:52.532064Z"
    }
   },
   "outputs": [],
   "source": [
    "Rq = TSFindClass(Q,wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:45:52.741734Z",
     "start_time": "2019-03-07T17:45:52.734844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PAPER(title{}{'2015', 'author'}),\n",
       " PAPER(title{}{'2015'}),\n",
       " PAPER(title{}{'author'}),\n",
       " PAPER(title{}{'author'},year{}{'2015'}),\n",
       " PAPER(title{}{'datacenter'}),\n",
       " PAPER(title{}{'datacenter'},year{}{'2015'}),\n",
       " PAPER(year{}{'2015'})}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class SchemaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:28:59.800378Z",
     "start_time": "2019-03-07T19:28:59.576845Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint \n",
    "\n",
    "class SchemaGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__graph = {}\n",
    "    \n",
    "    def addRelationship(self,tableA,columnA,tableB, columnB, direction = -1):        \n",
    "        tsA = Tupleset(tableA)\n",
    "        tsB = Tupleset(tableB)\n",
    "        \n",
    "        #A->B\n",
    "        edge_info = (columnA,columnB,direction)\n",
    "        self.__graph.setdefault(tsA,{}).setdefault(tsB,[]).append(edge_info)\n",
    "        \n",
    "        #B<-A\n",
    "        edge_info = (columnB,columnA,direction*-1)\n",
    "        self.__graph.setdefault(tsB,{}).setdefault(tsA,[]).append(edge_info)\n",
    "        \n",
    "    def getEdgeInfos(self,tsA,tsB):        \n",
    "        return self.__graph[tsA][tsB]        \n",
    "        \n",
    "    def copyRelationships(self,sourceNode,targetNode):\n",
    "        # target->neighbours    =    source->neighbours\n",
    "        \n",
    "        #print('cpRelations s: {} t: {}'.format(sourceNode,targetNode))\n",
    "        \n",
    "        self.__graph[targetNode] = copy.deepcopy(self.__graph[sourceNode])\n",
    "            \n",
    "        \n",
    "        # neighbours->target    =    neighbours->source\n",
    "        for neighbourNode in self.__graph[targetNode]:\n",
    "            for node, edge_infos in self.__graph[neighbourNode].items():\n",
    "                if node == sourceNode:\n",
    "                    self.__graph[neighbourNode][targetNode] = edge_infos\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "    def getMatchGraph(self,Match):\n",
    "        \n",
    "        Gts = copy.deepcopy(self)\n",
    "        \n",
    "        for ts in Match:\n",
    "            Gts.copyRelationships(Tupleset(ts.table),ts)\n",
    "            \n",
    "        return Gts\n",
    "    \n",
    "    def getByTableName(self,tableName):\n",
    "        return self.__graph[Tupleset(tableName)]\n",
    "    \n",
    "    def tables(self):\n",
    "        return self.__graph.keys()\n",
    "        \n",
    "    def getAdjacentTables(self, table, sort = False):\n",
    "        \n",
    "        if not sort:\n",
    "            return self.getByTableName(table).keys()\n",
    "        else:\n",
    "            # Sorting adjacents with non free tuple sets first\n",
    "            return sorted(self.getByTableName(table).keys(),key=lambda ts : ts.isFreeTupleset() )\n",
    "        \n",
    "    def isJNTSound(self,Ji):\n",
    "        if len(Ji)<3:\n",
    "            return True\n",
    "        \n",
    "        #check if there is a case A->B<-C, when A.table=C.table\n",
    "        \n",
    "        for i in range(len(Ji)-2):\n",
    "            tsA = Ji[i]\n",
    "            tsB = Ji[i+1]\n",
    "            tsC = Ji[i+2]\n",
    "            \n",
    "            if tsA.table == tsB.table:\n",
    "                            \n",
    "                for edge_info in self.__graph[tsA][tsB]:\n",
    "                    (columnA,columnB,direction) = edge_info\n",
    "                    \n",
    "                    if direction == 1:\n",
    "                        return False\n",
    "        return True    \n",
    "    \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return pprint.pformat(self.__graph)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self.__graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:28:59.825204Z",
     "start_time": "2019-03-07T19:28:59.803239Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSchemaGraph(dbname='dblp',user='imdb',password='imdb'):\n",
    "    #Output: A Schema Graph G  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "    \n",
    "    G = SchemaGraph()\n",
    "    with psycopg2.connect(dbname=dbname,user=user,password=password) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                sql = \"SELECT DISTINCT tc.table_name, kcu.column_name, ccu.table_name AS foreign_table_name, ccu.column_name AS foreign_column_name FROM information_schema.table_constraints AS tc              JOIN information_schema.key_column_usage AS kcu                 ON tc.constraint_name = kcu.constraint_name             JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY'\"\n",
    "                cur.execute(sql)\n",
    "                relations = cur.fetchall()\n",
    "\n",
    "                for (table,column,foreign_table,foreign_column) in relations:\n",
    "                    #print('table,column,foreign_table,foreign_column\\n{}, {}, {}, {}'.format(table,column,foreign_table,foreign_column))\n",
    "                    G.addRelationship(table,column,foreign_table,foreign_column)  \n",
    "                print ('SCHEMA CREATED')          \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:28:59.955670Z",
     "start_time": "2019-03-07T19:28:59.917805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEMA CREATED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)]},\n",
       " PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
       "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)]},\n",
       " CITATION(): {PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]}}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G=getSchemaGraph()\n",
    "\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:00.852053Z",
     "start_time": "2019-03-07T19:29:00.295403Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "class Similarities:\n",
    "    \n",
    "    def __init__(self, model, attributeHash,schemaGraph):\n",
    "\n",
    "        self.model = model\n",
    "        self.attributeHash = attributeHash\n",
    "        self.schemaGraph = schemaGraph\n",
    "        \n",
    "        self.loadEmbeddingHashes()\n",
    "    \n",
    "    \n",
    "    def wordnet_similarity(self,wordA,wordB):\n",
    "        A = set(wn.synsets(wordA))\n",
    "        B = set(wn.synsets(wordB))\n",
    "\n",
    "        wupSimilarities = [0]\n",
    "        pathSimilarities = [0]\n",
    "        \n",
    "        for (sense1,sense2) in itertools.product(A,B):        \n",
    "            wupSimilarities.append(wn.wup_similarity(sense1,sense2) or 0)\n",
    "            pathSimilarities.append(wn.path_similarity(sense1,sense2) or 0)\n",
    "            \n",
    "        return max(max(wupSimilarities),max(pathSimilarities))\n",
    "\n",
    "    def jaccard_similarity(self,wordA,wordB):\n",
    "\n",
    "        A = set(wordA)\n",
    "        B = set(wordB)\n",
    "\n",
    "        return len(A & B ) / len(A | B)\n",
    "    \n",
    "    \n",
    "    def embedding10_similarity(self,word,table,column='*',Emb='B'):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        \n",
    "        # Os sinônimos do EmbA também são utilizados por todos\n",
    "        sim_list = self.EmbA[table][column]\n",
    "        \n",
    "        if column != '*':\n",
    "        \n",
    "            if Emb == 'B':\n",
    "                sim_list |= self.EmbB[table][column]\n",
    "\n",
    "            elif Emb == 'C':\n",
    "                sim_list |= self.EmbC[table][column]\n",
    "\n",
    "        return wnl.lemmatize(word) in sim_list\n",
    "    \n",
    "    \n",
    "    def embedding_similarity(self,wordA,wordB):\n",
    "        if wordA not in self.model or wordB not in self.model:\n",
    "            return 0\n",
    "        return self.model.similarity(wordA,wordB)\n",
    "    \n",
    "    \n",
    "    def word_similarity(self,word,table,column = '*',\n",
    "                    wn_sim=True, \n",
    "                    jaccard_sim=True,\n",
    "                    emb_sim=False,\n",
    "                    emb10_sim='B'):\n",
    "        sim_list=[0]\n",
    "    \n",
    "        if column == '*':\n",
    "            schema_term = table\n",
    "        else:\n",
    "            schema_term = column\n",
    "\n",
    "        if wn_sim:\n",
    "            sim_list.append( self.wordnet_similarity(schema_term,word) )\n",
    "\n",
    "        if jaccard_sim:\n",
    "            sim_list.append( self.jaccard_similarity(schema_term,word) )\n",
    "\n",
    "        if emb_sim:\n",
    "            sim_list.append( self.embedding_similarity(schema_term,word) )\n",
    "\n",
    "        sim = max(sim_list) \n",
    "\n",
    "        if emb10_sim:\n",
    "            if self.embedding10_similarity(word,table,column,emb10_sim):\n",
    "                if len(sim_list)==1:\n",
    "                    sim=1\n",
    "            else:\n",
    "                sim=0\n",
    "                \n",
    "        print('sim({},{}.{}) = {}'.format(word,table,column,sim))        \n",
    "        \n",
    "        return sim    \n",
    "    \n",
    "    def __getSimilarSet(self,word, inputType = 'word'):\n",
    "        if inputType == 'vector':\n",
    "            sim_list = model.similar_by_vector(word)\n",
    "        else:\n",
    "            sim_list = model.most_similar(word)        \n",
    "        return  {word.lower() for word,sim in sim_list}\n",
    "    \n",
    "    def loadEmbeddingHashes(self,weight=0.5):\n",
    "        \n",
    "        self.EmbA = {}\n",
    "        self.EmbB = {}\n",
    "        self.EmbC = {}\n",
    "    \n",
    "        for table in self.attributeHash:\n",
    "\n",
    "            if table not in self.model:\n",
    "                continue\n",
    "\n",
    "            self.EmbA[table]={}\n",
    "            self.EmbB[table]= {}\n",
    "            self.EmbC[table]= {}\n",
    "            \n",
    "            self.EmbA[table]['*'] = self.__getSimilarSet(table) \n",
    "\n",
    "            for column in self.attributeHash[table]:\n",
    "                if column not in model or column=='id':\n",
    "                    continue\n",
    "                \n",
    "                self.EmbA[table][column]=self.__getSimilarSet(column)\n",
    "                \n",
    "                self.EmbB[table][column]=self.__getSimilarSet( (table,column) )\n",
    "                  \n",
    "                avg_vec = (model[table]*weight + model[column]*(1-weight))                   \n",
    "                self.EmbC[table][column] = self.__getSimilarSet(avg_vec, inputType = 'vector')\n",
    "                \n",
    "                \n",
    "                \n",
    "        G = self.schemaGraph\n",
    "        for tableA in G.tables():\n",
    "\n",
    "            if tableA not in self.attributeHash or tableA not in model:\n",
    "                continue\n",
    "\n",
    "            for tableB in G.getAdjacentTables(tableA):\n",
    "\n",
    "                if tableB not in self.attributeHash or tableB not in model:\n",
    "                    continue\n",
    "\n",
    "                self.EmbB[tableB][tableA] = self.EmbB[tableA][tableB] = self.__getSimilarSet( (tableA,tableB) )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:00.873572Z",
     "start_time": "2019-03-07T19:29:00.853759Z"
    }
   },
   "outputs": [],
   "source": [
    "def SchSFind(Q,attributeHash,threshold=0.8, \n",
    "             sim_args={}):    \n",
    "    S = set()\n",
    "    \n",
    "    sm = Similarities(model,ah,G)\n",
    "    \n",
    "    for keyword in Q:\n",
    "        for table in attributeHash:            \n",
    "            for attribute in ['*']+list(attributeHash[table].keys()):\n",
    "                \n",
    "                if(attribute=='id'):\n",
    "                    continue\n",
    "                \n",
    "                sim = sm.word_similarity(keyword,table,attribute,**sim_args)\n",
    "                \n",
    "                if sim >= threshold:\n",
    "                    ts = Tupleset(table)\n",
    "                    ts.addSchemaMapping(keyword,attribute)\n",
    "                    S.add(ts)\n",
    "                    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:01.893978Z",
     "start_time": "2019-03-07T19:29:00.875517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(author,paper.*) = 0\n",
      "sim(author,paper.conference) = 0\n",
      "sim(author,paper.title) = 0\n",
      "sim(author,paper.year) = 0\n",
      "sim(author,author.*) = 1.0\n",
      "sim(author,author.name) = 0.631578947368421\n",
      "sim(datacenter,paper.*) = 0\n",
      "sim(datacenter,paper.conference) = 0\n",
      "sim(datacenter,paper.title) = 0\n",
      "sim(datacenter,paper.year) = 0\n",
      "sim(datacenter,author.*) = 0\n",
      "sim(datacenter,author.name) = 0\n",
      "sim(2015,paper.*) = 0\n",
      "sim(2015,paper.conference) = 0\n",
      "sim(2015,paper.title) = 0\n",
      "sim(2015,paper.year) = 0\n",
      "sim(2015,author.*) = 0\n",
      "sim(2015,author.name) = 0\n"
     ]
    }
   ],
   "source": [
    "Sq = SchSFind(Q,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:01.899234Z",
     "start_time": "2019-03-07T19:29:01.895831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AUTHOR(*{'author'}{})}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:01.919808Z",
     "start_time": "2019-03-07T19:29:01.900766Z"
    }
   },
   "outputs": [],
   "source": [
    "def MinimalCover(MC, Q):\n",
    "    #Input:  A subset MC (Match Candidate) to be checked as total and minimal cover\n",
    "    #Output: If the match candidate is a TOTAL and MINIMAL cover\n",
    "\n",
    "    Subset = [ts.getKeywords() for ts in MC]\n",
    "    u = set().union(*Subset)    \n",
    "    \n",
    "    isTotal = (u == set(Q))\n",
    "    for element in Subset:\n",
    "        \n",
    "        new_u = list(Subset)\n",
    "        new_u.remove(element)\n",
    "        \n",
    "        new_u = set().union(*new_u)\n",
    "        \n",
    "        if new_u == set(Q):\n",
    "            return False\n",
    "    \n",
    "    #print('MC({},{}) = {}'.format(MC,Q,isTotal))\n",
    "    \n",
    "    return isTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:01.964404Z",
     "start_time": "2019-03-07T19:29:01.921716Z"
    }
   },
   "outputs": [],
   "source": [
    "def QMGen(Q,Rq):\n",
    "    #Input:  A keyword query Q, The set of non-empty non-free tuple-sets Rq\n",
    "    #Output: The set Mq of query matches for Q\n",
    "    \n",
    "    '''\n",
    "    Query match is a set of tuple-sets that, if properly joined,\n",
    "    can produce networks of tuples that fulfill the query. They\n",
    "    can be thought as the leaves of a Candidate Network.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Mq = []\n",
    "    for i in range(1,len(Q)+1):\n",
    "        for subset in itertools.combinations(Rq,i):            \n",
    "            if(MinimalCover(subset,Q)):\n",
    "                print('----------------------------------------------\\nM')\n",
    "                pp(set(subset))\n",
    "                print('\\n')\n",
    "                \n",
    "                M = MInter(set(subset))\n",
    "                print('subset')\n",
    "                pp(M)\n",
    "                Mq.append(M)\n",
    "                \n",
    "                \n",
    "    return Mq\n",
    "\n",
    "def MInter(M):  \n",
    "    somethingChanged = False    \n",
    "    for tsA, tsB  in itertools.combinations(M,2):\n",
    "        \n",
    "        tsX = tsA.union(tsB, projectionOnly = True)\n",
    "\n",
    "        if tsX is not None:\n",
    "            \n",
    "            M.add(tsX)      \n",
    "            M.remove(tsA)\n",
    "            M.remove(tsB)\n",
    "            \n",
    "            return MInter(M)\n",
    "        \n",
    "    return M   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:01.969865Z",
     "start_time": "2019-03-07T19:29:01.966097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PAPER(title{}{'2015', 'author'}),\n",
       " PAPER(title{}{'2015'}),\n",
       " PAPER(title{}{'author'}),\n",
       " PAPER(title{}{'author'},year{}{'2015'}),\n",
       " PAPER(title{}{'datacenter'}),\n",
       " PAPER(title{}{'datacenter'},year{}{'2015'}),\n",
       " PAPER(year{}{'2015'})}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:01.976075Z",
     "start_time": "2019-03-07T19:29:01.971776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AUTHOR(*{'author'}{})}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:01.997166Z",
     "start_time": "2019-03-07T19:29:01.979087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), AUTHOR(*{'author'}{})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), AUTHOR(*{'author'}{})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), PAPER(title{}{'author'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), PAPER(title{}{'author'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author'},year{}{'2015'}),\n",
      " PAPER(title{}{'datacenter'},year{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author'},year{}{'2015'}),\n",
      " PAPER(title{}{'datacenter'},year{}{'2015'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), PAPER(title{}{'2015', 'author'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), PAPER(title{}{'2015', 'author'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author'},year{}{'2015'}), PAPER(title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author'},year{}{'2015'}), PAPER(title{}{'datacenter'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'datacenter'}), PAPER(title{}{'2015', 'author'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'datacenter'}), PAPER(title{}{'2015', 'author'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(title{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(title{}{'2015'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author'}),\n",
      " PAPER(title{}{'datacenter'}),\n",
      " PAPER(title{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author'}),\n",
      " PAPER(title{}{'datacenter'}),\n",
      " PAPER(title{}{'2015'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author'}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author'}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n"
     ]
    }
   ],
   "source": [
    "Mq = QMGen(Q,Rq|Sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:02.008020Z",
     "start_time": "2019-03-07T19:29:01.999119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), AUTHOR(*{'author'}{})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), PAPER(title{}{'author'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'author'},year{}{'2015'}),\n",
      " PAPER(title{}{'datacenter'},year{}{'2015'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'datacenter'},year{}{'2015'}), PAPER(title{}{'2015', 'author'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'author'},year{}{'2015'}), PAPER(title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'datacenter'}), PAPER(title{}{'2015', 'author'})}\n",
      "\n",
      "\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(title{}{'2015'})}\n",
      "\n",
      "\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'author'}),\n",
      " PAPER(title{}{'datacenter'}),\n",
      " PAPER(title{}{'2015'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'author'}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for M in Mq:\n",
    "    pp(M)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:02.121889Z",
     "start_time": "2019-03-07T19:29:02.019430Z"
    }
   },
   "outputs": [],
   "source": [
    "def QMRank(Mq, wordHash,attributeHash):\n",
    "    Ranking = []  \n",
    "    sm = Similarities(model,ah,G)\n",
    "    for M in Mq:\n",
    "        #print('=====================================\\n')\n",
    "        valueProd = 1 \n",
    "        schemaProd = 1\n",
    "        score = 1\n",
    "        \n",
    "        thereIsSchemaTerms = False\n",
    "        thereIsValueTerms = False\n",
    "        \n",
    "        for ts in M:\n",
    "            #print(ts)\n",
    "            for table, attribute, valueWords in ts.getValueMappings():\n",
    "                #print('t{} a{} v{}'.format(table,attribute,valueWords))             \n",
    "                \n",
    "                (Norm,numDistinctWords,numWords,maxFrequency) = attributeHash[table][attribute]                \n",
    "                wsum = 0\n",
    "                for term in valueWords:\n",
    "                \n",
    "                    #print('t{} a{} vt{}'.format(table,attribute,term))\n",
    "                \n",
    "                    IAF = wordHash.getIAF(term)\n",
    "                    \n",
    "                    frequency = len(wordHash.getMappings(term,table,attribute))\n",
    "                    TF = frequency/maxFrequency\n",
    "                    wsum = wsum + TF*IAF\n",
    "    \n",
    "                    thereIsValueTerms = True\n",
    "                \n",
    "                cos = wsum/Norm\n",
    "                valueProd *= cos\n",
    "                \n",
    "                \n",
    "            for table, attribute, schemaWords in ts.getSchemaMappings():\n",
    "                schemasum = 0\n",
    "                for term in schemaWords:\n",
    "                    sim = sm.word_similarity(term,table,attribute)\n",
    "                    schemasum += sim\n",
    "                    \n",
    "                    thereIsSchemaTerms = True\n",
    "                    \n",
    "                schemaProd *= schemasum           \n",
    "        \n",
    "        valueScore  = valueProd\n",
    "        schemaScore = schemaProd\n",
    "        \n",
    "        if thereIsValueTerms:\n",
    "            score *= valueScore\n",
    "        else:\n",
    "            valueScore = 0\n",
    "            \n",
    "            \n",
    "        if thereIsSchemaTerms:\n",
    "            score *= schemaScore\n",
    "        else:\n",
    "            schemaScore = 0\n",
    "                \n",
    "        Ranking.append( (M,score,valueScore,schemaScore) )\n",
    "                \n",
    "    return sorted(Ranking,key=lambda x: x[1],reverse=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:03.153608Z",
     "start_time": "2019-03-07T19:29:02.180667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(author,author.*) = 1.0\n",
      "sim(author,author.*) = 1.0\n",
      "sim(author,author.*) = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'},year{}{'2015'})},\n",
       "  2.055107000624585e-06,\n",
       "  2.055107000624585e-06,\n",
       "  1.0),\n",
       " ({AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})},\n",
       "  2.055107000624585e-06,\n",
       "  2.055107000624585e-06,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'2015', 'author'}), PAPER(title{}{'datacenter'})},\n",
       "  2.606678138449735e-09,\n",
       "  2.606678138449735e-09,\n",
       "  0),\n",
       " ({AUTHOR(*{'author'}{}),\n",
       "   PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  1.2289719975556887e-09,\n",
       "  1.2289719975556887e-09,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'2015', 'author'}),\n",
       "   PAPER(title{}{'datacenter'},year{}{'2015'})},\n",
       "  1.867282374786789e-10,\n",
       "  1.867282374786789e-10,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'}),\n",
       "   PAPER(year{}{'2015'})},\n",
       "  9.869137108185343e-11,\n",
       "  9.869137108185343e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}), PAPER(title{}{'datacenter'},year{}{'2015'})},\n",
       "  9.869137108185342e-11,\n",
       "  9.869137108185342e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'},year{}{'2015'}), PAPER(title{}{'datacenter'})},\n",
       "  9.869137108185342e-11,\n",
       "  9.869137108185342e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'},year{}{'2015'}),\n",
       "   PAPER(title{}{'datacenter'},year{}{'2015'})},\n",
       "  7.069712790635778e-12,\n",
       "  7.069712790635778e-12,\n",
       "  0),\n",
       " ({PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  5.901830484890236e-14,\n",
       "  5.901830484890236e-14,\n",
       "  0)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RankedMq = QMRank(Mq,wh,ah)\n",
    "\n",
    "RankedMq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:03.167252Z",
     "start_time": "2019-03-07T19:29:03.158963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)]},\n",
       " PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
       "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)]},\n",
       " CITATION(): {PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]}}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:03.180331Z",
     "start_time": "2019-03-07T19:29:03.173125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'},year{}{'2015'})}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = RankedMq[0][0]\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:03.186216Z",
     "start_time": "2019-03-07T19:29:03.182957Z"
    }
   },
   "outputs": [],
   "source": [
    "Gts = G.getMatchGraph(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:03.304662Z",
     "start_time": "2019-03-07T19:29:03.188302Z"
    }
   },
   "outputs": [],
   "source": [
    "from queue import deque\n",
    "def SingleCN(FM,Gts,TMax,showLog=False):  \n",
    "  \n",
    "    if showLog:\n",
    "        print('================================================================================\\nSINGLE CN')\n",
    "        print('Tmax ',TMax)\n",
    "        print('FM')\n",
    "        pp(FM)\n",
    "        \n",
    "        #print('\\n\\nGts')\n",
    "        #pp(Gts)\n",
    "        #print('\\n\\n')\n",
    "    \n",
    "    F = deque()\n",
    "\n",
    "    first_element = list(FM)[0]\n",
    "    J = [first_element]\n",
    "    \n",
    "    if len(FM)==1:\n",
    "        return J\n",
    "    \n",
    "    F.append(J)\n",
    "    \n",
    "    while F:\n",
    "        J = F.popleft()           \n",
    "        tsu = J[-1]\n",
    "        \n",
    "        sortedAdjacents = Gts.getAdjacentTables(tsu.table,sort = True)\n",
    "        \n",
    "        if showLog:\n",
    "            print('--------------------------------------------\\nParctial CN')\n",
    "            print('J ',J,'\\n')\n",
    "\n",
    "            print('\\nAdjacents:')\n",
    "            pp(Gts.getAdjacentTables(tsu.table))\n",
    "            \n",
    "            print('\\nSorted Adjacents:')\n",
    "            pp(sortedAdjacents)\n",
    "            \n",
    "            print('F:')\n",
    "            pp(F)\n",
    "        \n",
    "        for tsv in sortedAdjacents:\n",
    "            \n",
    "            if showLog:\n",
    "                print('Checking adj:')\n",
    "                pp(tsv)\n",
    "                print()\n",
    "\n",
    "            if (tsv.isFreeTupleset()) or (tsv not in J):\n",
    "                \n",
    "                Ji = J + [tsv]\n",
    "                \n",
    "                if (Ji not in F) and (len(Ji)<TMax) and (Gts.isJNTSound(Ji)):\n",
    "                    \n",
    "                    if showLog:\n",
    "                        print('isSound=True')\n",
    "                    \n",
    "                    containsMatch = True\n",
    "                    for ts in FM:\n",
    "                        if ts not in Ji:\n",
    "                            containsMatch = False    \n",
    "                            \n",
    "                    if containsMatch:\n",
    "                        if showLog:\n",
    "                            print('--------------------------------------------\\nGenerated CN')\n",
    "                            print('J ',Ji,'\\n')\n",
    "                        \n",
    "                        return Ji\n",
    "                    else:\n",
    "                        F.append(Ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:03.325699Z",
     "start_time": "2019-03-07T19:29:03.306571Z"
    }
   },
   "outputs": [],
   "source": [
    "def MatchCN(G,Sq,Rq,RankedMq,TMax=5):    \n",
    "    Cns = []                        \n",
    "    for  (M,score,schemascore,valuescore) in RankedMq:\n",
    "\n",
    "        Gts = G.getMatchGraph(M)\n",
    "        Cn = SingleCN(M,Gts,TMax=TMax)\n",
    "        if(Cn is not None):\n",
    "            \n",
    "            \n",
    "            #Dividindo score pelo tamanho da cn (SEGUNDA PARTE DO RANKING)\n",
    "            \n",
    "            CnScore = score/len(Cn)\n",
    "            \n",
    "            Cns.append( (Cn,Gts,CnScore,schemascore,valuescore) )\n",
    "    \n",
    "    #Ordena CNs pelo CnScore\n",
    "    RankedCns=sorted(Cns,key=lambda x: x[3],reverse=True)\n",
    "    \n",
    "    return RankedCns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:40:52.994367Z",
     "start_time": "2019-03-07T17:40:52.987533Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:29:03.341707Z",
     "start_time": "2019-03-07T19:29:03.327334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAPER(title{}{'datacenter'},year{}{'2015'}), AUTHOR(*{'author'}{})]\n"
     ]
    }
   ],
   "source": [
    "Cns = MatchCN(G,Sq,Rq,RankedMq)\n",
    "for (Cn,Gts,CnScore,schemascore,valuescore) in Cns:\n",
    "    pp(Cn)\n",
    "    \n",
    "    x = Cn\n",
    "    y = Gts\n",
    "    break\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getSQLfromCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:35:58.473460Z",
     "start_time": "2019-03-07T19:35:58.279099Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSQLfromCN(Gts,Cn,contract=True):\n",
    "    selected_attributes = [] \n",
    "    hashTables = {}\n",
    "    conditions=[]\n",
    "    relationships = set()\n",
    "    \n",
    "    tables_id=[]\n",
    "    tables=[]\n",
    "    joincondiditions=[]\n",
    "    \n",
    "    for i in range(len(Cn)):\n",
    "        \n",
    "        tsA = Cn[i]\n",
    "               \n",
    "        A = 't' + str(i)\n",
    "        \n",
    "        if contract and tsA.isFreeTupleset():\n",
    "            A = hashTables.setdefault(tsA.table,[A])[0]\n",
    "        else:\n",
    "            hashTables.setdefault(tsA.table, []).append(A)            \n",
    "        \n",
    "        for attr in tsA.getAttributes():\n",
    "            selected_attributes.append(A +'.'+ attr)\n",
    "        \n",
    "        for table, attr, valueWords in tsA.getValueMappings():\n",
    "            #tratamento de keywords\n",
    "            for term in valueWords:\n",
    "                condition = 'CAST('+A +'.'+ attr + ' AS VARCHAR) ILIKE \\'%' + term + '%\\''\n",
    "                conditions.append(condition)\n",
    "        \n",
    "        \n",
    "        #tratamento de join paths\n",
    "        if (i>0):\n",
    "            # B se refere ao tupleset anterior                \n",
    "            tsB = Cn[i-1]\n",
    "            \n",
    "            # B vai receber o último valor de tx adicionado em hashTables[tableB]\n",
    "            B = hashTables[tsB.table][-1]\n",
    "            \n",
    "            for joining_attrA,joining_attrB, direction in Gts.getEdgeInfos(tsA,tsB):            \n",
    "                \n",
    "                joincondiditions.append(A + '.' + joining_attrA + ' = ' + B + '.' + joining_attrB)\n",
    "                relationships.add( frozenset([B,A]) ) \n",
    "    \n",
    "    for tableX in hashTables.keys():\n",
    "        for tx in hashTables[tableX]:\n",
    "            tables_id.append(tx+'.__search_id')\n",
    "            tables.append(tableX+' '+tx)\n",
    "            \n",
    "        \n",
    "    relationshipsText = ['('+a+'.__search_id'+','+b+'.__search_id'+')' for (a,b) in relationships]\n",
    "    \n",
    "    sqlText = 'SELECT \\n '\n",
    "#     sqlText +=' ('+', '.join(tables_id)+') AS Tuples,\\n '\n",
    "#     if len(relationships)>0:\n",
    "#         sqlText +='('+', '.join(relationshipsText)+') AS Relationships,\\n '\n",
    "        \n",
    "    sqlText += ' ,\\n '.join(selected_attributes)\n",
    "    \n",
    "    sqlText +='\\nFROM\\n ' + ',\\n '.join(tables)\n",
    "    \n",
    "    sqlText +='\\nWHERE\\n '\n",
    "    \n",
    "    # Considerando que todas as pequisas tem ao menos um value term\n",
    "    if  len(conditions)==0:\n",
    "        sqlText+= ' 1=2'\n",
    "        return sqlText\n",
    "    \n",
    "    sqlText +='\\n AND '.join(joincondiditions)\n",
    "    sqlText +='\\n'\n",
    "    if len(joincondiditions)>0:\n",
    "        sqlText +='\\n AND '\n",
    "    sqlText +='\\n AND '.join(conditions)\n",
    "    \n",
    "    \n",
    "    #Considerando que nenhuma consulta tem mais de 1000 linhas no resultado\n",
    "    sqlText += '\\n LIMIT 1000'\n",
    "    \n",
    "    sqlText += ';'\n",
    "    '''\n",
    "    print('SELECT:\\n',selected_attributes)\n",
    "    print('TABLES:\\n',hashTables)\n",
    "    print('CONDITIONS:')\n",
    "    pp(conditions)\n",
    "    print('RELATIONSHIPS:')\n",
    "    pp(relationships)\n",
    "    '''    \n",
    "    #print('SQL:\\n',sql)\n",
    "    return sqlText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:35:58.514127Z",
     "start_time": "2019-03-07T19:35:58.509427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAPER(title{}{'datacenter'},year{}{'2015'}), AUTHOR(*{'author'}{})]\n"
     ]
    }
   ],
   "source": [
    "pp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:35:58.780957Z",
     "start_time": "2019-03-07T19:35:58.775012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      " t0.title ,\n",
      " t0.year ,\n",
      " t1.*\n",
      "FROM\n",
      " paper t0,\n",
      " author t1\n",
      "WHERE\n",
      " t1.paper_key = t0.paper_key\n",
      "\n",
      " AND CAST(t0.title AS VARCHAR) ILIKE '%datacenter%'\n",
      " AND CAST(t0.year AS VARCHAR) ILIKE '%2015%'\n",
      " LIMIT 1000;\n"
     ]
    }
   ],
   "source": [
    "sql = getSQLfromCN(y,x,contract=True)\n",
    "\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T19:21:59.668094Z",
     "start_time": "2019-03-07T19:21:59.661583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
       "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)]},\n",
       " AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)]},\n",
       " CITATION(): {PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatCNGenpy",
   "language": "python",
   "name": "matcngenpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
