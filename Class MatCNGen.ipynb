{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "import gc #garbage collector usado no createinvertedindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def loadWordEmbeddingsModel(filename = \"word_embeddings/word2vec/GoogleNews-vectors-negative300.bin\"):\n",
    "    model = KeyedVectors.load_word2vec_format(filename,\n",
    "                                                       binary=True, limit=500000)\n",
    "    return model\n",
    "\n",
    "model = loadWordEmbeddingsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabelItemsIter:\n",
    "    def __init__(self,babelhash):\n",
    "        __slots__ = ('__babelhash')\n",
    "        self.__babelhash = babelhash    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__babelhash)\n",
    "    \n",
    "    def __contains__(self,item):\n",
    "        (key,value) = item\n",
    "        return key in self.__babelhash and self.__babelhash[key]==value\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for key in self.__babelhash.keys():\n",
    "            yield key, self.__babelhash[key]\n",
    "            \n",
    "    #Apesar de que segundo o PEP 3106 (https://www.python.org/dev/peps/pep-3106/) recomenda que façamos\n",
    "    # outros métodos, como and,eq,ne para permitir que a saída seja um set,\n",
    "    # não estamos preocupados com isso aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabelHash(dict):\n",
    "    \n",
    "    def __init__(self,babel={}):\n",
    "        __slots__ = ('__babel')\n",
    "        dict.__init__(self)\n",
    "        self.__babel = babel\n",
    "        \n",
    "    def __getidfromkey__(self,key):\n",
    "        return self.__babel[key]\n",
    "    \n",
    "    def __getkeyfromid__(self,key_id):\n",
    "        key = self.__babel[key_id]\n",
    "        return key\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        return dict.__getitem__(self,key_id)\n",
    "    \n",
    "    def __setitem__(self,key,value):    \n",
    "        try:\n",
    "            key_id = self.__babel[key]\n",
    "        except KeyError:\n",
    "            key_id = len(self.__babel)+1\n",
    "                     \n",
    "            self.__babel[key] = key_id\n",
    "            self.__babel[key_id] = key\n",
    "        \n",
    "        dict.__setitem__(self, key_id,value)\n",
    "    \n",
    "    def __delitem__(self, key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        dict.__delitem__(self, key_id)\n",
    "        \n",
    "    def __missing__(self,key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        return key_id\n",
    "        \n",
    "    def __delitem__(self, key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        dict.__delitem__(self,key_id)\n",
    "    \n",
    "    def __contains__(self, key):\n",
    "        try:\n",
    "            key_id = self.__getidfromkey__(key)\n",
    "        except KeyError:\n",
    "            return False\n",
    "        \n",
    "        return dict.__contains__(self,key_id)    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for key_id in dict.keys(self):\n",
    "            yield self.__getkeyfromid__(key_id)\n",
    "    \n",
    "    def keys(self):\n",
    "        for key_id in dict.keys(self):\n",
    "            yield self.__getkeyfromid__(key_id)\n",
    "    \n",
    "    def items(self):\n",
    "        return BabelItemsIter(self)\n",
    "    \n",
    "    def get(self,key):\n",
    "        value = None\n",
    "        if key in self:\n",
    "            value = self.__getitem__(key)\n",
    "        return value\n",
    "    \n",
    "    def setdefault(self,key,default=None):\n",
    "        if key not in self:\n",
    "            self[key]=default\n",
    "        return self[key]\n",
    "    \n",
    "    def printBabel(self):\n",
    "        print(self.__babel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordHash(dict):      \n",
    "        \n",
    "    def __init__(self): \n",
    "        dict.__init__(self)\n",
    "    \n",
    "    def addMapping(self,word,table,attribute,ctid):\n",
    "        self.setdefault( word, (0, BabelHash() ) )                    \n",
    "        self[word].setdefault(table , BabelHash() )       \n",
    "        self[word][table].setdefault( attribute , [] ).append(ctid)        \n",
    "        \n",
    "    def getMappings(self,word,table,attribute):\n",
    "        return self[word][table][attribute]\n",
    "    \n",
    "    def getIAF(self,key):\n",
    "        return dict.__getitem__(self,key)[0]\n",
    "    \n",
    "    def setIAF(self,key,IAF):\n",
    "\n",
    "        oldIAF,oldValue = dict.__getitem__(self,key)\n",
    "        \n",
    "        dict.__setitem__(self, key,  (IAF,oldValue)  )\n",
    "    \n",
    "    def __getitem__(self,word):\n",
    "        return dict.__getitem__(self,word)[1]\n",
    "    \n",
    "    def __setitem__(self,word,value): \n",
    "        oldIAF,oldValue = dict.__getitem__(self,word)\n",
    "        dict.__setitem__(self, word,  (oldIAF,value)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "class DatabaseIter:\n",
    "    def __init__(self,embeddingModel,dbname='dblp',user='imdb',password='imdb'):\n",
    "        self.dbname=dbname\n",
    "        self.user=user\n",
    "        self.password =password\n",
    "        self.embeddingModel=embeddingModel\n",
    "\n",
    "    def __iter__(self):\n",
    "        with psycopg2.connect(dbname=self.dbname,user=self.user,password=self.password) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "\n",
    "                # Get list of tablenames\n",
    "\n",
    "                GET_TABLE_NAMES_SQL='''\n",
    "                    SELECT DISTINCT table_name\n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_schema='public';\n",
    "                ''' \n",
    "                cur.execute(GET_TABLE_NAMES_SQL)\n",
    "\n",
    "                tables = cur.fetchall()\n",
    "                print(tables)\n",
    "                for table in tables:\n",
    "                    table_name = table[0]\n",
    "\n",
    "                    if table_name not in self.embeddingModel:\n",
    "                        print('TABLE ',table_name, 'SKIPPED')\n",
    "                        continue\n",
    "\n",
    "                    print('INDEXING TABLE ',table_name)\n",
    "\n",
    "                    #Get all tuples for this tablename\n",
    "                    cur.execute(\n",
    "                        sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "                        #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "                    )\n",
    "\n",
    "                    printSkippedColumns = True\n",
    "\n",
    "                    for row in cur.fetchall(): \n",
    "                        for column in range(1,len(row)):\n",
    "                            column_name = cur.description[column][0] \n",
    "\n",
    "                            if column_name not in self.embeddingModel or column_name=='id':\n",
    "                                if printSkippedColumns:\n",
    "                                    print('\\tCOLUMN ',column_name,' SKIPPED')\n",
    "                                continue\n",
    "\n",
    "                            ctid = row[0]\n",
    "\n",
    "                            for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "\n",
    "                                #Ignoring STOPWORDS\n",
    "                                if word in stw_set:\n",
    "                                    continue\n",
    "\n",
    "                                yield table_name,ctid,column_name, word\n",
    "\n",
    "                        printSkippedColumns=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "def createInvertedIndex(embeddingModel,dbname='dblp',user='imdb',password='imdb',showLog=True):\n",
    "    #Output: wordHash (Term Index) with this structure below\n",
    "    #map['word'] = [ 'table': ( {column} , ['ctid'] ) ]\n",
    "\n",
    "    '''\n",
    "    The Term Index is built in a preprocessing step that scans only\n",
    "    once all the relations over which the queries will be issued.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    wh = WordHash()\n",
    "    ah = {}\n",
    "    \n",
    "    previousTable = None\n",
    "    \n",
    "    for table,ctid,column,word in DatabaseIter(model):        \n",
    "        wh.addMapping(word,table,column,ctid)\n",
    "        \n",
    "        ah.setdefault(table,{}).setdefault(column,{}).setdefault(word,1)\n",
    "        ah[table][column][word]+=1\n",
    "        \n",
    "    for table in ah:\n",
    "        for column in ah[table]:\n",
    "            \n",
    "            maxFrequency = numDistinctWords = numWords = 0            \n",
    "            for word, frequency in ah[table][column].items():\n",
    "                \n",
    "                numDistinctWords += 1\n",
    "                \n",
    "                numWords += frequency\n",
    "                \n",
    "                if frequency > maxFrequency:\n",
    "                    maxFrequency = frequency\n",
    "            \n",
    "            norm = 0\n",
    "            ah[table][column] = (norm,numDistinctWords,numWords,maxFrequency)\n",
    "\n",
    "    print ('INVERTED INDEX CREATED')\n",
    "    gc.collect()\n",
    "    return wh,ah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('paper',), ('citation',), ('author',)]\n",
      "INDEXING TABLE  paper\n",
      "\tCOLUMN  paper_key  SKIPPED\n",
      "\tCOLUMN  conf_key  SKIPPED\n",
      "INDEXING TABLE  citation\n",
      "\tCOLUMN  paper_cite_key  SKIPPED\n",
      "\tCOLUMN  paper_cited_key  SKIPPED\n",
      "INDEXING TABLE  author\n",
      "\tCOLUMN  paper_key  SKIPPED\n",
      "INVERTED INDEX CREATED\n"
     ]
    }
   ],
   "source": [
    "wh,ah = createInvertedIndex(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': {'name': (0, 535435, 16327107, 131605)},\n",
      " 'paper': {'conference': (0, 8660, 3148477, 69756),\n",
      "           'title': (0, 487592, 17033298, 218701),\n",
      "           'year': (0, 60, 2299223, 147970)}}\n"
     ]
    }
   ],
   "source": [
    "pp(ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log1p \n",
    "\n",
    "def processIAF(wordHash,attributeHash):\n",
    "    \n",
    "    total_attributes = sum([len(attribute) for attribute in attributeHash.values()])\n",
    "    \n",
    "    for (term, values) in wordHash.items():\n",
    "        attributes_with_this_term = sum([len(attribute) for attribute in wordHash[term].values()])\n",
    "        IAF = log1p(total_attributes/attributes_with_this_term)\n",
    "        wordHash.setIAF(term,IAF)        \n",
    "        \n",
    "    print('IAF PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAF PROCESSED\n"
     ]
    }
   ],
   "source": [
    "processIAF(wh,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNormsOfAttributes(wordHash,attributeHash):    \n",
    "    for word in wh:\n",
    "        for table in wh[word]:\n",
    "            for column, ctids in wh[word][table].items():\n",
    "                   \n",
    "                (prevNorm,numDistinctWords,numWords,maxFrequency) = attributeHash[table][column]\n",
    "\n",
    "                IAF = wordHash.getIAF(word)\n",
    "\n",
    "                frequency = len(ctids)\n",
    "                \n",
    "                TF = frequency/maxFrequency\n",
    "                \n",
    "                Norm = prevNorm + (TF*IAF)\n",
    "\n",
    "                attributeHash[table][column]=(Norm,numDistinctWords,numWords,maxFrequency)\n",
    "                \n",
    "    print ('NORMS OF ATTRIBUTES PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMS OF ATTRIBUTES PROCESSED\n"
     ]
    }
   ],
   "source": [
    "processNormsOfAttributes(wh,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAPER(title{}{'discover'}),\n",
      " PAPER(title{'2002'}{}),\n",
      " PAPER(title{'2002'}{'discover'})]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "class Tupleset:\n",
    "   \n",
    "    def __init__(self, table, predicates = None, tuples = None):            \n",
    "        \n",
    "        self.table = table\n",
    "        self.predicates= predicates if predicates is not None else {}\n",
    "        self.tuples= tuples if tuples is not None else set()\n",
    "        \n",
    "    def addTuple(self, tuple_id):\n",
    "        self.tuples.add(tuple_id)\n",
    "        \n",
    "    def addTuples(self, tuple_ids):\n",
    "        self.tuples.update(tuple_ids)\n",
    "        \n",
    "    def addAttribute(self,attribute):\n",
    "        self.attributes[attribute].setdefault( (set(),set()) )\n",
    "    \n",
    "    def union(self, otherTupleset, changeSources = False, projectionOnly = False):\n",
    "              \n",
    "        if self.table != otherTupleset.table:\n",
    "            return None\n",
    "        \n",
    "        if self.table == None:\n",
    "            return None\n",
    "        \n",
    "        if len(self.getKeywords() & otherTupleset.getKeywords())>0:\n",
    "            #tuple sets com palavras repetidas\n",
    "            return None\n",
    "\n",
    "        if projectionOnly:\n",
    "            if self.isValueFreeTupleset()==False or otherTupleset.isValueFreeTupleset() == False:\n",
    "                return None\n",
    "                \n",
    "        \n",
    "        jointTuples = self.tuples & otherTupleset.tuples\n",
    "        \n",
    "        jointPredicates = {}\n",
    "        \n",
    "        jointPredicates.update(copy.deepcopy(self.predicates))\n",
    "        \n",
    "        for attribute, (schemaWords, valueWords) in otherTupleset.predicates.items():  \n",
    "            jointPredicates.setdefault(attribute,   (set(),set())    ) \n",
    "            jointPredicates[attribute][0].update(schemaWords)\n",
    "            jointPredicates[attribute][1].update(valueWords)\n",
    "            \n",
    "        jointTupleset = Tupleset(self.table, jointPredicates , jointTuples)\n",
    "        \n",
    "        if changeSources:\n",
    "            self.tuples.difference_update(jointTuples)\n",
    "            otherTupleset.tuples.difference_update(jointTuples)\n",
    "        \n",
    "        return jointTupleset    \n",
    "        \n",
    "    def addValueMapping(self,valueWord,attribute='*'):\n",
    "        self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "        self.predicates[attribute][1].add(valueWord)\n",
    "        \n",
    "    \n",
    "    def addSchemaMapping(self,schemaWord,attribute='*'):\n",
    "        self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "        self.predicates[attribute][0].add(schemaWord)\n",
    "\n",
    "    \n",
    "    def getMappings(self):\n",
    "        for attribute, (schemaWords,valueWords) in self.predicates.items():\n",
    "            yield self.table,attribute,schemaWords,valueWords\n",
    "    \n",
    "    \n",
    "    def getValueMappings(self):\n",
    "        for attribute, ( _ ,valueWords) in self.predicates.items():\n",
    "            if valueWords == set():\n",
    "                continue\n",
    "            yield self.table,attribute,valueWords\n",
    "                \n",
    "    def getSchemaMappings(self):\n",
    "        for attribute, (schemaWords, _ ) in self.predicates.items():\n",
    "            if schemaWords == set():\n",
    "                continue\n",
    "            yield self.table,attribute,schemaWords\n",
    "                \n",
    "    def getKeywords(self):\n",
    "        keywords = set()\n",
    "        for attribute in self.predicates.keys():\n",
    "            \n",
    "            schemaWords,valueWords = self.predicates[attribute]\n",
    "            \n",
    "            keywords.update(schemaWords)                      \n",
    "            keywords.update(valueWords)\n",
    "        return frozenset(keywords)\n",
    "        \n",
    "    def isFreeTupleset(self):\n",
    "        return len(self.predicates)==0\n",
    "    \n",
    "    def isValueFreeTupleset(self):\n",
    "        for schemaWords,valueWords in self.predicates.values():\n",
    "            if len(valueWords)>0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def isSchemaFreeTupleset(self):\n",
    "        for schemaWords,valueWords in self.predicates.values():\n",
    "            if len(schemaWords)>0:\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def hasTuples(self):\n",
    "        return len(self.tuples)>0\n",
    "    \n",
    "    def clearTuples(self):\n",
    "        self.tuples.clear()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = self.table.upper()\n",
    "        str_predicates = []\n",
    "        \n",
    "        for attribute in self.predicates.keys():\n",
    "            schemaWords , valueWords = self.predicates[attribute]\n",
    "            \n",
    "            if schemaWords == set():\n",
    "                schemaWords = {}\n",
    "                \n",
    "            if valueWords == set():\n",
    "                valueWords = {}\n",
    "            \n",
    "            \n",
    "            str_predicates.append (attribute + str(schemaWords) + str(valueWords))\n",
    "            \n",
    "        result += \"(\" + ','.join(str_predicates) + \")\"\n",
    "        return result        \n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Tupleset) and self.table == other.table and self.predicates == other.predicates and self.tuples == other.tuples  \n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "    \n",
    "x = Tupleset('paper')\n",
    "x.addValueMapping('discover','title')\n",
    "x.addTuple(1)\n",
    "x.addTuple(2)\n",
    "\n",
    "y = Tupleset('paper')\n",
    "y.addSchemaMapping('2002','title')\n",
    "y.addTuple(1)\n",
    "y.addTuple(3)\n",
    "\n",
    "w = x.union(y,changeSources = True)\n",
    "pp([x,y,w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def TSFindClass(Q,wordHash):\n",
    "    #Input:  A keyword query Q=[k1, k2, . . . , km]\n",
    "    #Output: Set of non-free and non-empty tuple-sets Rq\n",
    "\n",
    "    '''\n",
    "    The tuple-set Rki contains the tuples of Ri that contain all\n",
    "    terms of K and no other keywords from Q\n",
    "    '''\n",
    "    \n",
    "    #Part 1: Find sets of tuples containing each keyword\n",
    "    P = set()\n",
    "    for keyword in Q:\n",
    "        \n",
    "        if keyword not in wordHash:\n",
    "            continue\n",
    "        \n",
    "        for table in wordHash[keyword]:\n",
    "            for (attribute,ctids) in wordHash[keyword][table].items():\n",
    "                \n",
    "                ts = Tupleset(table)\n",
    "                ts.addValueMapping(keyword,attribute)\n",
    "                ts.addTuples(ctids)                \n",
    "                P.add(ts)\n",
    "    \n",
    "    #Part 2: Find sets of tuples containing larger termsets\n",
    "    TSInterMartins(P)\n",
    "    \n",
    "    \n",
    "    #Part 3: Clean tuples\n",
    "    for ts in P:\n",
    "        ts.clearTuples()\n",
    "    \n",
    "    \n",
    "    return P\n",
    "\n",
    "# def TSInter(P):\n",
    "#     #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "#     #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     Termset is any non-empty subset K of the terms of a query Q        \n",
    "#     '''\n",
    "    \n",
    "#     Pprev = {}\n",
    "#     Pprev=copy.deepcopy(P)\n",
    "#     Pcurr = {}\n",
    "\n",
    "#     combinations = [x for x in itertools.combinations(Pprev.keys(),2)]\n",
    "#     for ( Ki , Kj ) in combinations:\n",
    "#         Tki = Pprev[Ki]\n",
    "#         Tkj = Pprev[Kj]\n",
    "        \n",
    "#         X = Ki | Kj\n",
    "#         Tx = Tki & Tkj        \n",
    "        \n",
    "#         if len(Tx) > 0:            \n",
    "#             Pcurr[X]  = Tx            \n",
    "#             Pprev[Ki] = Tki - Tx         \n",
    "#             Pprev[Kj] = Tkj - Tx\n",
    "            \n",
    "#     if Pcurr != {}:\n",
    "#         Pcurr = copy.deepcopy(TSInter(Pcurr))\n",
    "        \n",
    "#     #Pprev = Pprev U Pcurr\n",
    "#     Pprev.update(Pcurr)     \n",
    "#     return Pprev   \n",
    "\n",
    "\n",
    "def TSInterMartins(P):\n",
    "    #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "    #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    '''\n",
    "    Termset is any non-empty subset K of the terms of a query Q        \n",
    "    '''\n",
    "    \n",
    "#     print('TSInter\\n')\n",
    "#     pp(P)\n",
    "#     print('\\n====================================\\n')\n",
    "    \n",
    "    somethingChanged = False\n",
    "    \n",
    "    combinations = [x for x in itertools.combinations(P,2)]\n",
    "    for ( Ti , Tj ) in combinations:\n",
    "        \n",
    "#         print('\\nTESTANDO UNION {} \\n {} \\n'.format(Ti,Tj))\n",
    "        \n",
    "        \n",
    "#         print('´´´´´´´TSInter\\n')\n",
    "#         pp(P)\n",
    "        \n",
    "        Tx = Ti.union(Tj, changeSources = True)        \n",
    "        \n",
    "#         print('\\nUNION COMPILADO de {} \\n {} \\n {}\\n\\n\\n'.format(Ti,Tj,Tx))\n",
    "        \n",
    "#         if Tx is not None:\n",
    "#             print(len(Tx.tuples), 'tuples on union')\n",
    "            \n",
    "#         print('´´´´´´´TSInter\\n')\n",
    "#         pp(P)    \n",
    "        \n",
    "        \n",
    "        if Tx is not None and Tx.hasTuples():            \n",
    "            P.add(Tx)\n",
    "            \n",
    "            if Ti.hasTuples() == False:\n",
    "#                 print('Ti {} has not tuples',Ti)\n",
    "                P.remove(Ti)\n",
    "#             else:\n",
    "#                 print('{} has {} tuples'.format(Ti,len(Ti.tuples)))\n",
    "                \n",
    "            if Tj.hasTuples() == False:\n",
    "#                 print('Tj {} has not tuples',Tj)\n",
    "                P.remove(Tj)\n",
    "#             else:\n",
    "#                 print('{} has {} tuples'.format(Tj,len(Tj.tuples)))\n",
    "            \n",
    "            somethingChanged = True\n",
    "            \n",
    "    if somethingChanged:\n",
    "        TSInterMartins(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuerySets(filename='querysets/queryset_dblp_martins.txt'):\n",
    "    QuerySet = []\n",
    "    with open(filename,encoding='utf-8-sig') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            #The line bellow Remove words not in OLIVEIRA experiments\n",
    "            #Q = [word.strip(string.punctuation) for word in line.split() if word not in ['title','dr.',\"here's\",'char','name'] and word not in stw_set]  \n",
    "            \n",
    "            Q = tuple([word.strip(string.punctuation) for word in line.lower().split() if word not in stw_set])\n",
    "            \n",
    "            QuerySet.append(Q)\n",
    "    return QuerySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q= ['author','datacenter','2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rq = TSFindClass(Q,wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PAPER(title{}{'2015'}),\n",
       " PAPER(title{}{'author', '2015'}),\n",
       " PAPER(title{}{'author'}),\n",
       " PAPER(title{}{'datacenter'}),\n",
       " PAPER(year{}{'2015'}),\n",
       " PAPER(year{}{'2015'},title{}{'author'}),\n",
       " PAPER(year{}{'2015'},title{}{'datacenter'})}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "\n",
    "class SchemaGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__graph = {}\n",
    "    \n",
    "    def addRelationship(self,tableA,columnA,tableB, columnB, direction = -1):\n",
    "        \n",
    "        tsA = Tupleset(tableA)\n",
    "        tsB = Tupleset(tableB)\n",
    "        \n",
    "        #A->B\n",
    "        edge_info = (columnA,columnB,direction)\n",
    "        self.__graph.setdefault(tsA,{}).setdefault(tsB,[]).append(edge_info)\n",
    "        \n",
    "        #B<-A\n",
    "        edge_info = (columnB,columnA,direction*-1)\n",
    "        self.__graph.setdefault(tsB,{}).setdefault(tsA,[]).append(edge_info)\n",
    "        \n",
    "    def copyRelationships(self,sourceNode,targetNode):\n",
    "        # target->neighbours    =    source->neighbours\n",
    "        \n",
    "        print('cpRelations s: {} t: {}'.format(sourceNode,targetNode))\n",
    "        \n",
    "        self.__graph[targetNode] = copy.deepcopy(self.__graph[sourceNode])\n",
    "            \n",
    "        \n",
    "        # neighbours->target    =    neighbours->source\n",
    "        for neighbourNode in self.__graph[targetNode]:\n",
    "            for node, edge_infos in self.__graph[neighbourNode].items():\n",
    "                if node == sourceNode:\n",
    "                    self.__graph[neighbourNode][targetNode] = edge_infos\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "    def getMatchGraph(self,Match):\n",
    "        \n",
    "        Gts = copy.deepcopy(self)\n",
    "        \n",
    "        for ts in Match:\n",
    "            Gts.copyRelationships(Tupleset(ts.table),ts)\n",
    "            \n",
    "        return Gts\n",
    "    \n",
    "    def getByTableName(self,tableName):\n",
    "        return self.__graph[Tupleset(tableName)]\n",
    "    \n",
    "    def tables(self):\n",
    "        return self.__graph.keys()\n",
    "        \n",
    "    def getAdjacentTables(self, table, sort = False):\n",
    "        \n",
    "        if not sort:\n",
    "            return self.getByTableName(table).keys()\n",
    "        else:\n",
    "            # Sorting adjacents with non free tuple sets first\n",
    "            return sorted(self.getByTableName(table).keys(),key=lambda ts : ts.isFreeTupleset() )\n",
    "        \n",
    "    def isJNTSound(self,Ji):\n",
    "        if len(Ji)<3:\n",
    "            return True\n",
    "        \n",
    "        #check if there is a case A->B<-C, when A.table=C.table\n",
    "        \n",
    "        for i in range(len(Ji)-2):\n",
    "            tsA = Ji[i]\n",
    "            tsB = Ji[i+1]\n",
    "            tsC = Ji[i+2]\n",
    "            \n",
    "            if tsA.table == tsB.table:\n",
    "                            \n",
    "                for edge_info in self.__graph[tsA][tsB]:\n",
    "                    (columnA,columnB,direction) = edge_info\n",
    "                    \n",
    "                    if direction == 1:\n",
    "                        return False\n",
    "        return True    \n",
    "    \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return pprint.pformat(self.__graph)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self.__graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchemaGraph(dbname='dblp',user='imdb',password='imdb'):\n",
    "    #Output: A Schema Graph G  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "    \n",
    "    G = SchemaGraph()\n",
    "    with psycopg2.connect(dbname=dbname,user=user,password=password) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                sql = \"SELECT DISTINCT tc.table_name, kcu.column_name, ccu.table_name AS foreign_table_name, ccu.column_name AS foreign_column_name FROM information_schema.table_constraints AS tc              JOIN information_schema.key_column_usage AS kcu                 ON tc.constraint_name = kcu.constraint_name             JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY'\"\n",
    "                cur.execute(sql)\n",
    "                relations = cur.fetchall()\n",
    "\n",
    "                for (table,column,foreign_table,foreign_column) in relations:\n",
    "                    #print('table,column,foreign_table,foreign_column\\n{}, {}, {}, {}'.format(table,column,foreign_table,foreign_column))\n",
    "                    G.addRelationship(table,column,foreign_table,foreign_column)  \n",
    "                print ('SCHEMA CREATED')          \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEMA CREATED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
       "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)]},\n",
       " AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)]},\n",
       " CITATION(): {PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G=getSchemaGraph()\n",
    "\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "class Similarities:\n",
    "    \n",
    "    def __init__(self, model, attributeHash,schemaGraph):\n",
    "\n",
    "        self.model = model\n",
    "        self.attributeHash = attributeHash\n",
    "        self.schemaGraph = schemaGraph\n",
    "        \n",
    "        self.loadEmbeddingHashes()\n",
    "    \n",
    "    \n",
    "    def wordnet_similarity(self,wordA,wordB):\n",
    "        A = set(wn.synsets(wordA))\n",
    "        B = set(wn.synsets(wordB))\n",
    "\n",
    "        wupSimilarities = [0]\n",
    "        pathSimilarities = [0]\n",
    "        \n",
    "        for (sense1,sense2) in itertools.product(A,B):        \n",
    "            wupSimilarities.append(wn.wup_similarity(sense1,sense2) or 0)\n",
    "            pathSimilarities.append(wn.path_similarity(sense1,sense2) or 0)\n",
    "            \n",
    "        return max(max(wupSimilarities),max(pathSimilarities))\n",
    "\n",
    "    def jaccard_similarity(self,wordA,wordB):\n",
    "\n",
    "        A = set(wordA)\n",
    "        B = set(wordB)\n",
    "\n",
    "        return len(A & B ) / len(A | B)\n",
    "    \n",
    "    \n",
    "    def embedding10_similarity(self,word,table,column='*',Emb='B'):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        \n",
    "        # Os sinônimos do EmbA também são utilizados por todos\n",
    "        sim_list = self.EmbA[table][column]\n",
    "        \n",
    "        if column != '*':\n",
    "        \n",
    "            if Emb == 'B':\n",
    "                sim_list |= self.EmbB[table][column]\n",
    "\n",
    "            elif Emb == 'C':\n",
    "                sim_list |= self.EmbC[table][column]\n",
    "\n",
    "        return wnl.lemmatize(word) in sim_list\n",
    "    \n",
    "    \n",
    "    def embedding_similarity(self,wordA,wordB):\n",
    "        if wordA not in self.model or wordB not in self.model:\n",
    "            return 0\n",
    "        return self.model.similarity(wordA,wordB)\n",
    "    \n",
    "    \n",
    "    def word_similarity(self,word,table,column = '*',\n",
    "                    wn_sim=True, \n",
    "                    jaccard_sim=True,\n",
    "                    emb_sim=False,\n",
    "                    emb10_sim='B'):\n",
    "        sim_list=[0]\n",
    "    \n",
    "        if column == '*':\n",
    "            schema_term = table\n",
    "        else:\n",
    "            schema_term = column\n",
    "\n",
    "        if wn_sim:\n",
    "            sim_list.append( self.wordnet_similarity(schema_term,word) )\n",
    "\n",
    "        if jaccard_sim:\n",
    "            sim_list.append( self.jaccard_similarity(schema_term,word) )\n",
    "\n",
    "        if emb_sim:\n",
    "            sim_list.append( self.embedding_similarity(schema_term,word) )\n",
    "\n",
    "        sim = max(sim_list) \n",
    "\n",
    "        if emb10_sim:\n",
    "            if self.embedding10_similarity(word,table,column,emb10_sim):\n",
    "                if len(sim_list)==1:\n",
    "                    sim=1\n",
    "            else:\n",
    "                sim=0\n",
    "                \n",
    "        print('sim({},{}.{}) = {}'.format(word,table,column,sim))        \n",
    "        \n",
    "        return sim    \n",
    "    \n",
    "    def __getSimilarSet(self,word, inputType = 'word'):\n",
    "        if inputType == 'vector':\n",
    "            sim_list = model.similar_by_vector(word)\n",
    "        else:\n",
    "            sim_list = model.most_similar(word)        \n",
    "        return  {word.lower() for word,sim in sim_list}\n",
    "    \n",
    "    def loadEmbeddingHashes(self,weight=0.5):\n",
    "        \n",
    "        self.EmbA = {}\n",
    "        self.EmbB = {}\n",
    "        self.EmbC = {}\n",
    "    \n",
    "        for table in self.attributeHash:\n",
    "\n",
    "            if table not in self.model:\n",
    "                continue\n",
    "\n",
    "            self.EmbA[table]={}\n",
    "            self.EmbB[table]= {}\n",
    "            self.EmbC[table]= {}\n",
    "            \n",
    "            self.EmbA[table]['*'] = self.__getSimilarSet(table) \n",
    "\n",
    "            for column in self.attributeHash[table]:\n",
    "                if column not in model or column=='id':\n",
    "                    continue\n",
    "                \n",
    "                self.EmbA[table][column]=self.__getSimilarSet(column)\n",
    "                \n",
    "                self.EmbB[table][column]=self.__getSimilarSet( (table,column) )\n",
    "                  \n",
    "                avg_vec = (model[table]*weight + model[column]*(1-weight))                   \n",
    "                self.EmbC[table][column] = self.__getSimilarSet(avg_vec, inputType = 'vector')\n",
    "                \n",
    "                \n",
    "                \n",
    "        G = self.schemaGraph\n",
    "        for tableA in G.tables():\n",
    "\n",
    "            if tableA not in self.attributeHash or tableA not in model:\n",
    "                continue\n",
    "\n",
    "            for tableB in G.getAdjacentTables(tableA):\n",
    "\n",
    "                if tableB not in self.attributeHash or tableB not in model:\n",
    "                    continue\n",
    "\n",
    "                self.EmbB[tableB][tableA] = self.EmbB[tableA][tableB] = self.__getSimilarSet( (tableA,tableB) )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SchSFind(Q,attributeHash,threshold=0.8, \n",
    "             sim_args={}):    \n",
    "    S = set()\n",
    "    \n",
    "    sm = Similarities(model,ah,G)\n",
    "    \n",
    "    for keyword in Q:\n",
    "        for table in attributeHash:            \n",
    "            for attribute in ['*']+list(attributeHash[table].keys()):\n",
    "                \n",
    "                if(attribute=='id'):\n",
    "                    continue\n",
    "                \n",
    "                sim = sm.word_similarity(keyword,table,attribute,**sim_args)\n",
    "                \n",
    "                if sim >= threshold:\n",
    "                    ts = Tupleset(table)\n",
    "                    ts.addSchemaMapping(keyword,attribute)\n",
    "                    S.add(ts)\n",
    "                    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(author,paper.*) = 0\n",
      "sim(author,paper.conference) = 0\n",
      "sim(author,paper.year) = 0\n",
      "sim(author,paper.title) = 0\n",
      "sim(author,author.*) = 1.0\n",
      "sim(author,author.name) = 0.631578947368421\n",
      "sim(datacenter,paper.*) = 0\n",
      "sim(datacenter,paper.conference) = 0\n",
      "sim(datacenter,paper.year) = 0\n",
      "sim(datacenter,paper.title) = 0\n",
      "sim(datacenter,author.*) = 0\n",
      "sim(datacenter,author.name) = 0\n",
      "sim(2015,paper.*) = 0\n",
      "sim(2015,paper.conference) = 0\n",
      "sim(2015,paper.year) = 0\n",
      "sim(2015,paper.title) = 0\n",
      "sim(2015,author.*) = 0\n",
      "sim(2015,author.name) = 0\n"
     ]
    }
   ],
   "source": [
    "Sq = SchSFind(Q,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AUTHOR(*{'author'}{})}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinimalCover(MC, Q):\n",
    "    #Input:  A subset MC (Match Candidate) to be checked as total and minimal cover\n",
    "    #Output: If the match candidate is a TOTAL and MINIMAL cover\n",
    "\n",
    "    Subset = [ts.getKeywords() for ts in MC]\n",
    "    u = set().union(*Subset)    \n",
    "    \n",
    "    isTotal = (u == set(Q))\n",
    "    for element in Subset:\n",
    "        \n",
    "        new_u = list(Subset)\n",
    "        new_u.remove(element)\n",
    "        \n",
    "        new_u = set().union(*new_u)\n",
    "        \n",
    "        if new_u == set(Q):\n",
    "            return False\n",
    "    \n",
    "    #print('MC({},{}) = {}'.format(MC,Q,isTotal))\n",
    "    \n",
    "    return isTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QMGen(Q,Rq):\n",
    "    #Input:  A keyword query Q, The set of non-empty non-free tuple-sets Rq\n",
    "    #Output: The set Mq of query matches for Q\n",
    "    \n",
    "    '''\n",
    "    Query match is a set of tuple-sets that, if properly joined,\n",
    "    can produce networks of tuples that fulfill the query. They\n",
    "    can be thought as the leaves of a Candidate Network.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Mq = []\n",
    "    for i in range(1,len(Q)+1):\n",
    "        for subset in itertools.combinations(Rq,i):            \n",
    "            if(MinimalCover(subset,Q)):\n",
    "                print('----------------------------------------------\\nM')\n",
    "                pp(set(subset))\n",
    "                print('\\n')\n",
    "                \n",
    "                M = MInter(set(subset))\n",
    "                print('subset')\n",
    "                pp(M)\n",
    "                Mq.append(M)\n",
    "                \n",
    "                \n",
    "    return Mq\n",
    "\n",
    "def MInter(M):  \n",
    "    somethingChanged = False\n",
    "\n",
    "    combinations = [x for x in itertools.combinations(M,2)]\n",
    "    \n",
    "    for tsA, tsB  in combinations:\n",
    "        \n",
    "        tsX = tsA.union(tsB, projectionOnly = True)\n",
    "    \n",
    "        \n",
    "    \n",
    "        if tsX is not None:\n",
    "            \n",
    "            print('M before union')\n",
    "            pp(M)\n",
    "            \n",
    "            M.add(tsX)      \n",
    "            M.remove(tsA)\n",
    "            M.remove(tsB)\n",
    "            \n",
    "            somethingChanged = True\n",
    "            \n",
    "            print('\\n\\M after union')\n",
    "            pp(M)\n",
    "    \n",
    "    if somethingChanged:\n",
    "        MInter(M)\n",
    "        name\n",
    "    return M   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PAPER(title{}{'2015'}),\n",
       " PAPER(title{}{'author', '2015'}),\n",
       " PAPER(title{}{'author'}),\n",
       " PAPER(title{}{'datacenter'}),\n",
       " PAPER(year{}{'2015'}),\n",
       " PAPER(year{}{'2015'},title{}{'author'}),\n",
       " PAPER(year{}{'2015'},title{}{'datacenter'})}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AUTHOR(*{'author'}{})}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}),\n",
      " PAPER(year{}{'2015'},title{}{'author'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}),\n",
      " PAPER(year{}{'2015'},title{}{'author'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'},title{}{'author'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'},title{}{'author'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author', '2015'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author', '2015'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'datacenter'}), PAPER(title{}{'author', '2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'datacenter'}), PAPER(title{}{'author', '2015'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author'}),\n",
      " PAPER(title{}{'datacenter'}),\n",
      " PAPER(title{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author'}),\n",
      " PAPER(title{}{'datacenter'}),\n",
      " PAPER(title{}{'2015'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'author'}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'author'}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{PAPER(title{}{'2015'}), AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{PAPER(title{}{'2015'}), AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'})}\n",
      "----------------------------------------------\n",
      "M\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "subset\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n"
     ]
    }
   ],
   "source": [
    "Mq = QMGen(Q,Rq|Sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{PAPER(title{}{'author'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "{AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}),\n",
      " PAPER(year{}{'2015'},title{}{'author'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'},title{}{'author'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'author', '2015'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'datacenter'}), PAPER(title{}{'author', '2015'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'author'}),\n",
      " PAPER(title{}{'datacenter'}),\n",
      " PAPER(title{}{'2015'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'author'}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "{PAPER(title{}{'2015'}), AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for M in Mq:\n",
    "    pp(M)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QMRank(Mq, wordHash,attributeHash):\n",
    "    Ranking = []  \n",
    "    sm = Similarities(model,ah,G)\n",
    "    for M in Mq:\n",
    "        #print('=====================================\\n')\n",
    "        valueProd = 1 \n",
    "        schemaProd = 1\n",
    "        score = 1\n",
    "        \n",
    "        thereIsSchemaTerms = False\n",
    "        thereIsValueTerms = False\n",
    "        \n",
    "        for ts in M:\n",
    "            #print(ts)\n",
    "            for table, attribute, valueWords in ts.getValueMappings():\n",
    "                #print('t{} a{} v{}'.format(table,attribute,valueWords))             \n",
    "                \n",
    "                (Norm,numDistinctWords,numWords,maxFrequency) = attributeHash[table][attribute]                \n",
    "                wsum = 0\n",
    "                for term in valueWords:\n",
    "                \n",
    "                    #print('t{} a{} vt{}'.format(table,attribute,term))\n",
    "                \n",
    "                    IAF = wordHash.getIAF(term)\n",
    "                    \n",
    "                    frequency = len(wordHash.getMappings(term,table,attribute))\n",
    "                    TF = frequency/maxFrequency\n",
    "                    wsum = wsum + TF*IAF\n",
    "    \n",
    "                    thereIsValueTerms = True\n",
    "                \n",
    "                cos = wsum/Norm\n",
    "                valueProd *= cos\n",
    "                \n",
    "                \n",
    "            for table, attribute, schemaWords in ts.getSchemaMappings():\n",
    "                schemasum = 0\n",
    "                for term in schemaWords:\n",
    "                    sim = sm.word_similarity(term,table,attribute)\n",
    "                    schemasum += sim\n",
    "                    \n",
    "                    thereIsSchemaTerms = True\n",
    "                    \n",
    "                schemaProd *= schemasum           \n",
    "        \n",
    "        valueScore  = valueProd\n",
    "        schemaScore = schemaProd\n",
    "        \n",
    "        if thereIsValueTerms:\n",
    "            score *= valueScore\n",
    "        else:\n",
    "            valueScore = 0\n",
    "            \n",
    "            \n",
    "        if thereIsSchemaTerms:\n",
    "            score *= schemaScore\n",
    "        else:\n",
    "            schemaScore = 0\n",
    "                \n",
    "        Ranking.append( (M,score,valueScore,schemaScore) )\n",
    "                \n",
    "    return sorted(Ranking,key=lambda x: x[1],reverse=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(author,author.*) = 1.0\n",
      "sim(author,author.*) = 1.0\n",
      "sim(author,author.*) = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  2.055107000623907e-06,\n",
       "  2.055107000623907e-06,\n",
       "  1.0),\n",
       " ({AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})},\n",
       "  2.055107000623907e-06,\n",
       "  2.055107000623907e-06,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'author', '2015'}), PAPER(title{}{'datacenter'})},\n",
       "  2.6066781384480126e-09,\n",
       "  2.6066781384480126e-09,\n",
       "  0),\n",
       " ({AUTHOR(*{'author'}{}),\n",
       "   PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  1.228971997554877e-09,\n",
       "  1.228971997554877e-09,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'author', '2015'}),\n",
       "   PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  1.867282374785556e-10,\n",
       "  1.867282374785556e-10,\n",
       "  0),\n",
       " ({PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'},title{}{'author'})},\n",
       "  9.869137108178827e-11,\n",
       "  9.869137108178827e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}), PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  9.869137108178825e-11,\n",
       "  9.869137108178825e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'}),\n",
       "   PAPER(year{}{'2015'})},\n",
       "  9.869137108178825e-11,\n",
       "  9.869137108178825e-11,\n",
       "  0),\n",
       " ({PAPER(year{}{'2015'},title{}{'author'}),\n",
       "   PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  7.069712790631111e-12,\n",
       "  7.069712790631111e-12,\n",
       "  0),\n",
       " ({PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  5.901830484884387e-14,\n",
       "  5.901830484884387e-14,\n",
       "  0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RankedMq = QMRank(Mq,wh,ah)\n",
    "\n",
    "RankedMq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
       "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)]},\n",
       " AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)]},\n",
       " CITATION(): {PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = RankedMq[0][0]\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpRelations s: AUTHOR() t: AUTHOR(*{'author'}{})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'},title{}{'datacenter'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{PAPER(year{}{'2015'},title{}{'datacenter'}): {AUTHOR(): [('paper_key',\n",
       "                                                           'paper_key',\n",
       "                                                           1)],\n",
       "                                               AUTHOR(*{'author'}{}): [('paper_key',\n",
       "                                                                        'paper_key',\n",
       "                                                                        1)],\n",
       "                                               CITATION(): [('paper_key',\n",
       "                                                             'paper_cite_key',\n",
       "                                                             1),\n",
       "                                                            ('paper_key',\n",
       "                                                             'paper_cited_key',\n",
       "                                                             1)]},\n",
       " AUTHOR(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_key',\n",
       "                                                           'paper_key',\n",
       "                                                           -1)],\n",
       "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
       " PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
       "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)],\n",
       "           AUTHOR(*{'author'}{}): [('paper_key', 'paper_key', 1)]},\n",
       " CITATION(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_cite_key',\n",
       "                                                             'paper_key',\n",
       "                                                             -1),\n",
       "                                                            ('paper_cited_key',\n",
       "                                                             'paper_key',\n",
       "                                                             -1)],\n",
       "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]},\n",
       " AUTHOR(*{'author'}{}): {PAPER(): [('paper_key', 'paper_key', -1)],\n",
       "                         PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_key',\n",
       "                                                                        'paper_key',\n",
       "                                                                        -1)]}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.getMatchGraph(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import deque\n",
    "def SingleCN(FM,Gts,TMax,showLog=True):  \n",
    "  \n",
    "    if showLog:\n",
    "        print('================================================================================\\nSINGLE CN')\n",
    "        print('Tmax ',TMax)\n",
    "        print('FM')\n",
    "        pp(FM)\n",
    "        \n",
    "        print('\\n\\nGts')\n",
    "        pp(Gts)\n",
    "        print('\\n\\n')\n",
    "    \n",
    "    F = deque()\n",
    "\n",
    "    first_element = list(FM)[0]\n",
    "    J = [first_element]\n",
    "    \n",
    "    if len(FM)==1:\n",
    "        return J\n",
    "    \n",
    "    F.append(J)\n",
    "    \n",
    "    while F:\n",
    "        J = F.popleft()           \n",
    "        tsu = J[-1]\n",
    "        \n",
    "        sortedAdjacents = Gts.getAdjacentTables(tsu.table,sort = True)\n",
    "        \n",
    "        if showLog:\n",
    "            print('--------------------------------------------\\nParctial CN')\n",
    "            print('J ',J,'\\n')\n",
    "\n",
    "            print('\\nAdjacents:')\n",
    "            pp(Gts.getAdjacentTables(tsu.table))\n",
    "            \n",
    "            print('\\nSorted Adjacents:')\n",
    "            pp(sortedAdjacents)\n",
    "            \n",
    "            print('F:')\n",
    "            pp(F)\n",
    "        \n",
    "        for tsv in sortedAdjacents:\n",
    "            \n",
    "            if showLog:\n",
    "                pp(tsv)\n",
    "\n",
    "            if (tsv.isFreeTupleset()) or (tsv not in J):\n",
    "                \n",
    "                Ji = J + [tsv]\n",
    "                \n",
    "                if (Ji not in F) and (len(Ji)<TMax) and (Gts.isJNTSound(Ji)):\n",
    "                    \n",
    "                    if showLog:\n",
    "                        print('isSound:')\n",
    "                    \n",
    "                    containsMatch = False\n",
    "                    for ts in FM:\n",
    "                        if ts not in Ji:\n",
    "                            containsMatch = True                          \n",
    "                    \n",
    "                    if containsMatch:\n",
    "                        if showLog:\n",
    "                            print('--------------------------------------------\\nGenerated CN')\n",
    "                            print('J ',Ji,'\\n')\n",
    "                        \n",
    "                        return Ji\n",
    "                    else:\n",
    "                        F.append(Ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchCN(G,Sq,Rq,RankedMq,TMax=5):    \n",
    "    Cns = []                        \n",
    "    for  (M,score,schemascore,valuescore) in RankedMq:\n",
    "\n",
    "        Gts = G.getMatchGraph(M)\n",
    "        Cn = SingleCN(M,Gts,TMax=TMax)\n",
    "        if(Cn is not None):\n",
    "            \n",
    "            \n",
    "            #Dividindo score pelo tamanho da cn (SEGUNDA PARTE DO RANKING)\n",
    "            \n",
    "            CnScore = score/len(Cn)\n",
    "            \n",
    "            Cns.append( (Cn,M,CnScore,schemascore,valuescore) )\n",
    "    \n",
    "    #Ordena CNs pelo CnScore\n",
    "    RankedCns=sorted(Cns,key=lambda x: x[3],reverse=True)\n",
    "    \n",
    "    return RankedCns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEMA CREATED\n"
     ]
    }
   ],
   "source": [
    "G = getSchemaGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  2.055107000623907e-06,\n",
       "  2.055107000623907e-06,\n",
       "  1.0),\n",
       " ({AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})},\n",
       "  2.055107000623907e-06,\n",
       "  2.055107000623907e-06,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'author', '2015'}), PAPER(title{}{'datacenter'})},\n",
       "  2.6066781384480126e-09,\n",
       "  2.6066781384480126e-09,\n",
       "  0),\n",
       " ({AUTHOR(*{'author'}{}),\n",
       "   PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  1.228971997554877e-09,\n",
       "  1.228971997554877e-09,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'author', '2015'}),\n",
       "   PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  1.867282374785556e-10,\n",
       "  1.867282374785556e-10,\n",
       "  0),\n",
       " ({PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'},title{}{'author'})},\n",
       "  9.869137108178827e-11,\n",
       "  9.869137108178827e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}), PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  9.869137108178825e-11,\n",
       "  9.869137108178825e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'}),\n",
       "   PAPER(year{}{'2015'})},\n",
       "  9.869137108178825e-11,\n",
       "  9.869137108178825e-11,\n",
       "  0),\n",
       " ({PAPER(year{}{'2015'},title{}{'author'}),\n",
       "   PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  7.069712790631111e-12,\n",
       "  7.069712790631111e-12,\n",
       "  0),\n",
       " ({PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  5.901830484884387e-14,\n",
       "  5.901830484884387e-14,\n",
       "  0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RankedMq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpRelations s: AUTHOR() t: AUTHOR(*{'author'}{})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'},title{}{'datacenter'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}): {CITATION(): [('paper_key',\n",
      "                                                             'paper_cite_key',\n",
      "                                                             1),\n",
      "                                                            ('paper_key',\n",
      "                                                             'paper_cited_key',\n",
      "                                                             1)],\n",
      "                                               AUTHOR(*{'author'}{}): [('paper_key',\n",
      "                                                                        'paper_key',\n",
      "                                                                        1)],\n",
      "                                               AUTHOR(): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           1)]},\n",
      " CITATION(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1),\n",
      "                                                            ('paper_cited_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
      " PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "           AUTHOR(*{'author'}{}): [('paper_key', 'paper_key', 1)]},\n",
      " AUTHOR(*{'author'}{}): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_key',\n",
      "                                                                        'paper_key',\n",
      "                                                                        -1)],\n",
      "                         PAPER(): [('paper_key', 'paper_key', -1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [AUTHOR(*{'author'}{})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([PAPER(year{}{'2015'},title{}{'datacenter'}), PAPER()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[PAPER(year{}{'2015'},title{}{'datacenter'}), PAPER()]\n",
      "F:\n",
      "deque([])\n",
      "PAPER(year{}{'2015'},title{}{'datacenter'})\n",
      "isSound:\n",
      "PAPER()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [AUTHOR(*{'author'}{}), PAPER()] \n",
      "\n",
      "cpRelations s: AUTHOR() t: AUTHOR(*{'author'}{})\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'datacenter'})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(title{}{'datacenter'}): {AUTHOR(*{'author'}{}): [('paper_key',\n",
      "                                                         'paper_key',\n",
      "                                                         1)],\n",
      "                                CITATION(): [('paper_key',\n",
      "                                              'paper_cite_key',\n",
      "                                              1),\n",
      "                                             ('paper_key',\n",
      "                                              'paper_cited_key',\n",
      "                                              1)],\n",
      "                                AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " PAPER(year{}{'2015'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                         AUTHOR(*{'author'}{}): [('paper_key',\n",
      "                                                  'paper_key',\n",
      "                                                  1)],\n",
      "                         CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                                      ('paper_key', 'paper_cited_key', 1)]},\n",
      " PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "           AUTHOR(*{'author'}{}): [('paper_key', 'paper_key', 1)]},\n",
      " CITATION(): {PAPER(title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                              'paper_key',\n",
      "                                              -1),\n",
      "                                             ('paper_cited_key',\n",
      "                                              'paper_key',\n",
      "                                              -1)],\n",
      "              PAPER(year{}{'2015'}): [('paper_cite_key', 'paper_key', -1),\n",
      "                                      ('paper_cited_key', 'paper_key', -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(title{}{'datacenter'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(year{}{'2015'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
      " AUTHOR(*{'author'}{}): {PAPER(title{}{'datacenter'}): [('paper_key',\n",
      "                                                         'paper_key',\n",
      "                                                         -1)],\n",
      "                         PAPER(year{}{'2015'}): [('paper_key',\n",
      "                                                  'paper_key',\n",
      "                                                  -1)],\n",
      "                         PAPER(): [('paper_key', 'paper_key', -1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [AUTHOR(*{'author'}{})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'}), PAPER()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'}), PAPER()]\n",
      "F:\n",
      "deque([])\n",
      "PAPER(title{}{'datacenter'})\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'})] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'datacenter'})\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'author', '2015'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(title{}{'datacenter'}), PAPER(title{}{'author', '2015'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(title{}{'datacenter'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                                CITATION(): [('paper_key',\n",
      "                                              'paper_cite_key',\n",
      "                                              1),\n",
      "                                             ('paper_key',\n",
      "                                              'paper_cited_key',\n",
      "                                              1)]},\n",
      " PAPER(title{}{'author', '2015'}): {CITATION(): [('paper_key',\n",
      "                                                  'paper_cite_key',\n",
      "                                                  1),\n",
      "                                                 ('paper_key',\n",
      "                                                  'paper_cited_key',\n",
      "                                                  1)],\n",
      "                                    AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " AUTHOR(): {PAPER(title{}{'datacenter'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(title{}{'author', '2015'}): [('paper_key',\n",
      "                                                'paper_key',\n",
      "                                                -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
      " CITATION(): {PAPER(title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                              'paper_key',\n",
      "                                              -1),\n",
      "                                             ('paper_cited_key',\n",
      "                                              'paper_key',\n",
      "                                              -1)],\n",
      "              PAPER(title{}{'author', '2015'}): [('paper_cite_key',\n",
      "                                                  'paper_key',\n",
      "                                                  -1),\n",
      "                                                 ('paper_cited_key',\n",
      "                                                  'paper_key',\n",
      "                                                  -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(title{}{'datacenter'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(title{}{'datacenter'}), AUTHOR()] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'2015'})\n",
      "cpRelations s: AUTHOR() t: AUTHOR(*{'author'}{})\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'datacenter'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(title{}{'2015'}), AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(title{}{'datacenter'}): {CITATION(): [('paper_key',\n",
      "                                              'paper_cite_key',\n",
      "                                              1),\n",
      "                                             ('paper_key',\n",
      "                                              'paper_cited_key',\n",
      "                                              1)],\n",
      "                                AUTHOR(*{'author'}{}): [('paper_key',\n",
      "                                                         'paper_key',\n",
      "                                                         1)],\n",
      "                                AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " PAPER(title{}{'2015'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                          CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                                       ('paper_key', 'paper_cited_key', 1)],\n",
      "                          AUTHOR(*{'author'}{}): [('paper_key',\n",
      "                                                   'paper_key',\n",
      "                                                   1)]},\n",
      " PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "           AUTHOR(*{'author'}{}): [('paper_key', 'paper_key', 1)]},\n",
      " CITATION(): {PAPER(title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                              'paper_key',\n",
      "                                              -1),\n",
      "                                             ('paper_cited_key',\n",
      "                                              'paper_key',\n",
      "                                              -1)],\n",
      "              PAPER(title{}{'2015'}): [('paper_cite_key', 'paper_key', -1),\n",
      "                                       ('paper_cited_key', 'paper_key', -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(title{}{'datacenter'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(title{}{'2015'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
      " AUTHOR(*{'author'}{}): {PAPER(): [('paper_key', 'paper_key', -1)],\n",
      "                         PAPER(title{}{'datacenter'}): [('paper_key',\n",
      "                                                         'paper_key',\n",
      "                                                         -1)],\n",
      "                         PAPER(title{}{'2015'}): [('paper_key',\n",
      "                                                   'paper_key',\n",
      "                                                   -1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(title{}{'2015'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), AUTHOR(*{'author'}{}), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(*{'author'}{}), AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR(*{'author'}{})\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(title{}{'2015'}), AUTHOR(*{'author'}{})] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'author', '2015'})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'},title{}{'datacenter'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(title{}{'author', '2015'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}): {CITATION(): [('paper_key',\n",
      "                                                             'paper_cite_key',\n",
      "                                                             1),\n",
      "                                                            ('paper_key',\n",
      "                                                             'paper_cited_key',\n",
      "                                                             1)],\n",
      "                                               AUTHOR(): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           1)]},\n",
      " PAPER(title{}{'author', '2015'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                                    CITATION(): [('paper_key',\n",
      "                                                  'paper_cite_key',\n",
      "                                                  1),\n",
      "                                                 ('paper_key',\n",
      "                                                  'paper_cited_key',\n",
      "                                                  1)]},\n",
      " CITATION(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1),\n",
      "                                                            ('paper_cited_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1)],\n",
      "              PAPER(title{}{'author', '2015'}): [('paper_cite_key',\n",
      "                                                  'paper_key',\n",
      "                                                  -1),\n",
      "                                                 ('paper_cited_key',\n",
      "                                                  'paper_key',\n",
      "                                                  -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           -1)],\n",
      "            PAPER(title{}{'author', '2015'}): [('paper_key',\n",
      "                                                'paper_key',\n",
      "                                                -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
      " PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(title{}{'author', '2015'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(title{}{'author', '2015'}), AUTHOR()] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'datacenter'})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'},title{}{'author'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'},title{}{'author'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " PAPER(title{}{'datacenter'}): {CITATION(): [('paper_key',\n",
      "                                              'paper_cite_key',\n",
      "                                              1),\n",
      "                                             ('paper_key',\n",
      "                                              'paper_cited_key',\n",
      "                                              1)],\n",
      "                                AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " PAPER(year{}{'2015'},title{}{'author'}): {AUTHOR(): [('paper_key',\n",
      "                                                       'paper_key',\n",
      "                                                       1)],\n",
      "                                           CITATION(): [('paper_key',\n",
      "                                                         'paper_cite_key',\n",
      "                                                         1),\n",
      "                                                        ('paper_key',\n",
      "                                                         'paper_cited_key',\n",
      "                                                         1)]},\n",
      " CITATION(): {PAPER(title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                              'paper_key',\n",
      "                                              -1),\n",
      "                                             ('paper_cited_key',\n",
      "                                              'paper_key',\n",
      "                                              -1)],\n",
      "              PAPER(year{}{'2015'},title{}{'author'}): [('paper_cite_key',\n",
      "                                                         'paper_key',\n",
      "                                                         -1),\n",
      "                                                        ('paper_cited_key',\n",
      "                                                         'paper_key',\n",
      "                                                         -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(title{}{'datacenter'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(year{}{'2015'},title{}{'author'}): [('paper_key',\n",
      "                                                       'paper_key',\n",
      "                                                       -1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(title{}{'datacenter'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(title{}{'datacenter'}), AUTHOR()] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'author'})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'},title{}{'datacenter'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(title{}{'author'}), PAPER(year{}{'2015'},title{}{'datacenter'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}): {AUTHOR(): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           1)],\n",
      "                                               CITATION(): [('paper_key',\n",
      "                                                             'paper_cite_key',\n",
      "                                                             1),\n",
      "                                                            ('paper_key',\n",
      "                                                             'paper_cited_key',\n",
      "                                                             1)]},\n",
      " PAPER(title{}{'author'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                            CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                                         ('paper_key', 'paper_cited_key', 1)]},\n",
      " PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " CITATION(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1),\n",
      "                                                            ('paper_cited_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1)],\n",
      "              PAPER(title{}{'author'}): [('paper_cite_key', 'paper_key', -1),\n",
      "                                         ('paper_cited_key',\n",
      "                                          'paper_key',\n",
      "                                          -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           -1)],\n",
      "            PAPER(title{}{'author'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(title{}{'author'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(title{}{'author'}), AUTHOR()] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'author'})\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'datacenter'})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(title{}{'author'}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(title{}{'author'}): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                                         ('paper_key', 'paper_cited_key', 1)],\n",
      "                            AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " PAPER(title{}{'datacenter'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                                CITATION(): [('paper_key',\n",
      "                                              'paper_cite_key',\n",
      "                                              1),\n",
      "                                             ('paper_key',\n",
      "                                              'paper_cited_key',\n",
      "                                              1)]},\n",
      " PAPER(year{}{'2015'}): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                                      ('paper_key', 'paper_cited_key', 1)],\n",
      "                         AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " CITATION(): {PAPER(title{}{'author'}): [('paper_cite_key', 'paper_key', -1),\n",
      "                                         ('paper_cited_key',\n",
      "                                          'paper_key',\n",
      "                                          -1)],\n",
      "              PAPER(title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                              'paper_key',\n",
      "                                              -1),\n",
      "                                             ('paper_cited_key',\n",
      "                                              'paper_key',\n",
      "                                              -1)],\n",
      "              PAPER(year{}{'2015'}): [('paper_cite_key', 'paper_key', -1),\n",
      "                                      ('paper_cited_key', 'paper_key', -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(title{}{'author'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(title{}{'datacenter'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(year{}{'2015'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
      " PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(title{}{'author'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(title{}{'author'}), AUTHOR()] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'},title{}{'author'})\n",
      "cpRelations s: PAPER() t: PAPER(year{}{'2015'},title{}{'datacenter'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}),\n",
      " PAPER(year{}{'2015'},title{}{'author'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(year{}{'2015'},title{}{'datacenter'}): {CITATION(): [('paper_key',\n",
      "                                                             'paper_cite_key',\n",
      "                                                             1),\n",
      "                                                            ('paper_key',\n",
      "                                                             'paper_cited_key',\n",
      "                                                             1)],\n",
      "                                               AUTHOR(): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           1)]},\n",
      " PAPER(year{}{'2015'},title{}{'author'}): {AUTHOR(): [('paper_key',\n",
      "                                                       'paper_key',\n",
      "                                                       1)],\n",
      "                                           CITATION(): [('paper_key',\n",
      "                                                         'paper_cite_key',\n",
      "                                                         1),\n",
      "                                                        ('paper_key',\n",
      "                                                         'paper_cited_key',\n",
      "                                                         1)]},\n",
      " PAPER(): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "           CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)]},\n",
      " AUTHOR(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_key',\n",
      "                                                           'paper_key',\n",
      "                                                           -1)],\n",
      "            PAPER(year{}{'2015'},title{}{'author'}): [('paper_key',\n",
      "                                                       'paper_key',\n",
      "                                                       -1)],\n",
      "            PAPER(): [('paper_key', 'paper_key', -1)]},\n",
      " CITATION(): {PAPER(year{}{'2015'},title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1),\n",
      "                                                            ('paper_cited_key',\n",
      "                                                             'paper_key',\n",
      "                                                             -1)],\n",
      "              PAPER(year{}{'2015'},title{}{'author'}): [('paper_cite_key',\n",
      "                                                         'paper_key',\n",
      "                                                         -1),\n",
      "                                                        ('paper_cited_key',\n",
      "                                                         'paper_key',\n",
      "                                                         -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(year{}{'2015'},title{}{'author'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(year{}{'2015'},title{}{'author'}), AUTHOR()] \n",
      "\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'2015'})\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'author'})\n",
      "cpRelations s: PAPER() t: PAPER(title{}{'datacenter'})\n",
      "================================================================================\n",
      "SINGLE CN\n",
      "Tmax  5\n",
      "FM\n",
      "{PAPER(title{}{'author'}),\n",
      " PAPER(title{}{'datacenter'}),\n",
      " PAPER(title{}{'2015'})}\n",
      "\n",
      "\n",
      "Gts\n",
      "{PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                        ('paper_key', 'paper_cited_key', 1)],\n",
      "           AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " PAPER(title{}{'author'}): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                                         ('paper_key', 'paper_cited_key', 1)],\n",
      "                            AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
      " PAPER(title{}{'datacenter'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                                CITATION(): [('paper_key',\n",
      "                                              'paper_cite_key',\n",
      "                                              1),\n",
      "                                             ('paper_key',\n",
      "                                              'paper_cited_key',\n",
      "                                              1)]},\n",
      " PAPER(title{}{'2015'}): {AUTHOR(): [('paper_key', 'paper_key', 1)],\n",
      "                          CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
      "                                       ('paper_key', 'paper_cited_key', 1)]},\n",
      " CITATION(): {PAPER(title{}{'author'}): [('paper_cite_key', 'paper_key', -1),\n",
      "                                         ('paper_cited_key',\n",
      "                                          'paper_key',\n",
      "                                          -1)],\n",
      "              PAPER(title{}{'datacenter'}): [('paper_cite_key',\n",
      "                                              'paper_key',\n",
      "                                              -1),\n",
      "                                             ('paper_cited_key',\n",
      "                                              'paper_key',\n",
      "                                              -1)],\n",
      "              PAPER(title{}{'2015'}): [('paper_cite_key', 'paper_key', -1),\n",
      "                                       ('paper_cited_key', 'paper_key', -1)],\n",
      "              PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
      "                        ('paper_cited_key', 'paper_key', -1)]},\n",
      " AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(title{}{'author'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(title{}{'datacenter'}): [('paper_key', 'paper_key', -1)],\n",
      "            PAPER(title{}{'2015'}): [('paper_key', 'paper_key', -1)]}}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Parctial CN\n",
      "J  [PAPER(title{}{'2015'})] \n",
      "\n",
      "\n",
      "Adjacents:\n",
      "dict_keys([AUTHOR(), CITATION()])\n",
      "\n",
      "Sorted Adjacents:\n",
      "[AUTHOR(), CITATION()]\n",
      "F:\n",
      "deque([])\n",
      "AUTHOR()\n",
      "isSound:\n",
      "--------------------------------------------\n",
      "Generated CN\n",
      "J  [PAPER(title{}{'2015'}), AUTHOR()] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Cns = MatchCN(G,Sq,Rq,RankedMq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)],\n",
       "           AUTHOR(): [('paper_key', 'paper_key', 1)]},\n",
       " CITATION(): {PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]},\n",
       " AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)]}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEMA CREATED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{CITATION(): {PAPER(): [('paper_cite_key', 'paper_key', -1),\n",
       "                        ('paper_cited_key', 'paper_key', -1)]},\n",
       " AUTHOR(): {PAPER(): [('paper_key', 'paper_key', -1)]},\n",
       " PAPER(): {CITATION(): [('paper_key', 'paper_cite_key', 1),\n",
       "                        ('paper_key', 'paper_cited_key', 1)],\n",
       "           AUTHOR(): [('paper_key', 'paper_key', 1)]}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSchemaGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({AUTHOR(*{'author'}{}), PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  2.055107000623907e-06,\n",
       "  2.055107000623907e-06,\n",
       "  1.0),\n",
       " ({AUTHOR(*{'author'}{}), PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'})},\n",
       "  2.055107000623907e-06,\n",
       "  2.055107000623907e-06,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'author', '2015'}), PAPER(title{}{'datacenter'})},\n",
       "  2.6066781384480126e-09,\n",
       "  2.6066781384480126e-09,\n",
       "  0),\n",
       " ({AUTHOR(*{'author'}{}),\n",
       "   PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  1.228971997554877e-09,\n",
       "  1.228971997554877e-09,\n",
       "  1.0),\n",
       " ({PAPER(title{}{'author', '2015'}),\n",
       "   PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  1.867282374785556e-10,\n",
       "  1.867282374785556e-10,\n",
       "  0),\n",
       " ({PAPER(title{}{'datacenter'}), PAPER(year{}{'2015'},title{}{'author'})},\n",
       "  9.869137108178827e-11,\n",
       "  9.869137108178827e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}), PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  9.869137108178825e-11,\n",
       "  9.869137108178825e-11,\n",
       "  0),\n",
       " ({PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'}),\n",
       "   PAPER(year{}{'2015'})},\n",
       "  9.869137108178825e-11,\n",
       "  9.869137108178825e-11,\n",
       "  0),\n",
       " ({PAPER(year{}{'2015'},title{}{'author'}),\n",
       "   PAPER(year{}{'2015'},title{}{'datacenter'})},\n",
       "  7.069712790631111e-12,\n",
       "  7.069712790631111e-12,\n",
       "  0),\n",
       " ({PAPER(title{}{'2015'}),\n",
       "   PAPER(title{}{'author'}),\n",
       "   PAPER(title{}{'datacenter'})},\n",
       "  5.901830484884387e-14,\n",
       "  5.901830484884387e-14,\n",
       "  0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RankedMq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': {'name': (163.02599665435667, 535435, 16327107, 131605)},\n",
       " 'paper': {'conference': (53.05374706835997, 8660, 3148477, 69756),\n",
       "  'title': (94.39726835528738, 487592, 17033298, 218701),\n",
       "  'year': (15.291881120686318, 60, 2299223, 147970)}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatCNGenpy",
   "language": "python",
   "name": "matcngenpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
