{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "import gc #garbage collector usado no createinvertedindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def loadWordEmbeddingsModel(filename = \"word_embeddings/word2vec/GoogleNews-vectors-negative300.bin\"):\n",
    "    model = KeyedVectors.load_word2vec_format(filename,\n",
    "                                                       binary=True, limit=500000)\n",
    "    return model\n",
    "\n",
    "model = loadWordEmbeddingsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabelItemsIter:\n",
    "    def __init__(self,babelhash):\n",
    "        __slots__ = ('__babelhash')\n",
    "        self.__babelhash = babelhash    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__babelhash)\n",
    "    \n",
    "    def __contains__(self,item):\n",
    "        (key,value) = item\n",
    "        return key in self.__babelhash and self.__babelhash[key]==value\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for key in self.__babelhash.keys():\n",
    "            yield key, self.__babelhash[key]\n",
    "            \n",
    "    #Apesar de que segundo o PEP 3106 (https://www.python.org/dev/peps/pep-3106/) recomenda que façamos\n",
    "    # outros métodos, como and,eq,ne para permitir que a saída seja um set,\n",
    "    # não estamos preocupados com isso aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabelHash(dict):\n",
    "    \n",
    "    def __init__(self,babel={}):\n",
    "        __slots__ = ('__babel')\n",
    "        dict.__init__(self)\n",
    "        self.__babel = babel\n",
    "        \n",
    "    def __getidfromkey__(self,key):\n",
    "        return self.__babel[key]\n",
    "    \n",
    "    def __getkeyfromid__(self,key_id):\n",
    "        key = self.__babel[key_id]\n",
    "        return key\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        return dict.__getitem__(self,key_id)\n",
    "    \n",
    "    def __setitem__(self,key,value):    \n",
    "        try:\n",
    "            key_id = self.__babel[key]\n",
    "        except KeyError:\n",
    "            key_id = len(self.__babel)+1\n",
    "                     \n",
    "            self.__babel[key] = key_id\n",
    "            self.__babel[key_id] = key\n",
    "        \n",
    "        dict.__setitem__(self, key_id,value)\n",
    "    \n",
    "    def __delitem__(self, key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        dict.__delitem__(self, key_id)\n",
    "        \n",
    "    def __missing__(self,key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        return key_id\n",
    "        \n",
    "    def __delitem__(self, key):\n",
    "        key_id = self.__getidfromkey__(key)\n",
    "        dict.__delitem__(self,key_id)\n",
    "    \n",
    "    def __contains__(self, key):\n",
    "        try:\n",
    "            key_id = self.__getidfromkey__(key)\n",
    "        except KeyError:\n",
    "            return False\n",
    "        \n",
    "        return dict.__contains__(self,key_id)    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for key_id in dict.keys(self):\n",
    "            yield self.__getkeyfromid__(key_id)\n",
    "    \n",
    "    def keys(self):\n",
    "        for key_id in dict.keys(self):\n",
    "            yield self.__getkeyfromid__(key_id)\n",
    "    \n",
    "    def items(self):\n",
    "        return BabelItemsIter(self)\n",
    "    \n",
    "    def get(self,key):\n",
    "        value = None\n",
    "        if key in self:\n",
    "            value = self.__getitem__(key)\n",
    "        return value\n",
    "    \n",
    "    def setdefault(self,key,default=None):\n",
    "        if key not in self:\n",
    "            self[key]=default\n",
    "        return self[key]\n",
    "    \n",
    "    def printBabel(self):\n",
    "        print(self.__babel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordHash(dict):      \n",
    "        \n",
    "    def __init__(self,*args): \n",
    "        dict.__init__(self,args)\n",
    "    \n",
    "    def addMapping(self,word,table,attribute,ctid):\n",
    "        self.setdefault( word, (0, BabelHash() ) )                    \n",
    "        self[word].setdefault(table , BabelHash() )       \n",
    "        self[word][table].setdefault( attribute , [] ).append(ctid)\n",
    "        \n",
    "    def getMappings(self,word,table,attribute):\n",
    "        return self[word][table][attribute]\n",
    "    \n",
    "    def getIAF(self,key):\n",
    "        return dict.__getitem__(self,key)[0]\n",
    "    \n",
    "    def setIAF(self,key,IAF):\n",
    "        \n",
    "        oldIAF,oldValue = dict.__getitem__(self,key)\n",
    "        \n",
    "        dict.__setitem__(self, key,  (IAF,oldValue)  )\n",
    "    \n",
    "    def __getitem__(self,word):\n",
    "        return dict.__getitem__(self,word)[1]\n",
    "    \n",
    "    def __setitem__(self,word,value): \n",
    "        oldIAF,oldValue = dict.__getitem__(self,word)\n",
    "        dict.__setitem__(self, word,  (oldIAF,value)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "class DatabaseIter:\n",
    "    def __init__(self,embeddingModel,dbname='dblp',user='imdb',password='imdb'):\n",
    "        self.dbname=dbname\n",
    "        self.user=user\n",
    "        self.password =password\n",
    "        self.embeddingModel=embeddingModel\n",
    "\n",
    "    def __iter__(self):\n",
    "        with psycopg2.connect(dbname=self.dbname,user=self.user,password=self.password) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "\n",
    "                # Get list of tablenames\n",
    "\n",
    "                GET_TABLE_NAMES_SQL='''\n",
    "                    SELECT DISTINCT table_name\n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_schema='public';\n",
    "                ''' \n",
    "                cur.execute(GET_TABLE_NAMES_SQL)\n",
    "\n",
    "                for table in cur.fetchall():\n",
    "                    table_name = table[0]\n",
    "\n",
    "                    if table_name not in self.embeddingModel:\n",
    "                        print('TABLE ',table_name, 'SKIPPED')\n",
    "                        continue\n",
    "\n",
    "                    print('INDEXING TABLE ',table_name)\n",
    "\n",
    "                    #Get all tuples for this tablename\n",
    "                    cur.execute(\n",
    "                        sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "                        #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "                    )\n",
    "\n",
    "                    printSkippedColumns = True\n",
    "\n",
    "                    for row in cur.fetchall(): \n",
    "                        for column in range(1,len(row)):\n",
    "                            column_name = cur.description[column][0] \n",
    "\n",
    "                            if column_name not in self.embeddingModel or column_name=='id':\n",
    "                                if printSkippedColumns:\n",
    "                                    print('\\tCOLUMN ',column_name,' SKIPPED')\n",
    "                                continue\n",
    "\n",
    "                            ctid = row[0]\n",
    "\n",
    "                            for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "\n",
    "                                #Ignoring STOPWORDS\n",
    "                                if word in stw_set:\n",
    "                                    continue\n",
    "\n",
    "                                yield table_name,ctid,column_name, word\n",
    "\n",
    "                        printSkippedColumns=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paulo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import string\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "def createInvertedIndex(embeddingModel,dbname='dblp',user='imdb',password='imdb',showLog=True):\n",
    "    #Output: wordHash (Term Index) with this structure below\n",
    "    #map['word'] = [ 'table': ( {column} , ['ctid'] ) ]\n",
    "\n",
    "    '''\n",
    "    The Term Index is built in a preprocessing step that scans only\n",
    "    once all the relations over which the queries will be issued.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    wh = WordHash()\n",
    "    ah = {}\n",
    "    \n",
    "    previousTable = None\n",
    "    \n",
    "    for table,ctid,column,word in DatabaseIter(model):\n",
    "        \n",
    "        ah.setdefault(table,{}).setdefault(column,set()).add(word)\n",
    "        \n",
    "        wh.addMapping(word,table,column,ctid)\n",
    "        \n",
    "    for table in ah:\n",
    "        for (column, word_set) in ah[table].items():\n",
    "            num_distinct_words = len(word_set)\n",
    "            norm = 0\n",
    "            word_set.clear()\n",
    "            ah[table][column] = (norm,num_distinct_words)\n",
    "\n",
    "    print ('INVERTED INDEX CREATED')\n",
    "    gc.collect()\n",
    "    return wh,ah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXING TABLE  paper\n",
      "\tCOLUMN  paper_key  SKIPPED\n",
      "INDEXING TABLE  citation\n",
      "\tCOLUMN  paper_cite_key  SKIPPED\n",
      "\tCOLUMN  paper_cited_key  SKIPPED\n",
      "INDEXING TABLE  author\n",
      "\tCOLUMN  paper_key  SKIPPED\n",
      "INDEXING TABLE  conference\n",
      "\tCOLUMN  conf_key  SKIPPED\n",
      "INVERTED INDEX CREATED\n"
     ]
    }
   ],
   "source": [
    "wh,ah = createInvertedIndex(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log1p \n",
    "\n",
    "def processIAF(wordHash,attributeHash):\n",
    "    \n",
    "    total_attributes = sum([len(attribute) for attribute in attributeHash.values()])\n",
    "    \n",
    "    for (term, values) in wordHash.items():\n",
    "        attributes_with_this_term = sum([len(attribute) for attribute in wordHash[term].values()])\n",
    "        IAF = log1p(total_attributes/attributes_with_this_term)\n",
    "        wordHash.setIAF(term,IAF)\n",
    "        \n",
    "    print('IAF PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAF PROCESSED\n"
     ]
    }
   ],
   "source": [
    "processIAF(wh,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNormsOfAttributes(wordHash,attributeHash):    \n",
    "    for word in wh:\n",
    "        for table in wh[word]:\n",
    "            for column, ctids in wh[word][table].items():\n",
    "                   \n",
    "                (prevNorm,num_distinct_words)=attributeHash[table][column]\n",
    "\n",
    "                IAF = wordHash.getIAF(word)\n",
    "\n",
    "                TF = len(ctids)\n",
    "                \n",
    "                Norm = prevNorm + (TF*IAF)\n",
    "\n",
    "                attributeHash[table][column]=(Norm,num_distinct_words)\n",
    "                \n",
    "    print ('NORMS OF ATTRIBUTES PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMS OF ATTRIBUTES PROCESSED\n"
     ]
    }
   ],
   "source": [
    "processNormsOfAttributes(wh,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': {'name': (25454301.212786403, 535435)},\n",
       " 'conference': {'detail': (582255.903084951, 20340),\n",
       "  'name': (60830.038091471186, 9438)},\n",
       " 'paper': {'conference': (2974956.5728689986, 8660),\n",
       "  'title': (20413707.221213866, 487592),\n",
       "  'year': (1973389.3829474119, 60)}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAPER(title{}{'discover'}),\n",
      " PAPER(title{}{'2002'}),\n",
      " PAPER(title{}{'2002', 'discover'})]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "class Tupleset:\n",
    "   \n",
    "    def __init__(self, table, predicates = None, tuples = None):            \n",
    "        \n",
    "        self.table = table\n",
    "        self.predicates= predicates if predicates is not None else {}\n",
    "        self.tuples= tuples if tuples is not None else set()\n",
    "        \n",
    "    def addTuple(self, tuple_id):\n",
    "        self.tuples.add(tuple_id)\n",
    "        \n",
    "    def addTuples(self, tuple_ids):\n",
    "        self.tuples.update(tuple_ids)\n",
    "        \n",
    "    def addAttribute(self,attribute):\n",
    "        self.attributes[attribute].setdefault( (set(),set()) )\n",
    "    \n",
    "    def union(self, otherTupleset, changeSources = False):\n",
    "              \n",
    "        if self.table != otherTupleset.table:\n",
    "            return None\n",
    "        \n",
    "        if self.table == None:\n",
    "            return None\n",
    "        \n",
    "        if len(self.getKeywords() & otherTupleset.getKeywords())>0:\n",
    "#             print('tuple sets com palavras repetidas {} \\n {} \\n ******************'.format(self,otherTupleset))\n",
    "            return None\n",
    "        \n",
    "        jointTuples = self.tuples & otherTupleset.tuples\n",
    "        \n",
    "        jointPredicates = {}\n",
    "        \n",
    "        jointPredicates.update(copy.deepcopy(self.predicates))\n",
    "        \n",
    "        for attribute, (schemaWords, valueWords) in otherTupleset.predicates.items():  \n",
    "            jointPredicates.setdefault(attribute,   (set(),set())    ) \n",
    "            jointPredicates[attribute][0].update(schemaWords)\n",
    "            jointPredicates[attribute][1].update(valueWords)\n",
    "            \n",
    "        jointTupleset = Tupleset(self.table, jointPredicates , jointTuples)\n",
    "        \n",
    "        if changeSources:\n",
    "            self.tuples.difference_update(jointTuples)\n",
    "            otherTupleset.tuples.difference_update(jointTuples)\n",
    "        \n",
    "        return jointTupleset    \n",
    "        \n",
    "    def addValueMapping(self,attribute,valueWord):\n",
    "        self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "        self.predicates[attribute][1].add(valueWord)\n",
    "        \n",
    "    \n",
    "    def addSchemaMapping(self,attribute,schemaWord):\n",
    "        self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "        self.predicates[attribute][0].add(schemaWord)\n",
    "        \n",
    "        \n",
    "#     def addValueMappings(self,attribute,valueWords):\n",
    "#         self.predicates.setdefault(attribute,   (set(),set())    ) \n",
    "#         self.predicates[attribute][1].update(valueWords)\n",
    "        \n",
    "    def getKeywords(self):\n",
    "        keywords = set()\n",
    "        for attribute in self.predicates.keys():\n",
    "            \n",
    "            schemaWords,valueWords = self.predicates[attribute]\n",
    "            \n",
    "            keywords.update(schemaWords)                      \n",
    "            keywords.update(valueWords)\n",
    "        return frozenset(keywords)\n",
    "        \n",
    "    def isFreeTupleset(self):\n",
    "        return len(self.predicates)==0\n",
    "    \n",
    "    def hasTuples(self):\n",
    "        return len(self.tuples)>0\n",
    "    \n",
    "    def clearTuples(self):\n",
    "        self.tuples.clear()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = self.table.upper()\n",
    "        str_predicates = []\n",
    "        \n",
    "        for attribute in self.predicates.keys():\n",
    "            schemaWords , valueWords = self.predicates[attribute]\n",
    "            \n",
    "            if schemaWords == set():\n",
    "                schemaWords = {}\n",
    "                \n",
    "            if valueWords == set():\n",
    "                valueWords = {}\n",
    "            \n",
    "            \n",
    "            str_predicates.append (attribute + str(schemaWords) + str(valueWords))\n",
    "            \n",
    "        result += \"(\" + ','.join(str_predicates) + \")\"\n",
    "        return result\n",
    "        \n",
    "    \n",
    "        \n",
    "x = Tupleset('paper')\n",
    "x.addValueMapping('title','discover')\n",
    "x.addTuple(1)\n",
    "x.addTuple(2)\n",
    "\n",
    "y = Tupleset('paper')\n",
    "y.addValueMapping('title','2002')\n",
    "y.addTuple(1)\n",
    "y.addTuple(3)\n",
    "\n",
    "w = x.union(y,changeSources = True)\n",
    "pp([x,y,w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def TSFindClass(Q,wordHash):\n",
    "    #Input:  A keyword query Q=[k1, k2, . . . , km]\n",
    "    #Output: Set of non-free and non-empty tuple-sets Rq\n",
    "\n",
    "    '''\n",
    "    The tuple-set Rki contains the tuples of Ri that contain all\n",
    "    terms of K and no other keywords from Q\n",
    "    '''\n",
    "    \n",
    "    #Part 1: Find sets of tuples containing each keyword\n",
    "    P = []\n",
    "    for keyword in Q:\n",
    "        \n",
    "        if keyword not in wordHash:\n",
    "            continue\n",
    "        \n",
    "        for table in wordHash[keyword]:\n",
    "            for (attribute,ctids) in wordHash[keyword][table].items():\n",
    "                \n",
    "                ts = Tupleset(table)\n",
    "                ts.addValueMapping(attribute,keyword)\n",
    "                ts.addTuples(ctids)                \n",
    "                P.append(ts)\n",
    "    \n",
    "    #Part 2: Find sets of tuples containing larger termsets\n",
    "    TSInterMartins(P)\n",
    "    \n",
    "    \n",
    "    #Part 3: Clean tuples\n",
    "    for ts in P:\n",
    "        ts.clearTuples()\n",
    "    \n",
    "    \n",
    "    return P\n",
    "\n",
    "# def TSInter(P):\n",
    "#     #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "#     #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     Termset is any non-empty subset K of the terms of a query Q        \n",
    "#     '''\n",
    "    \n",
    "#     Pprev = {}\n",
    "#     Pprev=copy.deepcopy(P)\n",
    "#     Pcurr = {}\n",
    "\n",
    "#     combinations = [x for x in itertools.combinations(Pprev.keys(),2)]\n",
    "#     for ( Ki , Kj ) in combinations:\n",
    "#         Tki = Pprev[Ki]\n",
    "#         Tkj = Pprev[Kj]\n",
    "        \n",
    "#         X = Ki | Kj\n",
    "#         Tx = Tki & Tkj        \n",
    "        \n",
    "#         if len(Tx) > 0:            \n",
    "#             Pcurr[X]  = Tx            \n",
    "#             Pprev[Ki] = Tki - Tx         \n",
    "#             Pprev[Kj] = Tkj - Tx\n",
    "            \n",
    "#     if Pcurr != {}:\n",
    "#         Pcurr = copy.deepcopy(TSInter(Pcurr))\n",
    "        \n",
    "#     #Pprev = Pprev U Pcurr\n",
    "#     Pprev.update(Pcurr)     \n",
    "#     return Pprev   \n",
    "\n",
    "\n",
    "def TSInterMartins(P):\n",
    "    #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "    #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    '''\n",
    "    Termset is any non-empty subset K of the terms of a query Q        \n",
    "    '''\n",
    "    \n",
    "#     print('TSInter\\n')\n",
    "#     pp(P)\n",
    "#     print('\\n====================================\\n')\n",
    "    \n",
    "    somethingChanged = False\n",
    "    \n",
    "    combinations = [x for x in itertools.combinations(P,2)]\n",
    "    for ( Ti , Tj ) in combinations:\n",
    "        \n",
    "#         print('\\nTESTANDO UNION {} \\n {} \\n'.format(Ti,Tj))\n",
    "        \n",
    "        \n",
    "#         print('´´´´´´´TSInter\\n')\n",
    "#         pp(P)\n",
    "        \n",
    "        Tx = Ti.union(Tj, changeSources = True)        \n",
    "        \n",
    "#         print('\\nUNION COMPILADO de {} \\n {} \\n {}\\n\\n\\n'.format(Ti,Tj,Tx))\n",
    "        \n",
    "#         if Tx is not None:\n",
    "#             print(len(Tx.tuples), 'tuples on union')\n",
    "            \n",
    "#         print('´´´´´´´TSInter\\n')\n",
    "#         pp(P)    \n",
    "        \n",
    "        \n",
    "        if Tx is not None and Tx.hasTuples():            \n",
    "            P.append(Tx)\n",
    "            \n",
    "            if Ti.hasTuples() == False:\n",
    "#                 print('Ti {} has not tuples',Ti)\n",
    "                P.remove(Ti)\n",
    "#             else:\n",
    "#                 print('{} has {} tuples'.format(Ti,len(Ti.tuples)))\n",
    "                \n",
    "            if Tj.hasTuples() == False:\n",
    "#                 print('Tj {} has not tuples',Tj)\n",
    "                P.remove(Tj)\n",
    "#             else:\n",
    "#                 print('{} has {} tuples'.format(Tj,len(Tj.tuples)))\n",
    "            \n",
    "            somethingChanged = True\n",
    "            \n",
    "    if somethingChanged:\n",
    "        TSInterMartins(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuerySets(filename='querysets/queryset_dblp_martins.txt'):\n",
    "    QuerySet = []\n",
    "    with open(filename,encoding='utf-8-sig') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            #The line bellow Remove words not in OLIVEIRA experiments\n",
    "            #Q = [word.strip(string.punctuation) for word in line.split() if word not in ['title','dr.',\"here's\",'char','name'] and word not in stw_set]  \n",
    "            \n",
    "            Q = tuple([word.strip(string.punctuation) for word in line.lower().split() if word not in stw_set])\n",
    "            \n",
    "            QuerySet.append(Q)\n",
    "    return QuerySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q= ['datacenter','2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rq = TSFindClass(Q,wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PAPER(title{}{'datacenter'}),\n",
       " PAPER(title{}{'2015'}),\n",
       " PAPER(year{}{'2015'}),\n",
       " CONFERENCE(name{}{'2015'}),\n",
       " CONFERENCE(detail{}{'2015'}),\n",
       " PAPER(title{}{'datacenter'},year{}{'2015'})]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__graph = {}\n",
    "    \n",
    "    def addRelationship(self,tableA,columnA,tableB, columnB, direction = 1):\n",
    "        \n",
    "        #A->B\n",
    "        edge_info = (columnA,columnB,direction)\n",
    "        self.__graph.setdefault(tableA,{}).setdefault(tableB,[]).append(edge_info)\n",
    "        \n",
    "        #B<-A\n",
    "        edge_info = (columnB,columnA,direction*-1)\n",
    "        self.__graph.setdefault(tableB,{}).setdefault(tableA,[]).append(edge_info)\n",
    "        \n",
    "    def tables(self):\n",
    "        return self.__graph.keys()\n",
    "        \n",
    "    def getAdjacentTables(self, table):\n",
    "        return self.__graph[table].keys()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return repr(self.__graph)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self.__graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchemaGraph(dbname='dblp',user='imdb',password='imdb'):\n",
    "    #Output: A Schema Graph G  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "    \n",
    "    G = SchemaGraph()\n",
    "    with psycopg2.connect(dbname=dbname,user=user,password=password) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                sql = \"SELECT DISTINCT                 tc.table_name, kcu.column_name,                 ccu.table_name AS foreign_table_name, ccu.column_name AS foreign_column_name             FROM information_schema.table_constraints AS tc              JOIN information_schema.key_column_usage AS kcu                 ON tc.constraint_name = kcu.constraint_name             JOIN information_schema.constraint_column_usage AS ccu                 ON ccu.constraint_name = tc.constraint_name             WHERE constraint_type = 'FOREIGN KEY'\"\n",
    "                cur.execute(sql)\n",
    "                relations = cur.fetchall()\n",
    "\n",
    "                for (table,column,foreign_table,foreign_column) in relations:\n",
    "                    print('table,column,foreign_table,foreign_column\\n{}, {}, {}, {}'.format(table,column,foreign_table,foreign_column))\n",
    "                    G.addRelationship(table,column,foreign_table,foreign_column)  \n",
    "                print ('SCHEMA CREATED')          \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table,column,foreign_table,foreign_column\n",
      "author, paper_key, paper, paper_key\n",
      "table,column,foreign_table,foreign_column\n",
      "citation, paper_cite_key, paper, paper_key\n",
      "table,column,foreign_table,foreign_column\n",
      "citation, paper_cited_key, paper, paper_key\n",
      "SCHEMA CREATED\n"
     ]
    }
   ],
   "source": [
    "G=getSchemaGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "class Similarities:\n",
    "    \n",
    "    def __init__(self, model, attributeHash,schemaGraph):\n",
    "\n",
    "        self.model = model\n",
    "        self.attributeHash = attributeHash\n",
    "        self.schemaGraph = schemaGraph\n",
    "        \n",
    "        self.loadEmbeddingHashes()\n",
    "    \n",
    "    \n",
    "    def wordnet_similarity(self,wordA,wordB):\n",
    "        A = set(wn.synsets(wordA))\n",
    "        B = set(wn.synsets(wordB))\n",
    "\n",
    "        wupSimilarities = [0]\n",
    "        pathSimilarities = [0]\n",
    "        \n",
    "        for (sense1,sense2) in itertools.product(A,B):        \n",
    "            wupSimilarities.append(wn.wup_similarity(sense1,sense2) or 0)\n",
    "            pathSimilarities.append(wn.path_similarity(sense1,sense2) or 0)\n",
    "            \n",
    "        return max(max(wupSimilarities),max(pathSimilarities))\n",
    "\n",
    "    def jaccard_similarity(self,wordA,wordB):\n",
    "\n",
    "        A = set(wordA)\n",
    "        B = set(wordB)\n",
    "\n",
    "        return len(A & B ) / len(A | B)\n",
    "    \n",
    "    \n",
    "    def embedding10_similarity(self,word,table,column='*',Emb='B'):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        \n",
    "        # Os sinônimos do EmbA também são utilizados por todos\n",
    "        sim_list = self.EmbA[table][column]\n",
    "        \n",
    "        if column != '*':\n",
    "        \n",
    "            if Emb == 'B':\n",
    "                sim_list |= self.EmbB[table][column]\n",
    "\n",
    "            elif Emb == 'C':\n",
    "                sim_list |= self.EmbC[table][column]\n",
    "\n",
    "        return wnl.lemmatize(word) in sim_list\n",
    "    \n",
    "    \n",
    "    def embedding_similarity(self,wordA,wordB):\n",
    "        if wordA not in self.model or wordB not in self.model:\n",
    "            return 0\n",
    "        return self.model.similarity(wordA,wordB)\n",
    "    \n",
    "    \n",
    "    def word_similarity(self,word,table,column = '*',\n",
    "                    wn_sim=True, \n",
    "                    jaccard_sim=True,\n",
    "                    emb_sim=False,\n",
    "                    emb10_sim='B'):\n",
    "        sim_list=[0]\n",
    "    \n",
    "        if column == '*':\n",
    "            schema_term = table\n",
    "        else:\n",
    "            schema_term = column\n",
    "\n",
    "        if wn_sim:\n",
    "            sim_list.append( self.wordnet_similarity(schema_term,word) )\n",
    "\n",
    "        if jaccard_sim:\n",
    "            sim_list.append( self.jaccard_similarity(schema_term,word) )\n",
    "\n",
    "        if emb_sim:\n",
    "            sim_list.append( self.embedding_similarity(schema_term,word) )\n",
    "\n",
    "        sim = max(sim_list) \n",
    "\n",
    "        if emb10_sim:\n",
    "            if self.embedding10_similarity(word,table,column,emb10_sim):\n",
    "                if len(sim_list)==1:\n",
    "                    sim=1\n",
    "            else:\n",
    "                sim=0\n",
    "                \n",
    "        print('sim({},{}.{}) = {}'.format(word,table,column,sim))        \n",
    "        \n",
    "        return sim    \n",
    "    \n",
    "    def __getSimilarSet(self,word, inputType = 'word'):\n",
    "        if inputType == 'vector':\n",
    "            sim_list = model.similar_by_vector(word)\n",
    "        else:\n",
    "            sim_list = model.most_similar(word)        \n",
    "        return  {word.lower() for word,sim in sim_list}\n",
    "    \n",
    "    def loadEmbeddingHashes(self,weight=0.5):\n",
    "        \n",
    "        self.EmbA = {}\n",
    "        self.EmbB = {}\n",
    "        self.EmbC = {}\n",
    "    \n",
    "        for table in self.attributeHash:\n",
    "\n",
    "            if table not in self.model:\n",
    "                continue\n",
    "\n",
    "            self.EmbA[table]={}\n",
    "            self.EmbB[table]= {}\n",
    "            self.EmbC[table]= {}\n",
    "            \n",
    "            self.EmbA[table]['*'] = self.__getSimilarSet(table) \n",
    "\n",
    "            for column in self.attributeHash[table]:\n",
    "                if column not in model or column=='id':\n",
    "                    continue\n",
    "                \n",
    "                self.EmbA[table][column]=self.__getSimilarSet(column)\n",
    "                \n",
    "                self.EmbB[table][column]=self.__getSimilarSet( (table,column) )\n",
    "                  \n",
    "                avg_vec = (model[table]*weight + model[column]*(1-weight))                   \n",
    "                self.EmbC[table][column] = self.__getSimilarSet(avg_vec, inputType = 'vector')\n",
    "                \n",
    "                \n",
    "                \n",
    "        G = self.schemaGraph\n",
    "        for tableA in G.tables():\n",
    "\n",
    "            if tableA not in self.attributeHash or tableA not in model:\n",
    "                continue\n",
    "\n",
    "            for tableB in G.getAdjacentTables(tableA):\n",
    "\n",
    "                if tableB not in self.attributeHash or tableB not in model:\n",
    "                    continue\n",
    "\n",
    "                self.EmbB[tableB][tableA] = self.EmbB[tableA][tableB] = self.__getSimilarSet( (tableA,tableB) )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SchSFind(Q,attributeHash,threshold=0.8, \n",
    "             sim_args={}):    \n",
    "    S = []\n",
    "    \n",
    "    sm = Similarities(model,ah,G)\n",
    "    \n",
    "    for keyword in Q:\n",
    "        for (table,values) in attributeHash.items():\n",
    "            \n",
    "            sim = sm.word_similarity(keyword,table,**sim_args)\n",
    "            \n",
    "            if sim >= threshold:  \n",
    "                ts = Tupleset(table)\n",
    "                ts.addSchemaMapping('*',keyword)\n",
    "                S.append(ts)\n",
    "            \n",
    "            for attribute in values.keys():\n",
    "                \n",
    "                if(attribute=='id'):\n",
    "                    continue\n",
    "                \n",
    "                sim = sm.word_similarity(keyword,table,attribute,**sim_args)\n",
    "                \n",
    "                if sim >= threshold:\n",
    "                    ts = Tupleset(table)\n",
    "                    ts.addSchemaMapping(attribute,keyword)\n",
    "                    S.append(ts)\n",
    "                    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ['author','name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(author,paper.*) = 0\n",
      "sim(author,paper.title) = 0\n",
      "sim(author,paper.year) = 0\n",
      "sim(author,paper.conference) = 0\n",
      "sim(author,conference.*) = 0\n",
      "sim(author,conference.detail) = 0\n",
      "sim(author,conference.name) = 0\n",
      "sim(author,author.*) = 1.0\n",
      "sim(author,author.name) = 0.631578947368421\n",
      "sim(name,paper.*) = 0\n",
      "sim(name,paper.title) = 0\n",
      "sim(name,paper.year) = 0\n",
      "sim(name,paper.conference) = 0\n",
      "sim(name,conference.*) = 0\n",
      "sim(name,conference.detail) = 0\n",
      "sim(name,conference.name) = 1.0\n",
      "sim(name,author.*) = 0\n",
      "sim(name,author.name) = 1.0\n"
     ]
    }
   ],
   "source": [
    "Sq = SchSFind(Q,ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatCNGenpy",
   "language": "python",
   "name": "matcngenpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
