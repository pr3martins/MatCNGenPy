{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S-MatCNGenPy\n",
    "\n",
    "Este é um passo-a-passo da implementação em python do método **S-MatCNGenPy**, desenvolvido no trabalho \\[1\\]. O seu principal objetivo é garantir o suporte a referência ao esquema de dados em busca por palavras-chave em banco de dados. Observe que algumas consultas, como visto abaixo, não estão relacionadas apenas a valores do banco de dados, mas a própria estrutura do esquema.\n",
    "\n",
    "```\n",
    "    filmes do Will Smith\n",
    "```\n",
    "- **`filmes`** : relação Movie\n",
    "- **`Will`, `Smith`** : instâncias da tabela Person(Name) \n",
    "\n",
    "\n",
    "#### Leituras Importantes\n",
    "\n",
    "> [\\[1\\]](https://drive.google.com/file/d/1ZnljlKss9a8M_RDqseTYfZbQCjDhcJkk/view) MARTINS, Paulo Rodrigo O.; DA SILVA, Altigran Soares. *Uma Abordagem para Suporte a Referências ao Esquema em Consultas por Palavras-Chave em Bancos de Dados Relacionais*. Trabalho de Conclusão de Curso (Ciência da Computação), Universidade Federal do Amazonas, 2017. \n",
    "\n",
    "> [\\[2\\]]() DE OLIVEIRA, Pericles; DA SILVA, Altigran; DE MOURA, Edleno. *Match-Based Candidate Network Generation for Keyword Queries over Relational Databases*. In: Data Engineering (ICDE), 2018 IEEE 34st International Conference on. IEEE, 2016. Aceito pra Pubicação\n",
    "\n",
    "> [\\[3\\]](https://dl.acm.org/citation.cfm?id=1989383) BERGAMASCHI, Sonia et al. *Keyword search over relational databases: a metadata approach*. In: Proceedings of the 2011 ACM SIGMOD International Conference on Management of data. ACM, 2011. p. 565-576."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:51.696806Z",
     "start_time": "2018-09-10T14:18:50.998297Z"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from pprint import pprint as pp\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import itertools\n",
    "import copy\n",
    "from math import log1p\n",
    "from queue import deque\n",
    "import ast\n",
    "import gc\n",
    "from queue import deque\n",
    "\n",
    "import nltk \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "# Connect to an existing database\n",
    "conn = psycopg2.connect(\"dbname=imdb user=imdb password=imdb\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "\n",
    "Antes mesmo de receber os querysets, o sistema passa por um pré-processamento, que é responsavél pela criação de dois índices invertidos:\n",
    "\n",
    "* **wordHash**: tabela que associa cada termo do banco de dados com o seu **IAF (Inverse Attribute Frequency)** e também referencia todas Tabelas, Colunas e CTIDs em que a palavra ocorre. Nota: o CTID é o endereço físico de uma linha em uma tabela, utilizado para encontrar rapidamente uma tupla.\n",
    "```python\n",
    "wordHash['term'] = ( IAF , { 'table': { 'column' : [ctid] } } )\n",
    "```\n",
    "* **attributeHash**: tabela que para cada atributo (documento), armazena a sua norma e o número de palavras distintas.\n",
    "```python\n",
    "attributeHash['table']['column'] = ( norm , num_distinct_words )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação dos Índices Invertidos\n",
    "\n",
    "O processo de criação é realizado em três etapas. Primeiramente, o procedimento ```createInvertedIndex()``` faz uma varredura no banco de dados e preenche parcialmente o ```wordHash```, faltando apenas calcular os IAFs para cada termo. Além disso, este procedimento também ele também armazena no ```attributeHash``` o total de palavras distintas para cada atributo.\n",
    "\n",
    "Em seguida, os IAFs de cada termo são processados através do método ```processIAF(wordHash,attributeHash)```. Por último, as normas dos atributos (documentos) são calculadas no método ```processNormsOfAttributes(wordHash,attributeHash)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "def loadWordEmbeddingsModel(filename = \"word_embeddings/word2vec/GoogleNews-vectors-negative300.bin\"):\n",
    "    model = KeyedVectors.load_word2vec_format(filename,\n",
    "                                                       binary=True, limit=500000)\n",
    "    return model\n",
    "\n",
    "\n",
    "#GloVe\n",
    "#def loadWordEmbeddingsModel(filename = \"word_embeddings/word2vec/GoogleNews-vectors-negative300.bin\"):\n",
    "#    model = KeyedVectors.load_word2vec_format(filename, limit=500000)\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingModel = loadWordEmbeddingsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apesar de ID está no word embedding model, sabemos que esse campo não deve ser indexado\n",
    "#'id' in embeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:51.707575Z",
     "start_time": "2018-09-10T14:18:51.699810Z"
    }
   },
   "outputs": [],
   "source": [
    "def createInvertedIndex(embeddingModel):\n",
    "    #Output: wordHash (Term Index) with this structure below\n",
    "    #map['word'] = [ 'table': ( {column} , ['ctid'] ) ]\n",
    "\n",
    "    '''\n",
    "    The Term Index is built in a preprocessing step that scans only\n",
    "    once all the relations over which the queries will be issued.\n",
    "    '''\n",
    "    \n",
    "    wordHash = {}\n",
    "    attributeHash = {}\n",
    "    \n",
    "    \n",
    "    # Get list of tablenames\n",
    "    cur.execute(\"SELECT DISTINCT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\")\n",
    "    for table in cur.fetchall():\n",
    "        table_name = table[0]\n",
    "        \n",
    "        if table_name not in embeddingModel:\n",
    "            print('TABLE ',table_name, 'SKIPPED')\n",
    "            continue\n",
    "        \n",
    "        print('INDEXING TABLE ',table_name)\n",
    "        \n",
    "        attributeHash[table_name] = {}\n",
    "        \n",
    "        #Get all tuples for this tablename\n",
    "        cur.execute(\n",
    "            sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "            #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "        )\n",
    "        printSkippedColumns = True\n",
    "        for row in cur.fetchall(): \n",
    "            for column in range(1,len(row)):\n",
    "                column_name = cur.description[column][0] \n",
    "                \n",
    "                if column_name not in embeddingModel or column_name=='id':\n",
    "                    if printSkippedColumns:\n",
    "                        print('\\tCOLUMN ',column_name,' SKIPPED')\n",
    "                    continue\n",
    "                \n",
    "                ctid = row[0]\n",
    "\n",
    "                for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "                    \n",
    "                    #Ignoring STOPWORDS\n",
    "                    if word in stw_set:\n",
    "                        continue\n",
    "\n",
    "                    #If word entry doesn't exists, it will be inicialized (setdefault method),\n",
    "                    #Append the location for this word\n",
    "                    wordHash.setdefault(word, {})                    \n",
    "                    wordHash[word].setdefault( table_name , {} )\n",
    "                    wordHash[word][table_name].setdefault( column_name , [] ).append(ctid)\n",
    "                    \n",
    "                    attributeHash[table_name].setdefault(column_name,(0,set()))\n",
    "                    attributeHash[table_name][column_name][1].add(word)\n",
    "            printSkippedColumns=False\n",
    "        \n",
    "        #Count words\n",
    "        \n",
    "        for (column_name,(norm,wordSet)) in attributeHash[table_name].items():\n",
    "            num_distinct_words = len(wordSet)\n",
    "            wordSet.clear()\n",
    "            attributeHash[table_name][column_name] = (norm,num_distinct_words)\n",
    "        \n",
    "\n",
    "    print ('INVERTED INDEX CREATED')\n",
    "    return (wordHash,attributeHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:51.819730Z",
     "start_time": "2018-09-10T14:18:51.709287Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(wordHash,attributeHash) = createInvertedIndex(embeddingModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp(wordHash['denzel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp(attributeHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:51.887117Z",
     "start_time": "2018-09-10T14:18:51.824813Z"
    }
   },
   "outputs": [],
   "source": [
    "def processIAF(wordHash,attributeHash):\n",
    "    \n",
    "    total_attributes = sum([len(attribute) for attribute in attributeHash.values()])\n",
    "    \n",
    "    for (term, values) in wordHash.items():\n",
    "        \n",
    "        attributes_with_this_term = sum([len(attribute) for attribute in wordHash[term].values()])\n",
    "        \n",
    "        IAF = log1p(total_attributes/attributes_with_this_term)\n",
    "                \n",
    "        wordHash[term] = (IAF,values)\n",
    "    print('IAF PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:51.965347Z",
     "start_time": "2018-09-10T14:18:51.889726Z"
    }
   },
   "outputs": [],
   "source": [
    "processIAF(wordHash,attributeHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:52.031947Z",
     "start_time": "2018-09-10T14:18:51.972153Z"
    }
   },
   "outputs": [],
   "source": [
    "#pp(wordHash['denzel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:52.110481Z",
     "start_time": "2018-09-10T14:18:52.038932Z"
    }
   },
   "outputs": [],
   "source": [
    "def processNormsOfAttributes(wordHash,attributeHash,embeddingModel):\n",
    "  \n",
    "    # Get list of tablenames\n",
    "    cur.execute(\"SELECT DISTINCT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\")\n",
    "    for table in cur.fetchall():\n",
    "        table_name = table[0]\n",
    "        \n",
    "        if table_name not in embeddingModel:\n",
    "            print('TABLE ',table_name, 'SKIPPED')\n",
    "            continue\n",
    "        \n",
    "        print('PROCESSING TABLE ',table_name)\n",
    "        \n",
    "        #Get all tuples for this tablename\n",
    "        cur.execute(\n",
    "            sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "            #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "        )\n",
    "        \n",
    "        printSkippedColumns = False\n",
    "        for row in cur.fetchall():\n",
    "            for column in range(1,len(row)):\n",
    "                column_name = cur.description[column][0]  \n",
    "                \n",
    "                if column_name not in embeddingModel or column_name=='id':\n",
    "                    if printSkippedColumns:\n",
    "                        print('\\tCOLUMN ',column_name,' SKIPPED')\n",
    "                    continue\n",
    "                \n",
    "                ctid = row[0]\n",
    "\n",
    "                for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "                    \n",
    "                    #Ignoring STOPWORDS\n",
    "                    if word in stw_set:\n",
    "                        continue\n",
    "                    \n",
    "                    (prevNorm,num_distinct_words)=attributeHash[table_name][column_name]\n",
    "                    \n",
    "                    IAF = wordHash[word][0]\n",
    "                    \n",
    "                    Norm = prevNorm + IAF\n",
    "                    \n",
    "                    attributeHash[table_name][column_name]=(Norm,num_distinct_words)\n",
    "            printSkippedColumns = False\n",
    "\n",
    "    print ('NORMS OF ATTRIBUTES PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:52.210430Z",
     "start_time": "2018-09-10T14:18:52.113815Z"
    }
   },
   "outputs": [],
   "source": [
    "processNormsOfAttributes(wordHash,attributeHash,embeddingModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp(wordHash['denzel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:52.287128Z",
     "start_time": "2018-09-10T14:18:52.213169Z"
    }
   },
   "outputs": [],
   "source": [
    "#pp(attributeHash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "O processamento das consultas é realizado em "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:52.353192Z",
     "start_time": "2018-09-10T14:18:52.289723Z"
    }
   },
   "outputs": [],
   "source": [
    "def getQuerySets(filename='querysets/queryset_imdb_martins.txt'):\n",
    "    QuerySet = []\n",
    "    with open(filename,encoding='utf-8-sig') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            #The line bellow Remove words not in OLIVEIRA experiments\n",
    "            #Q = [word.strip(string.punctuation) for word in line.split() if word not in ['title','dr.',\"here's\",'char','name'] and word not in stw_set]  \n",
    "            \n",
    "            Q = [word.strip(string.punctuation) for word in line.lower().split() if word not in stw_set]  \n",
    "            \n",
    "            QuerySet.append(Q)\n",
    "    return QuerySet\n",
    "        \n",
    "QuerySet = getQuerySets()\n",
    "QuerySet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperação de Tuple-sets\n",
    "Esta etapa consiste em recuperar conjuntos de tuplas que contém cada palavra-chave, chamados de tuple-sets. O algoritmo `TSFind`, que realiza esse processo, pode ser é divido em três partes: \n",
    "* **Recuperação de tuplas:** Essa parte consiste em encontrar os conjuntos de tuplas que contém cada uma das palavras do Queryset. Essas informações já foram pré-processadas no índice invertido `wordHash`.\n",
    "* **Interseção de tuplas:** Esta parte acontece no algoritmo `TSInter` e é responsável por encontrar tuplas que contém mais de uma das palavras-chave. Além disso, esta etapa irá garantir que os tuple-sets `TABLE{word}` contenham apenas a palavra `word` e nenhuma outra palavra do queryset. Esta propriedade é necessária para encontrar a cobertura mínima (etapa de criação de query matches). \n",
    "* **Criação de tuple-sets:** Esta parte irá condensar os resultados. Em vez de listar todas as tuplas que contenham as palavras-chave, precisamos apenas saber quais colunas possuem cada uma das palavras. Por isso, os tuple-sets terão a estrutura (o primeiro atributo refere-se a *value* ou *schema*):\n",
    "```python\n",
    "TupleSet = ('table','column', frozenset({schemaWords}), frozenset({valueWords}))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:10.507069Z",
     "start_time": "2018-09-10T14:19:10.492763Z"
    }
   },
   "outputs": [],
   "source": [
    "def TSFind(Q):\n",
    "    #Input:  A keyword query Q=[k1, k2, . . . , km]\n",
    "    #Output: Set of non-free and non-empty tuple-sets Rq\n",
    "\n",
    "    '''\n",
    "    The tuple-set Rki contains the tuples of Ri that contain all\n",
    "    terms of K and no other keywords from Q\n",
    "    '''\n",
    "    \n",
    "    #Part 1: Find sets of tuples containing each keyword\n",
    "    global P\n",
    "    P = {}\n",
    "    for keyword in Q:\n",
    "        tupleset = set()\n",
    "        \n",
    "        if keyword not in wordHash:\n",
    "            continue\n",
    "        \n",
    "        for (table,attributes) in wordHash.get(keyword)[1].items():\n",
    "            for (attribute,ctids) in attributes.items():\n",
    "                for ctid in ctids:\n",
    "                    tupleset.add( (table,attribute,ctid) )\n",
    "        P[frozenset([keyword])] = tupleset\n",
    "    \n",
    "    #Part 2: Find sets of tuples containing larger termsets\n",
    "    P = TSInter(P)\n",
    "    \n",
    "    #Part 3:Build tuple-sets\n",
    "    Rq = set()\n",
    "    \n",
    "    schemaWords = frozenset()\n",
    "    for valueWords , tuples in P.items():\n",
    "        for (table,attribute,ctid) in tuples:\n",
    "            Rq.add( (table,attribute,schemaWords,valueWords) )\n",
    "    #print ('TUPLE SETS CREATED')\n",
    "    return Rq\n",
    "\n",
    "\n",
    "def TSInter(P):\n",
    "    #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "    #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    '''\n",
    "    Termset is any non-empty subset K of the terms of a query Q        \n",
    "    '''\n",
    "    \n",
    "    Pprev = {}\n",
    "    Pprev=copy.deepcopy(P)\n",
    "    Pcurr = {}\n",
    "\n",
    "    combinations = [x for x in itertools.combinations(Pprev.keys(),2)]\n",
    "    for ( Ki , Kj ) in combinations:\n",
    "        Tki = Pprev[Ki]\n",
    "        Tkj = Pprev[Kj]\n",
    "        \n",
    "        X = Ki | Kj\n",
    "        Tx = Tki & Tkj        \n",
    "        \n",
    "        if len(Tx) > 0:            \n",
    "            Pcurr[X]  = Tx            \n",
    "            Pprev[Ki] = Tki - Tx         \n",
    "            Pprev[Kj] = Tkj - Tx\n",
    "            \n",
    "    if Pcurr != {}:\n",
    "        Pcurr = copy.deepcopy(TSInter(Pcurr))\n",
    "        \n",
    "    #Pprev = Pprev U Pcurr\n",
    "    Pprev.update(Pcurr)     \n",
    "    return Pprev   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ['actor', 'james', 'bond']\n",
    "Rq = TSFind(Q)\n",
    "pp(Rq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação Schema-sets\n",
    "\n",
    "Esta etapa consiste na criação dos Schema-sets, que é uma estrutura análoga aos tuple-sets vistos na etapa anterior. Aqui, o processo também é divido em três partes: \n",
    "* **Mapeamento de Elementos do Esquema (*Schema Matching*):** Essa parte consiste em analisar a similaridade entre as palavras do querysets e elementos do esquema (nomes de relações e atributos).\n",
    "* **Análise de Termos Adjacentes:** Esta parte verifica as relações entre as palavras chave, muitas vezes uma palavras-chave relacioada a elemento do esquema delimita o domínio das palavras-chave adjacentes. Ex: Actor James Bond delimita a palavra James para nome de Pessoa, em vez de nome de Filme.\n",
    "* **Criação de Schema-sets:** Esta parte irá formatar os resultados para ficarem semelhantes à estrutura de tuple-sets, seguindo a estrutura a seguir (o primeiro atributo refere-se a *value* ou *schema*):\n",
    "```python\n",
    "SchemaSet = ('s','table','column', frozenset({words}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similaridades para o Schema-Matching\n",
    "\n",
    "Para o mapeamento de palavras para elementos do esquema, foram utilizadas métricas de similaridade de escrita e semântica.\n",
    "O Coeficiente de Jaccard é uma métrica que avalia a interseção entre duas palavras, sendo ideal para similaridades de escrita, como abreviações ou erros de digitação. \n",
    "\n",
    "Por outro lado, as métricas semânticas utilizam o dicionário léxico WordNet para encontrar similaridades de sentido. O pacote de ferramentas NLTK disponibiliza uma série de métricas semânticas [aqui](http://www.nltk.org/howto/wordnet.html \"WordNet Interface\"). Entre elas, as principais são a Path Similarity e a Wu-Palmer Similarity. A primeira métrica procura encontrar a menor distância entre duas palavras, no grafo de relações do WordNet, enquanto a segunda analisa o ancestral comum mais próximo entre duas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:13.436391Z",
     "start_time": "2018-09-10T14:19:13.426925Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordnet_similarity(wordA,wordB):\n",
    "    \n",
    "    A = set(wn.synsets(wordA))\n",
    "    B = set(wn.synsets(wordB))\n",
    "    \n",
    "    wupSimilarities = [0]\n",
    "    pathSimilarities = [0]\n",
    "    for (sense1,sense2) in itertools.product(A,B):        \n",
    "        wupSimilarities.append(wn.wup_similarity(sense1,sense2) or 0)\n",
    "        pathSimilarities.append(wn.path_similarity(sense1,sense2) or 0)\n",
    "    return max(max(wupSimilarities),max(pathSimilarities))\n",
    "\n",
    "def jaccard_similarity(wordA,wordB):\n",
    "    \n",
    "    A = set(wordA)\n",
    "    B = set(wordB)\n",
    "    \n",
    "    return len(A & B ) / len(A | B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:14.784801Z",
     "start_time": "2018-09-10T14:19:14.766128Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSchemaGraph():\n",
    "    #Output: A Schema Graph G  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "    \n",
    "    \n",
    "    G = {} \n",
    "    cur.execute(\"SELECT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\")\n",
    "    for table in cur.fetchall():\n",
    "        G.setdefault(table[0],{})\n",
    "    \n",
    "    sql = \"SELECT DISTINCT                 tc.table_name, kcu.column_name,                 ccu.table_name AS foreign_table_name, ccu.column_name AS foreign_column_name             FROM information_schema.table_constraints AS tc              JOIN information_schema.key_column_usage AS kcu                 ON tc.constraint_name = kcu.constraint_name             JOIN information_schema.constraint_column_usage AS ccu                 ON ccu.constraint_name = tc.constraint_name             WHERE constraint_type = 'FOREIGN KEY'\"\n",
    "    cur.execute(sql)\n",
    "    relations = cur.fetchall()\n",
    "    \n",
    "    for (table,column,foreign_table,foreign_column) in relations:\n",
    "        G[table][foreign_table] = (1,column, foreign_column)\n",
    "        G[foreign_table][table] = (-1,foreign_column,column)\n",
    "    print ('SCHEMA CREATED')\n",
    "    return G\n",
    "G = getSchemaGraph()\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:15.386258Z",
     "start_time": "2018-09-10T14:19:15.380000Z"
    }
   },
   "outputs": [],
   "source": [
    "def createEmbeddingsHash(model,attributeHash,weight=0.5):\n",
    "    \n",
    "    wordEmbeddingsHashA = {}\n",
    "    \n",
    "    for table in attributeHash:\n",
    "        \n",
    "        if table not in model:\n",
    "            continue\n",
    "        \n",
    "        wordEmbeddingsHashA[table]={word.lower() for word,sim in model.most_similar(table)}\n",
    "        #wordEmbeddingsHashA[table]={wnl.lemmatize(word).lower() for word,sim in model.most_similar(table)}\n",
    "            \n",
    "        for column in attributeHash[table]:\n",
    "            if column not in model or column=='id':\n",
    "                continue\n",
    "            wordEmbeddingsHashA[column]={wnl.lemmatize(word).lower() for word,sim in model.most_similar(column)}\n",
    "            #wordEmbeddingsHashA[column]={wnl.lemmatize(word).lower() for word,sim in model.most_similar(column)}\n",
    "    \n",
    "    wordEmbeddingsHashB = copy.deepcopy(wordEmbeddingsHashA)\n",
    "    \n",
    "    for table in attributeHash:\n",
    "        \n",
    "        if table not in model:\n",
    "            continue\n",
    "        \n",
    "        for column in attributeHash[table]:\n",
    "            \n",
    "            if column not in model or column=='id':\n",
    "                continue\n",
    "            \n",
    "            similarSet = { wnl.lemmatize(word).lower() for word,sim in model.most_similar(positive=(table,column))}\n",
    "            wordEmbeddingsHashB[column].update(similarSet)\n",
    "            \n",
    "    G = getSchemaGraph()\n",
    "    for tableA in G:\n",
    "        \n",
    "        if tableA not in model:\n",
    "            continue\n",
    "        \n",
    "        for tableB in G[tableA]:\n",
    "            \n",
    "            if tableB not in model:\n",
    "                continue\n",
    "            \n",
    "            similarSet = { wnl.lemmatize(word).lower() for word,sim in model.most_similar(positive=(tableA,tableB))}\n",
    "            wordEmbeddingsHashB[tableA].update(similarSet)\n",
    "            wordEmbeddingsHashB[tableB].update(similarSet)\n",
    "            \n",
    "            \n",
    "            \n",
    "    wordEmbeddingsHashC = copy.deepcopy(wordEmbeddingsHashA)\n",
    "    \n",
    "    for table in attributeHash:\n",
    "        \n",
    "        if table not in model:\n",
    "            continue\n",
    "        \n",
    "        for column in attributeHash[table]:\n",
    "            \n",
    "            if column not in model or column=='id':\n",
    "                continue\n",
    "            \n",
    "            avg_vec = (model[table]*weight + model[column]*(1-weight))   \n",
    "            similarSet = { wnl.lemmatize(word).lower() \n",
    "                          for word,sim in model.similar_by_vector(avg_vec)}\n",
    "            wordEmbeddingsHashC[column].update(similarSet)\n",
    "            \n",
    "    G = getSchemaGraph()\n",
    "    for tableA in G:\n",
    "        \n",
    "        if tableA not in model:\n",
    "            continue\n",
    "        \n",
    "        for tableB in G[tableA]:\n",
    "            \n",
    "            if tableB not in model:\n",
    "                continue\n",
    "            \n",
    "            avg_vec = (model[table]*weight + model[column]*(1-weight))\n",
    "            similarSet = { wnl.lemmatize(word).lower() \n",
    "                          for word,sim in model.similar_by_vector(avg_vec)}\n",
    "            wordEmbeddingsHashC[tableA].update(similarSet)            \n",
    "    \n",
    "    return wordEmbeddingsHashA,wordEmbeddingsHashB,wordEmbeddingsHashC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:15.929216Z",
     "start_time": "2018-09-10T14:19:15.924632Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding10_similarity(schema,word,wordEmbeddingsHash):\n",
    "    if schema not in wordEmbeddingsHash:\n",
    "        return 0\n",
    "    \n",
    "    #lemmatize is used to remove plural form   wnl.lemmatize('wolves')='wolf'\n",
    "    if wnl.lemmatize(word) in wordEmbeddingsHash[schema]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:16.475087Z",
     "start_time": "2018-09-10T14:19:16.467625Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding_similarity(wordA,wordB,model):\n",
    "    if wordA not in model or wordB not in model:\n",
    "        return 0\n",
    "    return model.similarity(wordA,wordB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo para Criação dos Schema-Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(schema_term,word,\n",
    "                    wn_sim=True, jaccard_sim=True,\n",
    "                    emb_sim=False,  emb_model=None,\n",
    "                    emb10_sim=False, emb10_hash=None):\n",
    "    \n",
    "    \n",
    "    sim_list=[0]\n",
    "    \n",
    "    if wn_sim:\n",
    "        sim_list.append( wordnet_similarity(schema_term,word) )\n",
    "\n",
    "    if jaccard_sim:\n",
    "        sim_list.append( jaccard_similarity(schema_term,word) )\n",
    "\n",
    "    if emb_sim and emb_model is not None:\n",
    "        sim_list.append( embedding_similarity(schema_term,word,emb_model) )\n",
    "\n",
    "    sim = max(sim_list) \n",
    "\n",
    "    if emb10_sim and emb10_hash is not None:\n",
    "        if embedding10_similarity(schema_term,word,emb10_hash) == 0:\n",
    "            sim=0\n",
    "        else:\n",
    "            if len(sim_list)==1:\n",
    "                sim=1\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SchSFind(Q,threshold=0.8, \n",
    "             sim_args={}):    \n",
    "    S = []\n",
    "    for keyword in Q:\n",
    "        for (table,values) in attributeHash.items():\n",
    "            \n",
    "            sim = word_similarity(table,keyword,**sim_args)\n",
    "            \n",
    "            if sim >= threshold:\n",
    "                S.append( (table,'*',{keyword},sim) )\n",
    "            \n",
    "            for attribute in values.keys():\n",
    "                \n",
    "                if(attribute=='id'):\n",
    "                    continue\n",
    "                \n",
    "                sim = word_similarity(attribute,keyword,**sim_args)\n",
    "                \n",
    "                if sim >= threshold:\n",
    "                    S.append( (table,attribute,{keyword},sim) )\n",
    "    #S = SchSInter(S)\n",
    "\n",
    "    #print ('SCHEMA SETS CREATED')\n",
    "    valueWords = frozenset()\n",
    "    Sq = {(table,attribute,frozenset(schemaWords),valueWords) for (table,attribute,schemaWords,sim) in S}\n",
    "        \n",
    "    return Sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEmbeddingsModel=loadWordEmbeddingsModel()\n",
    "(wordEmbeddingsHashA,wordEmbeddingsHashB,wordEmbeddingsHashC) = createEmbeddingsHash(wordEmbeddingsModel,attributeHash,weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:24.664069Z",
     "start_time": "2018-09-10T14:19:24.660398Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = ['actor', 'james', 'bond']\n",
    "SimilarityCoeficient = 0.799999999999\n",
    "Sq = SchSFind(Q,SimilarityCoeficient,{'emb10_sim':True,'emb10_hash':wordEmbeddingsHashB})\n",
    "Sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Query Matches\n",
    "\n",
    "As etapas anteriores, de criação de schema-sets e tuple-sets, foram responsáveis por identificar quais relações possuem alguma informação sobre as palavras-chave. Nesta etapa de criação de full matches, o objetivo é combinar esses tuple-sets e schema-sets para se obter uma resposta completa, mínima e relevante para o usuário. \n",
    "\n",
    "O algoritmo `QMGen` é responsável por encontrar combinações de tuple-sets/schema-sets que compõem uma cobertura mínima (`MinimalCover`) sobre o queryset.\n",
    "- **Total**: Cada palavra-chave deve estar presente em ao menos uma das tuplas da query-match.\n",
    "- **Mínima**: Não é possível remover nenhum tuple-set/schema-set da query-match e manter a cobertura total sobre o queryset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:25.756525Z",
     "start_time": "2018-09-10T14:19:25.752788Z"
    }
   },
   "outputs": [],
   "source": [
    "def MinimalCover(MC, Q):\n",
    "    #Input:  A subset MC (Match Candidate) to be checked as total and minimal cover\n",
    "    #Output: If the match candidate is a TOTAL and MINIMAL cover\n",
    "\n",
    "    Subset = [schemaWords|valueWords for table,attribute,schemaWords,valueWords in MC]\n",
    "    u = set().union(*Subset)    \n",
    "    \n",
    "    isTotal = (u == set(Q))\n",
    "    for element in Subset:\n",
    "        \n",
    "        new_u = list(Subset)\n",
    "        new_u.remove(element)\n",
    "        \n",
    "        new_u = set().union(*new_u)\n",
    "        \n",
    "        if new_u == set(Q):\n",
    "            return False\n",
    "    \n",
    "    return isTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:27.066815Z",
     "start_time": "2018-09-10T14:19:27.057492Z"
    }
   },
   "outputs": [],
   "source": [
    "def QMGen(Q,Rq):\n",
    "    #Input:  A keyword query Q, The set of non-empty non-free tuple-sets Rq\n",
    "    #Output: The set Mq of query matches for Q\n",
    "    \n",
    "    '''\n",
    "    Query match is a set of tuple-sets that, if properly joined,\n",
    "    can produce networks of tuples that fulfill the query. They\n",
    "    can be thought as the leaves of a Candidate Network.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Mq = []\n",
    "    for i in range(1,len(Q)+1):\n",
    "        for subset in itertools.combinations(Rq,i):            \n",
    "            if(MinimalCover(subset,Q)):\n",
    "                #print('----------------------------------------------\\nM')\n",
    "                #pp(set(subset))\n",
    "                #print('\\n')\n",
    "                M = MInter(set(subset))\n",
    "                #pp(M)\n",
    "                Mq.append(M)\n",
    "                \n",
    "                \n",
    "    return Mq\n",
    "\n",
    "def MInter(M):\n",
    "    #print('M',M)\n",
    "    Mprev = copy.deepcopy(M)\n",
    "    Mcurr = set()\n",
    "\n",
    "    combinations = [x for x in itertools.combinations(Mprev,2)]\n",
    "\n",
    "    \n",
    "    for ( (tableA,attributeA,schemaWordsA,valueWordsA) , (tableB,attributeB,schemaWordsB,valueWordsB) ) in combinations:\n",
    "          \n",
    "        #se  forem tabelas diferentes ou não tiverem value words mapeadas em ambos os tuplesets\n",
    "        if (tableA!=tableB) or (len(valueWordsA)>0 and len(valueWordsB)>0):\n",
    "            continue             \n",
    "        \n",
    "        tableC=tableA\n",
    "        \n",
    "        if len(valueWordsA)>0:\n",
    "            attributeC=attributeA\n",
    "        else:\n",
    "            attributeC=attributeB\n",
    "        \n",
    "        schemaWordsC = schemaWordsA|schemaWordsB\n",
    "        valueWordsC  = valueWordsA | valueWordsB #levando em consideração que um deles é vazio\n",
    "        \n",
    "        Mcurr.add( (tableC,attributeC,frozenset(schemaWordsC),frozenset(valueWordsC)) )\n",
    "        \n",
    "        Mprev = Mprev - {(tableA,attributeA,schemaWordsA,valueWordsA)}\n",
    "        Mprev = Mprev - {(tableB,attributeB,schemaWordsB,valueWordsB)}\n",
    "            \n",
    "    if len(Mcurr)>0:\n",
    "        Mcurr = copy.deepcopy(MInter(Mcurr))\n",
    "        \n",
    "    Mprev.update(Mcurr)     \n",
    "    return Mprev   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ['actor', 'james', 'bond']\n",
    "\n",
    "Rq = TSFind(Q)\n",
    "\n",
    "SimilarityCoeficient = 0.799999999999\n",
    "Sq = SchSFind(Q,SimilarityCoeficient,{'emb10_sim':True,'emb10_hash':wordEmbeddingsHashB})\n",
    "\n",
    "Mq= QMGen(Q,Rq|Sq)\n",
    "\n",
    "for element in Mq:\n",
    "    pp(element)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:27.575984Z",
     "start_time": "2018-09-10T14:19:27.566119Z"
    }
   },
   "outputs": [],
   "source": [
    "def QMRank(Mq,mi,smi,sim_args={}):\n",
    "    Ranking = []\n",
    "    for M in Mq:\n",
    "        cosprod = schemaprod = 1\n",
    "        thereIsValueTerms = thereIsSchemaTerms = False\n",
    "        \n",
    "        for (table,attribute,schemaWords,valueWords) in M:           \n",
    "            \n",
    "            if (len(valueWords)>0):\n",
    "                \n",
    "                thereIsValueTerms=True\n",
    "                \n",
    "                (norm_attribute,distinct_terms) = attributeHash[table][attribute]\n",
    "\n",
    "                wsum = 0\n",
    "\n",
    "                for term in valueWords:\n",
    "\n",
    "                    IAF = wordHash[term][0]\n",
    "\n",
    "                    ctids = wordHash[term][1][table][attribute]\n",
    "                    fkj = len(ctids)\n",
    "\n",
    "                    if fkj>0:\n",
    "\n",
    "                        TF = log1p(fkj) / log1p(distinct_terms)\n",
    "\n",
    "                        wsum = wsum + TF*IAF\n",
    "                \n",
    "                cos = wsum/norm_attribute\n",
    "                cosprod *= cos\n",
    "                \n",
    "            if (len(schemaWords)>0):\n",
    "                \n",
    "                thereIsSchemaTerms=True\n",
    "                \n",
    "                if(attribute == '*'):\n",
    "                    schemaElement = table\n",
    "                else:\n",
    "                    schemaElement = attribute\n",
    "                \n",
    "                schemasum = 0\n",
    "                \n",
    "                for term in schemaWords:\n",
    "                    schemasum+=word_similarity(schemaElement,term,sim_args)\n",
    "                \n",
    "                schemaprod *= schemasum\n",
    "                \n",
    "        valuescore = schemascore = 0\n",
    "        \n",
    "        # O tamanho da query match não está sendo considerado no ranking, mas será analisado no ranking de Cns.\n",
    "        #score = 1/len(M)\n",
    "        score = 1.0\n",
    "        \n",
    "        if thereIsValueTerms:\n",
    "            valuescore = mi * cosprod \n",
    "            score*=valuescore\n",
    "        \n",
    "        if thereIsSchemaTerms:\n",
    "           \n",
    "            schemascore = smi * schemaprod\n",
    "            score*=schemascore\n",
    "            \n",
    "        Ranking.append( (M,score,schemascore,valuescore) )\n",
    "    return sorted(Ranking,key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:28.259241Z",
     "start_time": "2018-09-10T14:19:28.254954Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mi = 46457610.86662768\n",
    "smi = 1\n",
    "\n",
    "RankedMq = QMRank(Mq,mi,smi)\n",
    "\n",
    "\n",
    "for (j, (M,score,schemascore,valuescore) ) in enumerate(RankedMq):\n",
    "    if j>10:\n",
    "        break\n",
    "    print(j+1,'ª QM')\n",
    "    print('Schema Score:',\"%.8f\" % schemascore,\n",
    "          '\\nValue Score: ',\"%.8f\" % valuescore,\n",
    "          '\\n|M|: ',\"%02d (Não considerado para calcular o total score)\" % len(M),\n",
    "          '\\nTotal Score: ',\"%.8f\" % score)\n",
    "    pp(M)\n",
    "    print('----------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação e Ranking de Candidate Networks\n",
    "\n",
    "Na etapa anterior, obteve-se as full matches, que compreendem todas as informações necessárias para o usuário. O próximo passo é encontrar maneiras de conectar estas informações para formar uma resposta para o usuário. Estas conexões, chamadas de candidate networks, são derivadas das restrições de integridade referencial do banco de dados, também conhecidas como chaves estrangeiras.\n",
    "\n",
    "A criação de candidate networks utiliza dois grafos:\n",
    "- **Schema Graph**: vértice que representa o banco de dados e é utilizado como base para o match graph. Ele contém como vértices os free tuple-sets associados a cada relação do banco de dados e como arestas as restrições de integridade referencial.\n",
    "\n",
    "    O Schema Graph foi implementado como um dicionário, no qual cada vértice aponta para um outro vértice. Além disso, também é armazenada informações sobre as arestas, como direção e quais atributos entre as tabelas tem a relação de restrição referencial. A estrutura do Schema Graph pode ser observada a seguir:\n",
    "   \n",
    "```python\n",
    "    G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "```\n",
    "\n",
    "Como existem diferentes maneiras de se conectar as informações associadas as palavras-chave, várias candidate networks serão geradas. Entretanto, na maioria das vezes, apenas uma delas contém uma resposta relevante para o usuário. Por este motivo, esta esta etapa irá ranquear as candidate networks por relevância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:29.314228Z",
     "start_time": "2018-09-10T14:19:29.305096Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSchemaGraph():\n",
    "    #Output: A Schema Graph G  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "    \n",
    "    \n",
    "    G = {} \n",
    "    cur.execute(\"SELECT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\")\n",
    "    for table in cur.fetchall():\n",
    "        G.setdefault(table[0],{})\n",
    "    \n",
    "    sql = '''\n",
    "        SELECT DISTINCT\n",
    "            tc.table_name, kcu.column_name,\n",
    "            ccu.table_name AS foreign_table_name, ccu.column_name AS foreign_column_name             \n",
    "        FROM\n",
    "            information_schema.table_constraints AS tc\n",
    "            JOIN information_schema.key_column_usage AS kcu \n",
    "                ON tc.constraint_name = kcu.constraint_name\n",
    "            JOIN information_schema.constraint_column_usage AS ccu \n",
    "                ON ccu.constraint_name = tc.constraint_name\n",
    "        WHERE constraint_type = 'FOREIGN KEY'\n",
    "    '''\n",
    "    cur.execute(sql)\n",
    "    relations = cur.fetchall()\n",
    "    \n",
    "    for (table,column,foreign_table,foreign_column) in relations:\n",
    "        G[table][foreign_table] = (1,column, foreign_column)\n",
    "        G[foreign_table][table] = (-1,foreign_column,column)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = getSchemaGraph()\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Match Graph**: grafo gerado a partir de uma query match e o schema graph. No entanto, no match graph tuple-sets/schema-sets também são modelados como vértices. Para criá-lo, adiciona-se ao schema graph os tuple-sets/schema-sets presentes na query match. Um tuple-set de uma tabela x terá os mesmos relacionamentos (arestas) que o vértice x.\n",
    "\n",
    "```python\n",
    "    Gts['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "\n",
    "    Gts[('s','table','column', frozenset({words}))] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:30.283641Z",
     "start_time": "2018-09-10T14:19:30.279644Z"
    }
   },
   "outputs": [],
   "source": [
    "def MatchGraph(Rq, G, M):\n",
    "    #Input:  The set of non-empty non-free tuple-sets Rq,\n",
    "    #        The Schema Graph G,\n",
    "    #        A Query Match M\n",
    "    #Output: A Schema Graph Gts  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "\n",
    "    '''\n",
    "    A Match Subgraph Gts[M] is a subgraph of G that contains:\n",
    "        The set of free tuple-sets of G\n",
    "        The query match M\n",
    "    '''\n",
    "    \n",
    "    Gts = copy.deepcopy(G)\n",
    "    \n",
    "    #Insert non-free nodes\n",
    "    for (table ,attribute, schemaWords, valueWords) in M:\n",
    "        Gts[(table ,attribute, schemaWords, valueWords)]=copy.deepcopy(Gts[table])\n",
    "        for foreign_table , (direction,column,foreign_column) in Gts[(table ,attribute, schemaWords, valueWords)].items():\n",
    "            Gts[foreign_table][(table ,attribute, schemaWords, valueWords)] = (direction*(-1),foreign_column,column)\n",
    "    return Gts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:30.825969Z",
     "start_time": "2018-09-10T14:19:30.822600Z"
    }
   },
   "outputs": [],
   "source": [
    "M = RankedMq[0][0]\n",
    "Gts = MatchGraph(Rq|Sq, G, M)\n",
    "\n",
    "print('QM:')\n",
    "pp(M)\n",
    "print('\\nGts:')\n",
    "pp(Gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo para Criação e Ranking de Candidate Networks\n",
    "\n",
    "Para criar uma candidate network, o algoritmo `SingleCN` procura um caminho mínimo no match graph que visite todas os non-free tuple-sets/schema-sets da query match. \n",
    "\n",
    "Este caminho deve ser:\n",
    "- **Mínimo:** garantido através do algoritmo de caminho mínima baseado em busca por largura (BFS).\n",
    "- **Total:** a função `containsMatch` garante que todos os tuple-sets/schema-sets da query match sejam visitados.\n",
    "- **Seguro (*Sound*):** uma joining networks of tuple-sets é considerado sound se ela não contém uma subárvore na forma $R^K - S^L - R^M $, na qual $R$ e $S$ são relações e o schema graph tem uma aresta $R \\rightarrow S$.\n",
    "\n",
    "O ranking das Candidate Networks agora é feito parcialmente na etapa de ranking de Query Matches. Restando apenas penalizar Candidate Networks grandes, dividindo o score pelo seu tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:32.097342Z",
     "start_time": "2018-09-10T14:19:32.092844Z"
    }
   },
   "outputs": [],
   "source": [
    "def containsMatch(Ji,M):\n",
    "    for relation in M:\n",
    "        if relation not in Ji:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def isJNTSound(Gts,Ji):\n",
    "    if len(Ji)<3:\n",
    "        return True\n",
    "    \n",
    "    for i in range(len(Ji)-2):\n",
    "        \n",
    "        if type(Ji[i]) is str:\n",
    "            tableA = Ji[i]\n",
    "        else:\n",
    "            (tableA,attributeA,schemaWordsA,valueWordsA) = Ji[i]\n",
    "            \n",
    "        if type(Ji[i+2]) is str:\n",
    "            tableB = Ji[i+2]\n",
    "        else:\n",
    "            (tableB,attributeB,schemaWordsB,valueWordsB) = Ji[i+2]         \n",
    "            \n",
    "        if tableA==tableB:\n",
    "            edge_info = Gts[Ji[i]][Ji[i+1]]\n",
    "            if(edge_info[0] == -1):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:32.497392Z",
     "start_time": "2018-09-10T14:19:32.484919Z"
    }
   },
   "outputs": [],
   "source": [
    "def SingleCN(FM,Gts,Tmax,showLog=False):  \n",
    "  \n",
    "    if showLog:\n",
    "        print('================================================================================\\nSINGLE CN')\n",
    "        print('Tmax ',Tmax)\n",
    "        print('FM')\n",
    "        pp(FM)\n",
    "\n",
    "        print('\\n\\nGts')\n",
    "        pp(Gts)\n",
    "        print('\\n\\n')\n",
    "    \n",
    "    F = deque()\n",
    "\n",
    "    first_element = list(FM)[0]\n",
    "    J = [first_element]\n",
    "    \n",
    "    if len(FM)==1:\n",
    "        return J\n",
    "    \n",
    "    F.append(J)\n",
    "    \n",
    "    while F:\n",
    "        J = F.popleft()           \n",
    "        u = J[-1]\n",
    "        \n",
    "        sortedAdjacents = sorted(Gts[u].items(),key=lambda x : type(x[0]) is str)\n",
    "        \n",
    "        if showLog:\n",
    "            print('--------------------------------------------\\nParctial CN')\n",
    "            print('J ',J,'\\n')\n",
    "\n",
    "            print('\\nAdjacents:')\n",
    "            pp(Gts[u].items())\n",
    "            \n",
    "            print('\\nSorted Adjacents:')\n",
    "            pp(sortedAdjacents)\n",
    "            \n",
    "            print('F:')\n",
    "            pp(F)\n",
    "    \n",
    "        for (adjacent,edge_info) in sortedAdjacents:\n",
    "            if showLog:\n",
    "                pp(adjacent)\n",
    "                print('is str',(type(adjacent) is str),'notinJ',(adjacent not in J))\n",
    "            if (type(adjacent) is str) or (adjacent not in J):\n",
    "                Ji = J + [adjacent]\n",
    "                \n",
    "                \n",
    "                if (Ji not in F) and (len(Ji)<Tmax) and (isJNTSound(Gts,Ji)):\n",
    "                    \n",
    "                    if showLog:\n",
    "                        print('isSound:')\n",
    "                    \n",
    "                    if(containsMatch(Ji,FM)):\n",
    "                        \n",
    "                        if showLog:\n",
    "                            print('--------------------------------------------\\nGenerated CN')\n",
    "                            print('J ',Ji,'\\n')\n",
    "                        \n",
    "                        return Ji\n",
    "                    else:\n",
    "                        F.append(Ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:32.855780Z",
     "start_time": "2018-09-10T14:19:32.853576Z"
    }
   },
   "outputs": [],
   "source": [
    "SingleCN(M,Gts,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in range(10)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:33.217638Z",
     "start_time": "2018-09-10T14:19:33.211084Z"
    }
   },
   "outputs": [],
   "source": [
    "def MatchCN(G,Sq,Rq,RankedMq,topK=10):    \n",
    "    Cns = []                        \n",
    "    for  (M,score,schemascore,valuescore) in RankedMq[:topK]:\n",
    "        Gts = MatchGraph(Rq|Sq, G, M)\n",
    "        Cn = SingleCN(M,Gts,10)\n",
    "        if(Cn is not None):\n",
    "            \n",
    "            \n",
    "            #Dividindo score pelo tamanho da cn (SEGUNDA PARTE DO RANKING)\n",
    "            \n",
    "            CnScore = score/len(Cn)\n",
    "            \n",
    "            Cns.append( (Cn,Gts,M,CnScore,schemascore,valuescore) )\n",
    "    \n",
    "    #Ordena CNs pelo CnScore\n",
    "    RankedCns=sorted(Cns,key=lambda x: x[3],reverse=True)\n",
    "    \n",
    "    return RankedCns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RankedCns = MatchCN(G,Sq,Rq,RankedMq)\n",
    "for (j, (Cn,Gts,M,score,schemascore,valuescore) ) in enumerate(RankedCns):\n",
    "    if j>10:\n",
    "        break\n",
    "    print(j+1,'ª CN')\n",
    "    print('Schema Score:',\"%.8f\" % schemascore,\n",
    "          '\\nValue Score: ',\"%.8f\" % valuescore,\n",
    "          '\\n|Cn|: ',\"%02d (Considerado para o Total Score)\" % len(Cn),\n",
    "          '\\nTotal Score: ',\"%.8f\" % score)\n",
    "    pp(Cn)\n",
    "    print('----------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSQLfromCN(Gts,Cn):\n",
    "    #print('CN:\\n',Cn)\n",
    "    \n",
    "    selected_attributes = [] \n",
    "    tables = []\n",
    "    conditions=[]\n",
    "    relationships = []\n",
    "    \n",
    "    for i in range(len(Cn)):\n",
    "        \n",
    "        if(type(Cn[i]) is str):\n",
    "            tableA = Cn[i]\n",
    "            attrA=''\n",
    "            valueWords=[]\n",
    "        else:\n",
    "            (tableA,attrA, _ ,valueWords) = Cn[i]             \n",
    "                \n",
    "        A = 't' + str(i)\n",
    "        \n",
    "        if(attrA != ''):\n",
    "            selected_attributes.append(A +'.'+ attrA)\n",
    "        \n",
    "        tables.append(tableA+' '+A)\n",
    "            \n",
    "        #tratamento de keywords\n",
    "        for term in valueWords:\n",
    "            condition = 'CAST('+A +'.'+ attrA + ' AS VARCHAR) ILIKE \\'%' + term + '%\\''\n",
    "            conditions.append(condition)\n",
    "        \n",
    "        if(i<len(Cn)-1):\n",
    "            if(type(Cn[i+1]) is str):\n",
    "                tableB = Cn[i+1]\n",
    "            else:\n",
    "                (tableB,attrB, _ , _ )=Cn[i+1]\n",
    "                  \n",
    "            B = 't'+str(i+1)\n",
    "            \n",
    "            edge_info = Gts[Cn[i]][Cn[i+1]]\n",
    "            (direction,joining_attrA,joining_attrB) = edge_info\n",
    "            \n",
    "            relationships.append( (A,B) )\n",
    "            \n",
    "            condition = A + '.' + joining_attrA + ' = ' + B + '.' + joining_attrB         \n",
    "            conditions.append(condition)\n",
    "    \n",
    "    tables_id = ['t'+str(i)+'.__search_id' for i in range(len(tables))]\n",
    "    \n",
    "    relationshipsText = ['('+str(a)+'.__search_id'+','+str(b)+'.__search_id'+')' for (a,b) in relationships]\n",
    "    \n",
    "    \n",
    "    sqlText = 'SELECT '\n",
    "    sqlText +=' ('+', '.join(tables_id)+') AS Tuples '\n",
    "    if len(relationships)>0:\n",
    "        sqlText +=', ('+', '.join(relationshipsText)+') AS Relationships'\n",
    "        \n",
    "    sqlText += ' , ' + ' , '.join(selected_attributes)\n",
    "    \n",
    "    sqlText +=' FROM ' + ', '.join(tables)\n",
    "    sqlText +=' WHERE ' + ' AND '.join(conditions)\n",
    "    '''\n",
    "    print('SELECT:\\n',selected_attributes)\n",
    "    print('TABLES:\\n',tables)\n",
    "    print('CONDITIONS:')\n",
    "    pp(conditions)\n",
    "    print('RELATIONSHIPS:')\n",
    "    pp(relationships)\n",
    "    '''    \n",
    "    #print('SQL:\\n',sql)\n",
    "    return sqlText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (j, (Cn,Gts,M,score,schemascore,valuescore) ) in enumerate(RankedCns):\n",
    "    pp(Cn)\n",
    "    print('\\n',getSQLfromCN(Gts,Cn))\n",
    "    print('\\n--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGoldenStandards():\n",
    "    goldenStandards = {}\n",
    "    for i in range(1,51):\n",
    "        filename = 'golden_standards/0'+str(i).zfill(2) +'.txt'\n",
    "        with open(filename) as f:\n",
    "\n",
    "            listOfTuples = []\n",
    "            Q = ()\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "              \n",
    "                line_without_comment =line.split('#')[0]\n",
    "                \n",
    "                if(i==2):\n",
    "                    comment_of_line = line.split('#')[1]\n",
    "                    \n",
    "                    #Remove words not in OLIVEIRA experiments\n",
    "                    Q = tuple([word for word in comment_of_line.split() if word not in ['title','dr.',\"here's\",'char','name'] and word not in stw_set])\n",
    "                \n",
    "                if line_without_comment:                    \n",
    "                    \n",
    "                    relevantResult = eval(line_without_comment)\n",
    "                    listOfTuples.append( relevantResult )\n",
    "            \n",
    "            goldenStandards[Q]=listOfTuples\n",
    "            \n",
    "    return goldenStandards\n",
    "\n",
    "\n",
    "goldenStandards = getGoldenStandards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateCN(CnResult,goldenStandard):\n",
    "    '''\n",
    "    print('Verificar se são iguais:\\n')\n",
    "    print('Result: \\n',CnResult)\n",
    "    print('Golden Result: \\n',goldenStandard)\n",
    "    '''\n",
    "    \n",
    "    tuplesOfCNResult =  set(CnResult[0])\n",
    "    \n",
    "    tuplesOfStandard =  set(goldenStandard[0])\n",
    "        \n",
    "    #Check if the CN result have all tuples in golden standard\n",
    "    if tuplesOfCNResult.issuperset(tuplesOfStandard) == False:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    relationshipsOfCNResult = CnResult[1]\n",
    "    \n",
    "    for goldenRelationship in goldenStandard[1]:\n",
    "        \n",
    "        (A,B) = goldenRelationship\n",
    "        \n",
    "        if (A,B) not in relationshipsOfCNResult and (B,A) not in relationshipsOfCNResult:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "def evaluanteResult(Result,Query):\n",
    "    \n",
    "    goldenStandard = goldenStandards[tuple(Query)]\n",
    "    \n",
    "    for goldenRow in goldenStandard:\n",
    "\n",
    "        found = False\n",
    "\n",
    "        for row in Result:\n",
    "            if evaluateCN(row,goldenRow):\n",
    "                found = True\n",
    "\n",
    "        if not found:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "            \n",
    "            \n",
    "x=[('(39292828,5360667,21231023)', '(\"(39292828,5360667)\",\"(5360667,21231023)\")', 'Hamill, Mark', 'Luke Skywalker'), ('(39292828,5360749,21231023)', '(\"(39292828,5360749)\",\"(5360749,21231023)\")', 'Hamill, Mark', 'Luke Skywalker'), ('(39292828,5360752,21231023)', '(\"(39292828,5360752)\",\"(5360752,21231023)\")', 'Hamill, Mark', 'Luke Skywalker'), ('(39292828,5360753,21231023)', '(\"(39292828,5360753)\",\"(5360753,21231023)\")', 'Hamill, Mark', 'Luke Skywalker')]\n",
    "q = ['hamill', 'skywalker']\n",
    "\n",
    "def normalizeResult(ResultFromDatabase):\n",
    "    normalizedResult = []\n",
    "    \n",
    "    for row in ResultFromDatabase:        \n",
    "        if type(row[0]) == int:\n",
    "            tuples = [row[0]]\n",
    "        else:\n",
    "            tuples = eval(str(row[0]))\n",
    "        \n",
    "        try:\n",
    "            relationships = eval(row[1])\n",
    "            relationships = [eval(element) for element in relationships]\n",
    "        except:\n",
    "            relationships = []\n",
    "            \n",
    "        \n",
    "        normalizedResult.append( (tuples,relationships) )\n",
    "    return normalizedResult\n",
    "\n",
    "normX = normalizeResult(x)\n",
    "\n",
    "evaluanteResult(normX,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantPosition(RankedCns,Q):\n",
    "    \n",
    "    for (position,(Cn,Gts,M,score)) in enumerate(RankedCns):\n",
    "\n",
    "        #print('CN:\\n')\n",
    "        #pp(Cn)\n",
    "        \n",
    "        SQL = getSQLfromCN(Gts,Cn)\n",
    "\n",
    "        #print(SQL)\n",
    "        \n",
    "        cur.execute(SQL)\n",
    "        Results = cur.fetchall()\n",
    "\n",
    "        NResults = normalizeResult(Results)\n",
    "\n",
    "        Relevance = evaluanteResult(NResults,Q)\n",
    "\n",
    "        if Relevance == True:\n",
    "            return position+1\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais abaixo tem a execução para outras CNS (querysets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:37.857373Z",
     "start_time": "2018-09-10T14:19:37.851911Z"
    }
   },
   "outputs": [],
   "source": [
    "def preProcessing(emb_model=\"word_embeddings/word2vec/GoogleNews-vectors-negative300.bin\"):\n",
    "    global wordHash\n",
    "    global attributeHash\n",
    "    global wordEmbeddingsModel\n",
    "    global wordEmbeddingsHashA\n",
    "    global wordEmbeddingsHashB\n",
    "    global wordEmbeddingsHashC\n",
    "    \n",
    "    wordEmbeddingsModel=loadWordEmbeddingsModel(emb_model)\n",
    "    \n",
    "    (wordHash,attributeHash) = createInvertedIndex(wordEmbeddingsModel)\n",
    "    processIAF(wordHash,attributeHash)\n",
    "    processNormsOfAttributes(wordHash,attributeHash,wordEmbeddingsModel)\n",
    "    \n",
    "    (wordEmbeddingsHashA,wordEmbeddingsHashB,wordEmbeddingsHashC) = createEmbeddingsHash(wordEmbeddingsModel,attributeHash,weight=0.5)\n",
    "    \n",
    "    print('PRE-PROCESSING STAGE FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:19:38.337624Z",
     "start_time": "2018-09-10T14:19:38.320376Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(mi,smi,sim_args={},showLog=False,querySetFileName='querysets/queryset_imdb_martins.txt'):   \n",
    "    QuerySets = getQuerySets(querySetFileName)\n",
    "    maxscores = (list(),list())\n",
    "    for (i,Q) in enumerate(QuerySets):\n",
    "       \n",
    "        print('QUERY-SET ',Q,'\\n')\n",
    "        \n",
    "        print('FINDING TUPLE-SETS')\n",
    "        Rq = TSFind(Q)\n",
    "        print(len(Rq),'TUPLE-SETS CREATED\\n')\n",
    "        \n",
    "        print('FINDING SCHEMA-SETS')\n",
    "        SimilarityThreshold = 0.799999999999\n",
    "        Sq = SchSFind(Q,SimilarityThreshold,sim_args)\n",
    "\n",
    "        print(len(Sq),' SCHEMA-SETS CREATED\\n')\n",
    "        \n",
    "        print('GENERATING QUERY MATCHES')\n",
    "        Mq = QMGen(Q,Sq|Rq)\n",
    "        print (len(Mq),'QUERY MATCHES CREATED\\n')\n",
    "        \n",
    "        RankedMq = QMRank(Mq,mi,smi)\n",
    "        \n",
    "         \n",
    "        '''    \n",
    "        for (j, (M,score , ( valuescore , schemascore , tam )) ) in enumerate(RankedMq):\n",
    "            if j>10:\n",
    "                break\n",
    "            print(j+1,'ª QM')\n",
    "            print('Value Score: ',\"%.8f\" % valuescore,'\\nSchema Score:',\"%.8f\" % schemascore, '\\n|M|: ',tam,'\\nTotal Score: ',\"%.8f\" % score)\n",
    "            pp(M)\n",
    "            print('----------------------------------------------------------------------\\n')\n",
    "        '''    \n",
    "        Mq=[M for (M,score , ( valuescore , schemascore , tam )) in RankedMq][:20]\n",
    "        \n",
    "        if showLog:\n",
    "            for M in Mq[:20]:\n",
    "                pp(M)\n",
    "                print('\\n\\n')\n",
    "        \n",
    "        print('GENERATING CANDIDATE NETWORKS')\n",
    "        G = getSchemaGraph()\n",
    "        \n",
    "        Cns = MatchCN(G,Rq,Sq,Mq)\n",
    "        \n",
    "        print (len(Cns),'CANDIDATE NETWORKS CREATED\\n')\n",
    "        \n",
    "        if showLog:\n",
    "            for Cn in Cns[:20]:\n",
    "                pp(Cn[0])\n",
    "                print('\\n\\n')\n",
    "                #pp(Cn[1])\n",
    "                #print('\\n\\n\\n==================================================================================\\n')\n",
    "                \n",
    "        print('RANKING CANDIDATE NETWORKS')\n",
    "        RankedCns = CNRank(Cns,mi,smi)\n",
    "        for (j,Cn) in enumerate(RankedCns):\n",
    "            if j>10:\n",
    "                break\n",
    "            print(j+1,'ª CN')\n",
    "            print('Value Score: ',\"%.8f\" % Cn[4][0],'\\nSchema Score:',\"%.8f\" % Cn[4][1], '\\n|Cn|: ',Cn[4][2],'\\nTotal Score: ',\"%.8f\" % Cn[3])\n",
    "            pp(Cn[0])\n",
    "            print('----------------------------------------------------------------------\\n')\n",
    "        \n",
    "            maxscores[0].append(Cn[4][0])\n",
    "            maxscores[1].append(Cn[4][1])\n",
    "        gc.collect()\n",
    "        \n",
    "        print('==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================')\n",
    "    return maxscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:22:33.450094Z",
     "start_time": "2018-09-10T14:22:33.445627Z"
    }
   },
   "outputs": [],
   "source": [
    "#pp(wordHash['denzel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:27:38.642236Z",
     "start_time": "2018-09-10T14:27:38.635609Z"
    }
   },
   "outputs": [],
   "source": [
    "#pp(attributeHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-10T14:23:27.585Z"
    }
   },
   "outputs": [],
   "source": [
    "#pp(wordEmbeddingsHashA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-10T14:23:34.794Z"
    }
   },
   "outputs": [],
   "source": [
    "#pp(wordEmbeddingsHashB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:21:14.808276Z",
     "start_time": "2018-09-10T14:19:46.123Z"
    }
   },
   "outputs": [],
   "source": [
    "mi = 0.90/1.9372498568291752e-06\n",
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:21:14.861968Z",
     "start_time": "2018-09-10T14:19:46.703Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mi = 464576.1086662768\n",
    "smi = 1\n",
    "maxscores = main(mi,smi)\n",
    "maxscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observando os maxscores que eu atribui valores a constante mi para normalizar o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T14:18:52.615498Z",
     "start_time": "2018-09-10T14:18:51.138Z"
    }
   },
   "outputs": [],
   "source": [
    "#max(maxscores[0]), max(maxscores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimento(mi,smi,threshold,goldenMappings,sim_args={},\n",
    "                showLog=False,querySetFileName='querysets/queryset_imdb_martins.txt'):   \n",
    "    QuerySets = getQuerySets(querySetFileName)\n",
    "    \n",
    "    goldenMappings = goldenMappings.copy()\n",
    "    TP=[]\n",
    "    FP=[]\n",
    "    FN=[]\n",
    "    \n",
    "    for (i,Q) in enumerate(QuerySets):\n",
    "               \n",
    "        Sq = SchSFind(Q,threshold,sim_args)\n",
    "\n",
    "        for schema_mapping in Sq:\n",
    "\n",
    "            if schema_mapping in goldenMappings:\n",
    "                TP.append(schema_mapping)\n",
    "                goldenMappings.remove(schema_mapping)\n",
    "            else:\n",
    "                FP.append(schema_mapping)\n",
    "\n",
    "    FN=goldenMappings\n",
    "\n",
    "    #print('TP: ')\n",
    "    #pp(TP)\n",
    "\n",
    "    #print('FP: ')\n",
    "    #pp(FP)\n",
    "\n",
    "    #print('FN: ')\n",
    "    #pp(FN)\n",
    "    \n",
    "    tp=len(TP)\n",
    "    fp=len(FP)\n",
    "    fn=len(FN)\n",
    "    \n",
    "    #print(tp,fp,fn)\n",
    "    try:\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1=(precision*recall)/(precision+recall)\n",
    "    except ZeroDivisionError:\n",
    "        precision=recall=f1=-1\n",
    "        pass\n",
    "    \n",
    "    return (precision,recall,f1,TP,FP,FN)\n",
    "                    \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldenMappings = [\n",
    "    #William Smith nickname\n",
    "    ('s', 'person', 'name', frozenset({'nickname'})),\n",
    "    ('s', 'character', 'name', frozenset({'nickname'})),\n",
    "    #protagonist sound music\n",
    "    ('s', 'character', '*', frozenset({'protagonist'})),\n",
    "    #character Forrest Gump\n",
    "    ('s', 'character', '*', frozenset({'character'})),\n",
    "    #script of Casablanca\n",
    "    ('s', 'casting', 'note', frozenset({'script'})),\n",
    "    #best movie award James Cameron\n",
    "    ('s', 'movie', '*', frozenset({'movie'})),\n",
    "    #actor James Bond\n",
    "    ('s', 'person', '*', frozenset({'actor'})),\n",
    "    #flick Ellen Page thriller\n",
    "    ('s', 'movie', '*', frozenset({'flick'})),\n",
    "    #movie Terry Gilliam Benicio del Toro Dr gonzo\n",
    "    ('s', 'movie', '*', frozenset({'movie'})),\n",
    "    #director artificial intelligent Haley Joel Osment\n",
    "    #Trivia Don Quixote\n",
    "    #Movie Steven Spielberg\n",
    "    ('s', 'movie', '*', frozenset({'movie'})),\n",
    "    #German fellow actor Mel Gibson\n",
    "    ('s', 'person', '*', frozenset({'actor'})),\n",
    "    #Fellowship Ring King Towers\n",
    "    #Lord of the Rings films\n",
    "    ('s', 'movie', '*', frozenset({'films'})),\n",
    "    #Director John Hughes Matthew Broderick 1986\n",
    "    #cast Friends\n",
    "    ('s', 'casting', '*', frozenset({'cast'})),\n",
    "    #Henry Fonda mine\n",
    "    #name of actress in Lara Croft film\n",
    "    ('s', 'character', 'name', frozenset({'name'})),\n",
    "    ('s', 'person', 'name', frozenset({'name'})),\n",
    "    ('s', 'person', '*', frozenset({'actress'})),\n",
    "    ('s', 'movie', '*', frozenset({'film'})),\n",
    "    #Russell Crowe gladiator char name\n",
    "    ('s', 'character', '*', frozenset({'character'})),\n",
    "    ('s', 'character', 'name', frozenset({'name'})),\n",
    "    ('s', 'person', 'name', frozenset({'name'})),\n",
    "    #Darth Vader\n",
    "    #Norman Bates\n",
    "    #Atticus surname\n",
    "    ('s', 'character', 'name', frozenset({'surname'})),\n",
    "    ('s', 'person', 'name', frozenset({'surname'})),\n",
    "    #social network\n",
    "    #Space Odyssey Adventure year\n",
    "    #Chihiro animation\n",
    "    #actor Draco Harry Potter\n",
    "    ('s', 'person', '*', frozenset({'actor'})),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((\"%.2f; %.2f; %.2f; %.2f;\" % (threshold,precision,recall,f1)).replace('.',','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=wordEmbeddingsModel\n",
    "model.most_similar(positive=('person','movie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wordEmbeddingsHashA,wordEmbeddingsHashB,\n",
    " wordEmbeddingsHashC) = createEmbeddingsHash(wordEmbeddingsModel,\n",
    "                                             attributeHash,weight=0.5)\n",
    "pp(wordEmbeddingsHashC['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wn_sim=True, jaccard_sim=True,\n",
    "#emb_sim=False,  emb_model=None,\n",
    "#emb10_sim=False, emb10_hash=None\n",
    "#wordEmbeddingsHashB\n",
    "\n",
    "#sim_args={}\n",
    "#sim_args={'wn_sim':False,'jaccard_sim':False,'emb10_sim':True,'emb10_hash':wordEmbeddingsHashB}\n",
    "#sim_args={'emb10_sim':True,'emb10_hash':wordEmbeddingsHashC}\n",
    "#sim_args={'emb_sim':True,'emb_model':wordEmbeddingsModel}\n",
    "results = []\n",
    "threshold=0.9\n",
    "for threshold in [x/100 for x in range(50,101)][::5]:\n",
    "    print('threshold:', threshold)\n",
    "    for weight in [x/100 for x in range(50,101)][::5]:\n",
    "        (wordEmbeddingsHashA,wordEmbeddingsHashB,\n",
    "     wordEmbeddingsHashC) = createEmbeddingsHash(wordEmbeddingsModel,\n",
    "                                                 attributeHash,weight=weight)\n",
    "\n",
    "        sim_args={'emb10_sim':True,'emb10_hash':wordEmbeddingsHashC}\n",
    "\n",
    "        precision,recall,f1,TP,FP,FN = experimento(mi,smi,threshold,goldenMappings,sim_args=sim_args)\n",
    "\n",
    "        results.append( (weight,precision,recall,f1) )\n",
    "\n",
    "        print((\"%.2f; %.2f; %.2f; %.2f; %d; %d; %d\" % \n",
    "               (weight,precision,recall,f1,len(TP),len(FP),len(FN))).replace('.',','))\n",
    "\n",
    "        #print('threshold',threshold)\n",
    "        #print('precision',precision)\n",
    "        #print('recall',recall)\n",
    "        #print('f1',f1)\n",
    "\n",
    "        if False:\n",
    "            print('TP')\n",
    "            pp(TP)\n",
    "\n",
    "            print('FP')\n",
    "            pp(FP)\n",
    "\n",
    "            print('FN')\n",
    "            pp(FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wn_sim=True, jaccard_sim=True,\n",
    "#emb_sim=False,  emb_model=None,\n",
    "#emb10_sim=False, emb10_hash=None\n",
    "#wordEmbeddingsHashB\n",
    "\n",
    "#sim_args={}\n",
    "#sim_args={'wn_sim':False,'jaccard_sim':False,'emb10_sim':True,'emb10_hash':wordEmbeddingsHashB}\n",
    "#sim_args={'emb10_sim':True,'emb10_hash':wordEmbeddingsHashC}\n",
    "#sim_args={'emb_sim':True,'emb_model':wordEmbeddingsModel}\n",
    "results = []\n",
    "for weight in [x/100 for x in range(50,101)][::5]:\n",
    "    (wordEmbeddingsHashA,wordEmbeddingsHashB,\n",
    " wordEmbeddingsHashC) = createEmbeddingsHash(wordEmbeddingsModel,\n",
    "                                             attributeHash,weight=weight)\n",
    "    \n",
    "    sim_args={'wn_sim':False,'jaccard_sim':False,'emb10_sim':True,'emb10_hash':wordEmbeddingsHashC}\n",
    "    \n",
    "    precision,recall,f1,TP,FP,FN = experimento(mi,smi,threshold,goldenMappings,sim_args=sim_args)\n",
    "    \n",
    "    results.append( (weight,precision,recall,f1) )\n",
    "    \n",
    "    print((\"%.2f; %.2f; %.2f; %.2f; %d; %d; %d\" % \n",
    "           (weight,precision,recall,f1,len(TP),len(FP),len(FN))).replace('.',','))\n",
    "    \n",
    "    #print('threshold',threshold)\n",
    "    #print('precision',precision)\n",
    "    #print('recall',recall)\n",
    "    #print('f1',f1)\n",
    "    \n",
    "    if False:\n",
    "        print('TP')\n",
    "        pp(TP)\n",
    "\n",
    "        print('FP')\n",
    "        pp(FP)\n",
    "\n",
    "        print('FN')\n",
    "        pp(FN)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando apenas os querysets clássicos para \n",
    "## NÃO ENCONTRAR FALSO POSITIVOS schema mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wn_sim=True, jaccard_sim=True,\n",
    "#emb_sim=False,  emb_model=None,\n",
    "#emb10_sim=False, emb10_hash=None\n",
    "#wordEmbeddingsHashB\n",
    "\n",
    "sim_args={}\n",
    "#sim_args={'wn_sim':False,'jaccard_sim':False,'emb10_sim':True,'emb10_hash':wordEmbeddingsHashB}\n",
    "#sim_args={'emb10_sim':True,'emb10_hash':wordEmbeddingsHashA}\n",
    "#sim_args={'emb_sim':True,'emb_model':wordEmbeddingsModel}\n",
    "results = []\n",
    "for threshold in [x/100 for x in range(50,101)][::5]:\n",
    "    precision,recall,f1,TP,FP,FN = experimento(mi,smi,threshold,goldenMappings,sim_args=sim_args,\n",
    "                                               querySetFileName='querysets/queryset_imdb_spark.txt')\n",
    "    \n",
    "    results.append( (threshold,precision,recall,f1) )\n",
    "    \n",
    "    print((\"%.2f; %.2f; %.2f; %.2f; %d; %d; %d\" % (threshold,precision,recall,f1,len(TP),len(FP),len(FN))).replace('.',','))\n",
    "    \n",
    "    #print('threshold',threshold)\n",
    "    #print('precision',precision)\n",
    "    #print('recall',recall)\n",
    "    #print('f1',f1)\n",
    "    \n",
    "    if True:\n",
    "        #print('TP')\n",
    "        #pp(TP)\n",
    "\n",
    "        print('FP')\n",
    "        pp(FP)\n",
    "\n",
    "        #print('FN')\n",
    "        #pp(FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatCNGenpy",
   "language": "python",
   "name": "matcngenpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
