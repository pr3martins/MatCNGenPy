{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S-MatCNGenPy\n",
    "\n",
    "Este é um passo-a-passo da implementação em python do método **S-MatCNGenPy**, desenvolvido no trabalho \\[1\\]. O seu principal objetivo é garantir o suporte a referência ao esquema de dados em busca por palavras-chave em banco de dados. Observe que algumas consultas, como visto abaixo, não estão relacionadas apenas a valores do banco de dados, mas a própria estrutura do esquema.\n",
    "\n",
    "```\n",
    "    filmes do Will Smith\n",
    "```\n",
    "- **`filmes`** : relação Movie\n",
    "- **`Will`, `Smith`** : instâncias da tabela Person(Name) \n",
    "\n",
    "\n",
    "#### Leituras Importantes\n",
    "\n",
    "> [\\[1\\]](https://drive.google.com/file/d/1ZnljlKss9a8M_RDqseTYfZbQCjDhcJkk/view) MARTINS, Paulo Rodrigo O.; DA SILVA, Altigran Soares. *Uma Abordagem para Suporte a Referências ao Esquema em Consultas por Palavras-Chave em Bancos de Dados Relacionais*. Trabalho de Conclusão de Curso (Ciência da Computação), Universidade Federal do Amazonas, 2017. \n",
    "\n",
    "> [\\[2\\]]() DE OLIVEIRA, Pericles; DA SILVA, Altigran; DE MOURA, Edleno. *Match-Based Candidate Network Generation for Keyword Queries over Relational Databases*. In: Data Engineering (ICDE), 2018 IEEE 34st International Conference on. IEEE, 2016. Aceito pra Pubicação\n",
    "\n",
    "> [\\[3\\]](https://dl.acm.org/citation.cfm?id=1989383) BERGAMASCHI, Sonia et al. *Keyword search over relational databases: a metadata approach*. In: Proceedings of the 2011 ACM SIGMOD International Conference on Management of data. ACM, 2011. p. 565-576."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/MatCNGenPy/venv/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from pprint import pprint as pp\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import itertools\n",
    "import copy\n",
    "from math import log1p\n",
    "from queue import deque\n",
    "import ast\n",
    "import gc\n",
    "from queue import deque\n",
    "\n",
    "import nltk \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stw_set = set(stopwords.words('english')) - {'will'}\n",
    "\n",
    "# Connect to an existing database\n",
    "conn = psycopg2.connect(\"dbname=imdb user=postgres\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "\n",
    "Antes mesmo de receber os querysets, o sistema passa por um pré-processamento, que é responsavél pela criação de dois índices invertidos:\n",
    "\n",
    "* **wordHash**: tabela que associa cada termo do banco de dados com o seu **IAF (Inverse Attribute Frequency)** e também referencia todas Tabelas, Colunas e CTIDs em que a palavra ocorre. Nota: o CTID é o endereço físico de uma linha em uma tabela, utilizado para encontrar rapidamente uma tupla.\n",
    "```python\n",
    "wordHash['term'] = ( IAF , { 'table': { 'column' : [ctid] } } )\n",
    "```\n",
    "* **attributeHash**: tabela que para cada atributo (documento), armazena a sua norma e o número de palavras distintas.\n",
    "```python\n",
    "attributeHash['table']['column'] = ( norm , num_distinct_words )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação dos Índices Invertidos\n",
    "\n",
    "O processo de criação é realizado em três etapas. Primeiramente, o procedimento ```createInvertedIndex()``` faz uma varredura no banco de dados e preenche parcialmente o ```wordHash```, faltando apenas calcular os IAFs para cada termo. Além disso, este procedimento também ele também armazena no ```attributeHash``` o total de palavras distintas para cada atributo.\n",
    "\n",
    "Em seguida, os IAFs de cada termo são processados através do método ```processIAF(wordHash,attributeHash)```. Por último, as normas dos atributos (documentos) são calculadas no método ```processNormsOfAttributes(wordHash,attributeHash)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInvertedIndex():\n",
    "    #Output: wordHash (Term Index) with this structure below\n",
    "    #map['word'] = [ 'table': ( {column} , ['ctid'] ) ]\n",
    "\n",
    "    '''\n",
    "    The Term Index is built in a preprocessing step that scans only\n",
    "    once all the relations over which the queries will be issued.\n",
    "    '''\n",
    "    \n",
    "    wordHash = {}\n",
    "    attributeHash = {}\n",
    "    \n",
    "    # Get list of tablenames\n",
    "    cur.execute(\"SELECT DISTINCT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\")\n",
    "    for table in cur.fetchall():\n",
    "        table_name = table[0]\n",
    "        print('INDEXING TABLE ',table_name)\n",
    "        \n",
    "        attributeHash[table_name] = {}\n",
    "        \n",
    "        #Get all tuples for this tablename\n",
    "        cur.execute(\n",
    "            sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "            #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "        )\n",
    "\n",
    "        for row in cur.fetchall():\n",
    "            for column in range(1,len(row)):\n",
    "                column_name = cur.description[column][0]   \n",
    "                ctid = row[0]\n",
    "\n",
    "                for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "                    \n",
    "                    #Ignoring STOPWORDS\n",
    "                    if word in stw_set:\n",
    "                        continue\n",
    "\n",
    "                    #If word entry doesn't exists, it will be inicialized (setdefault method),\n",
    "                    #Append the location for this word\n",
    "                    wordHash.setdefault(word, {})                    \n",
    "                    wordHash[word].setdefault( table_name , {} )\n",
    "                    wordHash[word][table_name].setdefault( column_name , [] ).append(ctid)\n",
    "                    \n",
    "                    attributeHash[table_name].setdefault(column_name,(0,set()))\n",
    "                    attributeHash[table_name][column_name][1].add(word)\n",
    "        \n",
    "        #Count words\n",
    "        \n",
    "        for (column_name,(norm,wordSet)) in attributeHash[table_name].items():\n",
    "            num_distinct_words = len(wordSet)\n",
    "            wordSet.clear()\n",
    "            attributeHash[table_name][column_name] = (norm,num_distinct_words)\n",
    "        \n",
    "\n",
    "    print ('INVERTED INDEX CREATED')\n",
    "    return (wordHash,attributeHash)\n",
    "\n",
    "(wordHash,attributeHash) = createInvertedIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(wordHash['denzel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(attributeHash['movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processIAF(wordHash,attributeHash):\n",
    "    \n",
    "    total_attributes = sum([len(attribute) for attribute in attributeHash.values()])\n",
    "    \n",
    "    for (term, values) in wordHash.items():\n",
    "        \n",
    "        attributes_with_this_term = sum([len(attribute) for attribute in wordHash[term].values()])\n",
    "        \n",
    "        IAF = log1p(total_attributes/attributes_with_this_term)\n",
    "                \n",
    "        wordHash[term] = (IAF,values)\n",
    "    print('IAF PROCESSED')\n",
    "\n",
    "processIAF(wordHash,attributeHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(wordHash['denzel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNormsOfAttributes(wordHash,attributeHash):\n",
    "  \n",
    "    # Get list of tablenames\n",
    "    cur.execute(\"SELECT DISTINCT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\")\n",
    "    for table in cur.fetchall():\n",
    "        table_name = table[0]\n",
    "        print('PROCESSING TABLE ',table_name)\n",
    "        \n",
    "        #Get all tuples for this tablename\n",
    "        cur.execute(\n",
    "            sql.SQL(\"SELECT ctid, * FROM {};\").format(sql.Identifier(table_name))\n",
    "            #NOTE: sql.SQL is needed to specify this parameter as table name (can't be passed as execute second parameter)\n",
    "        )\n",
    "\n",
    "        for row in cur.fetchall():\n",
    "            for column in range(1,len(row)):\n",
    "                column_name = cur.description[column][0]   \n",
    "                ctid = row[0]\n",
    "\n",
    "                for word in [word.strip(string.punctuation) for word in str(row[column]).lower().split()]:\n",
    "                    \n",
    "                    #Ignoring STOPWORDS\n",
    "                    if word in stw_set:\n",
    "                        continue\n",
    "                    \n",
    "                    (prevNorm,num_distinct_words)=attributeHash[table_name][column_name]\n",
    "                    \n",
    "                    IAF = wordHash[word][0]\n",
    "                    \n",
    "                    Norm = prevNorm + IAF\n",
    "                    \n",
    "                    attributeHash[table_name][column_name]=(Norm,num_distinct_words)\n",
    "                    \n",
    "\n",
    "    print ('NORMS OF ATTRIBUTES PROCESSED')\n",
    "\n",
    "processNormsOfAttributes(wordHash,attributeHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(attributeHash['movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "O processamento das consultas é realizado em "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuerySets():\n",
    "    QuerySet = []\n",
    "    with open('querysets/queryset_imdb_martins.txt') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            #The line bellow Remove words not in OLIVEIRA experiments\n",
    "            Q = [word.strip(string.punctuation) for word in line.split() if word not in ['title','dr.',\"here's\",'char','name'] and word not in stw_set]  \n",
    "            \n",
    "            #Q = [word.strip(string.punctuation) for word in line.split() if word not in stw_set]  \n",
    "            \n",
    "            QuerySet.append(Q)\n",
    "    return QuerySet\n",
    "        \n",
    "QuerySet = getQuerySets()\n",
    "QuerySet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperação de Tuple-sets\n",
    "Esta etapa consiste em recuperar conjuntos de tuplas que contém cada palavra-chave, chamados de tuple-sets. O algoritmo `TSFind`, que realiza esse processo, pode ser é divido em três partes: \n",
    "* **Recuperação de tuplas:** Essa parte consiste em encontrar os conjuntos de tuplas que contém cada uma das palavras do Queryset. Essas informações já foram pré-processadas no índice invertido `wordHash`.\n",
    "* **Interseção de tuplas:** Esta parte acontece no algoritmo `TSInter` e é responsável por encontrar tuplas que contém mais de uma das palavras-chave. Além disso, esta etapa irá garantir que os tuple-sets `TABLE{word}` contenham apenas a palavra `word` e nenhuma outra palavra do queryset. Esta propriedade é necessária para encontrar a cobertura mínima (etapa de criação de query matches). \n",
    "* **Criação de tuple-sets:** Esta parte irá condensar os resultados. Em vez de listar todas as tuplas que contenham as palavras-chave, precisamos apenas saber quais colunas possuem cada uma das palavras. Por isso, os tuple-sets terão a estrutura (o primeiro atributo refere-se a *value* ou *schema*):\n",
    "```python\n",
    "TupleSet = ('v','table','column', frozenset({words}))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSFind(Q):\n",
    "    #Input:  A keyword query Q=[k1, k2, . . . , km]\n",
    "    #Output: Set of non-free and non-empty tuple-sets Rq\n",
    "\n",
    "    '''\n",
    "    The tuple-set Rki contains the tuples of Ri that contain all\n",
    "    terms of K and no other keywords from Q\n",
    "    '''\n",
    "    \n",
    "    #Part 1: Find sets of tuples containing each keyword\n",
    "    global P\n",
    "    P = {}\n",
    "    for keyword in Q:\n",
    "        tupleset = set()\n",
    "        \n",
    "        if keyword not in wordHash:\n",
    "            continue\n",
    "        \n",
    "        for (table,attributes) in wordHash.get(keyword)[1].items():\n",
    "            for (attribute,ctids) in attributes.items():\n",
    "                for ctid in ctids:\n",
    "                    tupleset.add( (table,attribute,ctid) )\n",
    "        P[frozenset([keyword])] = tupleset\n",
    "    \n",
    "    #Part 2: Find sets of tuples containing larger termsets\n",
    "    P = TSInter(P)\n",
    "    \n",
    "    #Part 3:Build tuple-sets\n",
    "    Rq = set()\n",
    "    for keyword , tuples in P.items():\n",
    "        for (table,attribute,ctid) in tuples:\n",
    "            Rq.add( ('v',table,attribute,keyword) )\n",
    "    print ('TUPLE SETS CREATED')\n",
    "    return Rq\n",
    "\n",
    "\n",
    "def TSInter(P):\n",
    "    #Input: A Set of non-empty tuple-sets for each keyword alone P \n",
    "    #Output: The Set P, but now including larger termsets (process Intersections)\n",
    "\n",
    "    '''\n",
    "    Termset is any non-empty subset K of the terms of a query Q        \n",
    "    '''\n",
    "    \n",
    "    Pprev = {}\n",
    "    Pprev=copy.deepcopy(P)\n",
    "    Pcurr = {}\n",
    "\n",
    "    combinations = [x for x in itertools.combinations(Pprev.keys(),2)]\n",
    "    for ( Ki , Kj ) in combinations:\n",
    "        Tki = Pprev[Ki]\n",
    "        Tkj = Pprev[Kj]\n",
    "        \n",
    "        X = Ki | Kj\n",
    "        Tx = Tki & Tkj        \n",
    "        \n",
    "        if len(Tx) > 0:            \n",
    "            Pcurr[X]  = Tx            \n",
    "            Pprev[Ki] = Tki - Tx         \n",
    "            Pprev[Kj] = Tkj - Tx\n",
    "            \n",
    "    if Pcurr != {}:\n",
    "        Pcurr = copy.deepcopy(TSInter(Pcurr))\n",
    "        \n",
    "    #Pprev = Pprev U Pcurr\n",
    "    Pprev.update(Pcurr)     \n",
    "    return Pprev   \n",
    "\n",
    "Q = ['actor', 'james', 'bond']\n",
    "Rq = TSFind(Q)\n",
    "pp(Rq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação Schema-sets\n",
    "\n",
    "Esta etapa consiste na criação dos Schema-sets, que é uma estrutura análoga aos tuple-sets vistos na etapa anterior. Aqui, o processo também é divido em três partes: \n",
    "* **Mapeamento de Elementos do Esquema (*Schema Matching*):** Essa parte consiste em analisar a similaridade entre as palavras do querysets e elementos do esquema (nomes de relações e atributos).\n",
    "* **Análise de Termos Adjacentes:** Esta parte verifica as relações entre as palavras chave, muitas vezes uma palavras-chave relacioada a elemento do esquema delimita o domínio das palavras-chave adjacentes. Ex: Actor James Bond delimita a palavra James para nome de Pessoa, em vez de nome de Filme.\n",
    "* **Criação de Schema-sets:** Esta parte irá formatar os resultados para ficarem semelhantes à estrutura de tuple-sets, seguindo a estrutura a seguir (o primeiro atributo refere-se a *value* ou *schema*):\n",
    "```python\n",
    "SchemaSet = ('s','table','column', frozenset({words}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similaridades para o Schema-Matching\n",
    "\n",
    "Para o mapeamento de palavras para elementos do esquema, foram utilizadas métricas de similaridade de escrita e semântica.\n",
    "O Coeficiente de Jaccard é uma métrica que avalia a interseção entre duas palavras, sendo ideal para similaridades de escrita, como abreviações ou erros de digitação. \n",
    "\n",
    "Por outro lado, as métricas semânticas utilizam o dicionário léxico WordNet para encontrar similaridades de sentido. O pacote de ferramentas NLTK disponibiliza uma série de métricas semânticas [aqui](http://www.nltk.org/howto/wordnet.html \"WordNet Interface\"). Entre elas, as principais são a Path Similarity e a Wu-Palmer Similarity. A primeira métrica procura encontrar a menor distância entre duas palavras, no grafo de relações do WordNet, enquanto a segunda analisa o ancestral comum mais próximo entre duas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordNetSimilarity(wordA,wordB):\n",
    "    \n",
    "    A = set(wn.synsets(wordA))\n",
    "    B = set(wn.synsets(wordB))\n",
    "    \n",
    "    wupSimilarities = [0]\n",
    "    pathSimilarities = [0]\n",
    "    for (sense1,sense2) in itertools.product(A,B):        \n",
    "        wupSimilarities.append(wn.wup_similarity(sense1,sense2) or 0)\n",
    "        pathSimilarities.append(wn.path_similarity(sense1,sense2) or 0)\n",
    "    return max(max(wupSimilarities),max(pathSimilarities))\n",
    "\n",
    "def jaccard_similarity(wordA,wordB):\n",
    "    \n",
    "    A = set(wordA)\n",
    "    B = set(wordB)\n",
    "    \n",
    "    return len(A & B ) / len(A | B)\n",
    "    \n",
    "def wordSimilarity(wordA,wordB):\n",
    "    return max( (jaccard_similarity(wordA,wordB),wordNetSimilarity(wordA,wordB)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(wn.synsets('come'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo para Criação dos Schema-Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SchSFind(Q,threshold):\n",
    "    S = []\n",
    "    \n",
    "    for (position,keyword) in enumerate(Q):\n",
    "        for (table,values) in attributeHash.items():\n",
    "            \n",
    "            sim = wordSimilarity(keyword,table)\n",
    "            if sim >= threshold:\n",
    "                S.append( (table,'*',{keyword},position,sim) )\n",
    "            \n",
    "            for attribute in values.keys():\n",
    "                \n",
    "                if(attribute=='id'):\n",
    "                    continue\n",
    "                \n",
    "                sim = wordSimilarity(keyword,attribute)\n",
    "                \n",
    "                if sim >= threshold:\n",
    "                    S.append( (table,attribute,{keyword},position,sim) )\n",
    "    #S = SchSInter(S)\n",
    "\n",
    "    print ('SCHEMA SETS CREATED')\n",
    "    Sq = {('s',table,attribute,frozenset(keywords)) for (table,attribute,keywords,position,sim) in S}\n",
    "        \n",
    "    return Sq\n",
    "\n",
    "'''\n",
    "Em vez de interseções, deve ser feita uma análise dos adjacentes..\n",
    "\n",
    "def SchSInter(S):\n",
    "    \n",
    "    Scurr= S.copy()\n",
    "    \n",
    "    somethingChanged = False\n",
    "\n",
    "    combinations = [x for x in itertools.combinations(Scurr,2)]\n",
    "    \n",
    "    for ( A , B ) in combinations:    \n",
    "    \n",
    "        (tableA,attributeA,wordsA,positionA,simA) = A\n",
    "        (tableB,attributeB,wordsB,positionB,simB) = B\n",
    "        \n",
    "        if A not in Scurr or B not in Scurr:\n",
    "            continue\n",
    "        \n",
    "        if tableA == tableB and abs(positionA-positionB)<=1:\n",
    "            print('A:\\n',A)\n",
    "            print('B:\\n',B)\n",
    "            \n",
    "            AB = (tableA, '*' , wordsA | wordsB, max((positionA,positionB)) , max((simA,simB)) )\n",
    "            \n",
    "            Scurr.remove(A)\n",
    "            Scurr.remove(B)\n",
    "            Scurr.append(AB)\n",
    "            \n",
    "            somethingChanged = True \n",
    "   \n",
    "    if somethingChanged:\n",
    "        return SchSInter(Scurr)\n",
    "    \n",
    "    return Scurr\n",
    "'''\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = QuerySet[5] = ['actor', 'james', 'bond']\n",
    "SimilarityCoeficient = 0.799999999999\n",
    "Sq = SchSFind(Q,SimilarityCoeficient)\n",
    "Sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Query Matches\n",
    "\n",
    "As etapas anteriores, de criação de schema-sets e tuple-sets, foram responsáveis por identificar quais relações possuem alguma informação sobre as palavras-chave. Nesta etapa de criação de full matches, o objetivo é combinar esses tuple-sets e schema-sets para se obter uma resposta completa, mínima e relevante para o usuário. \n",
    "\n",
    "O algoritmo `QMGen` é responsável por encontrar combinações de tuple-sets/schema-sets que compõem uma cobertura mínima (`MinimalCover`) sobre o queryset.\n",
    "- **Total**: Cada palavra-chave deve estar presente em ao menos uma das tuplas da query-match.\n",
    "- **Mínima**: Não é possível remover nenhum tuple-set/schema-set da query-match e manter a cobertura total sobre o queryset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinimalCover(MC, Q):\n",
    "    #Input:  A subset MC (Match Candidate) to be checked as total and minimal cover\n",
    "    #Output: If the match candidate is a TOTAL and MINIMAL cover\n",
    "\n",
    "    Subset = [termset for category,table,attribute,termset in MC]\n",
    "    u = set().union(*Subset)    \n",
    "    \n",
    "    isTotal = (u == set(Q))\n",
    "    for element in Subset:\n",
    "        \n",
    "        new_u = list(Subset)\n",
    "        new_u.remove(element)\n",
    "        \n",
    "        new_u = set().union(*new_u)\n",
    "        \n",
    "        if new_u == set(Q):\n",
    "            return False\n",
    "    \n",
    "    return isTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QMGen(Q,Rq):\n",
    "    #Input:  A keyword query Q, The set of non-empty non-free tuple-sets Rq\n",
    "    #Output: The set Mq of query matches for Q\n",
    "    \n",
    "    '''\n",
    "    Query match is a set of tuple-sets that, if properly joined,\n",
    "    can produce networks of tuples that fulfill the query. They\n",
    "    can be thought as the leaves of a Candidate Network.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Mq = []\n",
    "    for i in range(1,len(Q)+1):\n",
    "        for subset in itertools.combinations(Rq,i):\n",
    "            if(MinimalCover(subset,Q)):\n",
    "                Mq.append(set(subset))\n",
    "    return Mq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q =['actor', 'draco', 'harry','potter']\n",
    "\n",
    "Rq = TSFind(Q)\n",
    "SimilarityCoeficient = 0.799999999999\n",
    "Sq = SchSFind(Q,SimilarityCoeficient)\n",
    "\n",
    "Mq = QMGen(Q,Sq|Rq)\n",
    "for match in Mq:\n",
    "    pp(match)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Candidate Networks\n",
    "\n",
    "Na etapa anterior, obteve-se as full matches, que compreendem todas as informações necessárias para o usuário. O próximo passo é encontrar maneiras de conectar estas informações para formar uma resposta para o usuário. Estas conexões, chamadas de candidate networks, são derivadas das restrições de integridade referencial do banco de dados, também conhecidas como chaves estrangeiras.\n",
    "\n",
    "A criação de candidate networks utiliza dois grafos:\n",
    "- **Schema Graph**: vértice que representa o banco de dados e é utilizado como base para o match graph. Ele contém como vértices os free tuple-sets associados a cada relação do banco de dados e como arestas as restrições de integridade referencial.\n",
    "\n",
    "    O Schema Graph foi implementado como um dicionário, no qual cada vértice aponta para um outro vértice. Além disso, também é armazenada informações sobre as arestas, como direção e quais atributos entre as tabelas tem a relação de restrição referencial. A estrutura do Schema Graph pode ser observada a seguir:\n",
    "   \n",
    "```python\n",
    "    G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchemaGraph():\n",
    "    #Output: A Schema Graph G  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "    \n",
    "    \n",
    "    G = {} \n",
    "    cur.execute(\"SELECT tablename FROM pg_tables WHERE schemaname!='pg_catalog' AND schemaname !='information_schema';\")\n",
    "    for table in cur.fetchall():\n",
    "        G.setdefault(table[0],{})\n",
    "    \n",
    "    sql = \"SELECT DISTINCT                 tc.table_name, kcu.column_name,                 ccu.table_name AS foreign_table_name, ccu.column_name AS foreign_column_name             FROM information_schema.table_constraints AS tc              JOIN information_schema.key_column_usage AS kcu                 ON tc.constraint_name = kcu.constraint_name             JOIN information_schema.constraint_column_usage AS ccu                 ON ccu.constraint_name = tc.constraint_name             WHERE constraint_type = 'FOREIGN KEY'\"\n",
    "    cur.execute(sql)\n",
    "    relations = cur.fetchall()\n",
    "    \n",
    "    for (table,column,foreign_table,foreign_column) in relations:\n",
    "        G[table][foreign_table] = (1,column, foreign_column)\n",
    "        G[foreign_table][table] = (-1,foreign_column,column)\n",
    "    print ('SCHEMA CREATED')\n",
    "    return G\n",
    "G = getSchemaGraph()\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Match Graph**: grafo gerado a partir de uma query match e o schema graph. No entanto, no match graph tuple-sets/schema-sets também são modelados como vértices. Para criá-lo, adiciona-se ao schema graph os tuple-sets/schema-sets presentes na query match. Um tuple-set de uma tabela x terá os mesmos relacionamentos (arestas) que o vértice x.\n",
    "\n",
    "```python\n",
    "    Gts['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "\n",
    "    Gts[('s','table','column', frozenset({words}))] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchGraph(Rq, G, M):\n",
    "    #Input:  The set of non-empty non-free tuple-sets Rq,\n",
    "    #        The Schema Graph G,\n",
    "    #        A Query Match M\n",
    "    #Output: A Schema Graph Gts  with the structure below:\n",
    "    # G['node'] = edges\n",
    "    # G['table'] = { 'foreign_table' : (direction, column, foreign_column) }\n",
    "\n",
    "    '''\n",
    "    A Match Subgraph Gts[M] is a subgraph of G that contains:\n",
    "        The set of free tuple-sets of G\n",
    "        The query match M\n",
    "    '''\n",
    "    \n",
    "    Gts = copy.deepcopy(G)\n",
    "    \n",
    "    tables = set()\n",
    "    #Insert non-free nodes\n",
    "    for (category,table ,attribute, keywords) in M:\n",
    "        Gts[(category,table,attribute,keywords)]=copy.deepcopy(Gts[table])\n",
    "        for foreign_table , (direction,column,foreign_column) in Gts[(category,table,attribute,keywords)].items():\n",
    "            Gts[foreign_table][(category,table,attribute,keywords)] = (direction*(-1),foreign_column,column)\n",
    "\n",
    "    return Gts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = {('s', 'person', '*', frozenset({'actor'})),\n",
    " ('v', 'char', 'name', frozenset({'draco'})),\n",
    " ('v', 'movie', 'title', frozenset({'potter', 'harry'}))}\n",
    "\n",
    "Gts = MatchGraph(Rq|Sq, G, M)\n",
    "pp(Gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo para Criação de Candidate Networks\n",
    "\n",
    "Para criar uma candidate network, o algoritmo `SingleCN` procura um caminho mínimo no match graph que visite todas os non-free tuple-sets/schema-sets da query match. \n",
    "\n",
    "Este caminho deve ser:\n",
    "- **Mínimo:** garantido através do algoritmo de caminho mínima baseado em busca por largura (BFS).\n",
    "- **Total:** a função `containsMatch` garante que todos os tuple-sets/schema-sets da query match sejam visitados.\n",
    "- **Seguro (*Sound*):** uma joining networks of tuple-sets é considerado sound se ela não contém uma subárvore na forma $R^K - S^L - R^M $, na qual $R$ e $S$ são relações e o schema graph tem uma aresta $R \\rightarrow S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsMatch(Ji,M):\n",
    "    for relation in M:\n",
    "        if relation not in Ji:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def isJNTSound(Gts,Ji):\n",
    "    if len(Ji)<3:\n",
    "        return True\n",
    "    \n",
    "    for i in range(len(Ji)-2):\n",
    "        \n",
    "        if type(Ji[i]) is str:\n",
    "            tableA = Ji[i]\n",
    "        else:\n",
    "            tableA = Ji[i][0]\n",
    "            \n",
    "        if type(Ji[i+2]) is str:\n",
    "            tableB = Ji[i+2]\n",
    "        else:\n",
    "            tableB = Ji[i+2][0]          \n",
    "            \n",
    "        if tableA==tableB:\n",
    "            edge_info = Gts[Ji[i]][Ji[i+1]]\n",
    "            if(edge_info[0] == -1):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleCN(FM,Gts,Tmax):    \n",
    "    '''\n",
    "    print('================================================================================\\nSINGLE CN')\n",
    "    print('Tmax ',Tmax)\n",
    "    print('FM')\n",
    "    pp(FM)\n",
    "    \n",
    "    print('\\n\\nGts')\n",
    "    pp(Gts)\n",
    "    print('\\n\\n')\n",
    "    '''\n",
    "    F = deque()\n",
    "\n",
    "    first_element = list(FM)[0]\n",
    "    J = [first_element]\n",
    "    \n",
    "    if len(FM)==1:\n",
    "        return J\n",
    "    \n",
    "    F.append(J)\n",
    "    \n",
    "    while F:\n",
    "        J = F.popleft()           \n",
    "        u = J[-1]\n",
    "        '''\n",
    "        print('--------------------------------------------\\nParctial CN')\n",
    "        print('J ',J,'\\n')\n",
    "        \n",
    "        print('\\nAdjacents:')\n",
    "        pp(Gts[u].items())\n",
    "        '''\n",
    "        for (adjacent,edge_info) in Gts[u].items():\n",
    "            if (type(adjacent) is str) or (adjacent not in J):\n",
    "                Ji = J + [adjacent]\n",
    "                if (Ji not in F) and (len(Ji)<Tmax) and (isJNTSound(Gts,Ji)):\n",
    "                    if(containsMatch(Ji,FM)):\n",
    "                        '''\n",
    "                        print('--------------------------------------------\\nGenerated CN')\n",
    "                        print('J ',Ji,'\\n')\n",
    "                        '''\n",
    "                        return Ji\n",
    "                    else:\n",
    "                        F.append(Ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleCN(M,Gts,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchCN(G,Sq,Rq,Mq):    \n",
    "    Cns = []                        \n",
    "    for M in Mq:\n",
    "        Gts = MatchGraph(Rq|Sq, G, M)\n",
    "        Cn = SingleCN(M,Gts,10)\n",
    "        if(Cn is not None):\n",
    "            Cns.append( (Cn,Gts,M) )\n",
    "    return Cns\n",
    "\n",
    "\n",
    "Cns = MatchCN(G,Rq,Sq,Mq)\n",
    "for Cn in Cns:\n",
    "    pp(Cn)\n",
    "    print('--------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de Candidate Networks\n",
    "\n",
    "Como existem diferentes maneiras de se conectar as informações associadas as palavras-chave, várias candidate networks serão geradas. Entretanto, na maioria das vezes, apenas uma delas contém uma resposta relevante para o usuário. Por este motivo, esta esta etapa irá avaliar e ranquear as candidate networks por relevância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNRank(Cns,mi,smi):\n",
    "    Ranking = []\n",
    "    for (Cn,Gts,M) in Cns:\n",
    "        cosprod = 1\n",
    "        valuecont = 0\n",
    "\n",
    "        schemaprod = 1\n",
    "        schemacont = 0\n",
    "        \n",
    "        for relation in Cn:\n",
    "            if(type(relation) is str):\n",
    "                continue\n",
    "            \n",
    "            (category,table,attribute,predicates) = relation\n",
    "            \n",
    "            if (category == 'v'):\n",
    "                \n",
    "                valuecont+=1\n",
    "                \n",
    "                if predicates == frozenset(['']):\n",
    "                    continue\n",
    "\n",
    "                (norm_attribute,distinct_terms) = attributeHash[table][attribute]\n",
    "\n",
    "                wsum = 0\n",
    "\n",
    "                for term in predicates:\n",
    "\n",
    "                    IAF = wordHash[term][0]\n",
    "\n",
    "                    ctids = wordHash[term][1][table][attribute]\n",
    "                    fkj = len(ctids)\n",
    "\n",
    "                    if fkj>0:\n",
    "\n",
    "                        TF = log1p(fkj) / log1p(distinct_terms)\n",
    "\n",
    "                        wsum = wsum + TF*IAF\n",
    "                \n",
    "                cos = wsum/norm_attribute\n",
    "                cosprod *= cos\n",
    "            elif (category == 's'):\n",
    "                \n",
    "                schemacont+=1\n",
    "                \n",
    "                if(attribute == '*'):\n",
    "                    schemaTerm = table\n",
    "                else:\n",
    "                    schemaTerm = attribute\n",
    "                \n",
    "                schemasum = 0\n",
    "                \n",
    "                for term in predicates:\n",
    "                    schemasum+=wordSimilarity(term, schemaTerm)\n",
    "                \n",
    "                schemaprod *= schemasum\n",
    "                \n",
    "            \n",
    "        \n",
    "        valuescore = schemascore = 0\n",
    "        \n",
    "        score = 1/len(Cn)\n",
    "        \n",
    "        if valuecont>0:\n",
    "            valuescore = mi * cosprod \n",
    "            score*=valuescore\n",
    "        \n",
    "        if schemacont>0:\n",
    "            schemascore = smi * schemaprod\n",
    "            score*=schemascore\n",
    "            \n",
    "        Ranking.append((Cn,Gts,M,score , ( valuescore , schemascore , len(Cn) )  ))\n",
    "    return sorted(Ranking,key=lambda x: x[3],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = 464576.1086662768\n",
    "smi = 1\n",
    "\n",
    "SortedCn = CNRank(Cns,mi,smi)\n",
    "SortedCn\n",
    "\n",
    "for Cn in SortedCn:\n",
    "    print('Value Score: ',\"%.8f\" % Cn[4][0],'\\nSchema Score:',\"%.8f\" % Cn[4][1], '\\n|Cn|: ',Cn[4][2],'\\nTotal Score: ',\"%.8f\" % Cn[3])\n",
    "    pp(Cn[0])\n",
    "    print('----------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais abaixo tem a execução para outras CNS (querysets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing():\n",
    "    global wordHash\n",
    "    global attributeHash\n",
    "    (wordHash,attributeHash) = createInvertedIndex()\n",
    "    processIAF(wordHash,attributeHash)\n",
    "    processNormsOfAttributes(wordHash,attributeHash)\n",
    "    print('PRE-PROCESSING STAGE FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mi,smi):   \n",
    "    QuerySets = getQuerySets()\n",
    "    maxscores = (list(),list())\n",
    "    for (i,Q) in enumerate(QuerySets):\n",
    "       \n",
    "        print('QUERY-SET ',Q,'\\n')\n",
    "        \n",
    "        print('FINDING TUPLE-SETS')\n",
    "        Rq = TSFind(Q)\n",
    "        print(len(Rq),'TUPLE-SETS CREATED\\n')\n",
    "        \n",
    "        print('FINDING SCHEMA-SETS')\n",
    "        SimilarityThreshold = 0.799999999999\n",
    "        Sq = SchSFind(Q,SimilarityThreshold)\n",
    "        print(len(Sq),' SCHEMA-SETS CREATED\\n')\n",
    "        \n",
    "        print('GENERATING QUERY MATCHES')\n",
    "        Mq = QMGen(Q,Sq|Rq)\n",
    "        print (len(Mq),'QUERY MATCHES CREATED\\n')\n",
    "        '''\n",
    "        for M in Mq[:20]:\n",
    "            pp(M)\n",
    "            print('\\n\\n')\n",
    "        '''\n",
    "        print('GENERATING CANDIDATE NETWORKS')\n",
    "        G = getSchemaGraph()\n",
    "        \n",
    "        Cns = MatchCN(G,Rq,Sq,Mq)\n",
    "        \n",
    "        print (len(Cns),'CANDIDATE NETWORKS CREATED\\n')\n",
    "        \n",
    "        '''\n",
    "        for Cn in Cns[:20]:\n",
    "            pp(Cn[0])\n",
    "            print('\\n\\n')\n",
    "            #pp(Cn[1])\n",
    "            #print('\\n\\n\\n==================================================================================\\n')\n",
    "        '''\n",
    "        print('RANKING CANDIDATE NETWORKS')\n",
    "        RankedCns = CNRank(Cns,mi,smi)\n",
    "        for (j,Cn) in enumerate(RankedCns):\n",
    "            print(j+1,'ª CN')\n",
    "            print('Value Score: ',\"%.8f\" % Cn[4][0],'\\nSchema Score:',\"%.8f\" % Cn[4][1], '\\n|Cn|: ',Cn[4][2],'\\nTotal Score: ',\"%.8f\" % Cn[3])\n",
    "            pp(Cn[0])\n",
    "            print('----------------------------------------------------------------------\\n')\n",
    "        \n",
    "            maxscores[0].append(Cn[4][0])\n",
    "            maxscores[1].append(Cn[4][1])\n",
    "        gc.collect()\n",
    "        \n",
    "        print('==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================')\n",
    "    return maxscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = 0.90/1.9372498568291752e-06\n",
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = 464576.1086662768\n",
    "smi = 1\n",
    "maxscores = main(mi,smi)\n",
    "maxscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observando os maxscores que eu atribui valores a constante mi para normalizar o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(maxscores[0]), max(maxscores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailedInfo(Q):   \n",
    "    print('QUERY-SET ',Q,'\\n')\n",
    "\n",
    "    print('FINDING TUPLE-SETS')\n",
    "    Rq = TSFind(Q)\n",
    "    print(len(Rq),'TUPLE-SETS CREATED\\n')\n",
    "\n",
    "    print('FINDING SCHEMA-SETS')\n",
    "    SimilarityThreshold = 0.799999999999\n",
    "    Sq = SchSFind(Q,SimilarityThreshold)\n",
    "    print(len(Sq),' SCHEMA-SETS CREATED\\n')\n",
    "\n",
    "    print('GENERATING QUERY MATCHES')\n",
    "    Mq = QMGen(Q,Sq|Rq)\n",
    "    print (len(Mq),'QUERY MATCHES CREATED\\n')\n",
    "    \n",
    "    for M in Mq[:20]:\n",
    "        pp(M)\n",
    "        print('\\n\\n')\n",
    "    \n",
    "    print('GENERATING CANDIDATE NETWORKS')\n",
    "    G = getSchemaGraph()\n",
    "\n",
    "    Cns = MatchCN(G,Rq,Sq,Mq)\n",
    "\n",
    "    print (len(Cns),'CANDIDATE NETWORKS CREATED\\n')\n",
    "\n",
    "    '''\n",
    "    for Cn in Cns[:20]:\n",
    "        pp(Cn[0])\n",
    "        print('\\n\\n')\n",
    "        #pp(Cn[1])\n",
    "        #print('\\n\\n\\n==================================================================================\\n')\n",
    "    '''\n",
    "    print('RANKING CANDIDATE NETWORKS')\n",
    "    mi = 2.7E+12\n",
    "    smi = 1\n",
    "    RankedCns = CNRank(Cns,mi,smi)\n",
    "    for (j,Cn) in enumerate(RankedCns):\n",
    "        print(j+1,'ª CN')\n",
    "        print('Value Score: ',\"%.8f\" % Cn[4][0],'\\nSchema Score:',\"%.8f\" % Cn[4][1], '\\n|Cn|: ',Cn[4][2],'\\nTotal Score: ',\"%.8f\" % Cn[3])\n",
    "        pp(Cn[0])\n",
    "        print('----------------------------------------------------------------------\\n')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print('==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================\\\n",
    "==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailedInfo(['ellen','page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todos os mapeamentos de elementos para esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for Q in getQuerySets():\n",
    "    Sq = SchSFind(Q,0.8)\n",
    "    print(Q)\n",
    "    pp(Sq)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matcngenpy",
   "language": "python",
   "name": "matcngenpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
